\begin{thebibliography}{10}

\bibitem{vaswani_attention_2017}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, ≈Å~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem{xiaoliang_design_2024}
Ma~Xiaoliang, Zhao {RuQiang}, Liu Ying, Deng Congjian, and Du~Dequan.
\newblock Design of a large language model for improving customer service in telecom operators.
\newblock {\em Electronics Letters}, 60(10):e13218, 2024.

\bibitem{dong_self-collaboration_2024}
Yihong Dong, Xue Jiang, Zhi Jin, and Ge~Li.
\newblock Self-collaboration code generation via {ChatGPT}.
\newblock {\em {ACM} Trans. Softw. Eng. Methodol.}, 33(7):189:1--189:38, 2024.

\bibitem{qian_chatdev_2024}
Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun.
\newblock {ChatDev}: Communicative agents for software development.
\newblock In {\em Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 15174--15186. Association for Computational Linguistics, 2024.

\bibitem{acharya_devin_2025}
Kamal Acharya.
\newblock Devin: A cautionary tale of the autonomous {AI} engineer, 2025.
\newblock Disponible en: \url{https://medium.com/@lotussavy/devin-a-cautionary-tale-of-the-autonomous-ai-engineer-e1339ede8f8a}.

\bibitem{gao_retrieval-augmented_2024}
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi~Dai, Jiawei Sun, Meng Wang, and Haofen Wang.
\newblock Retrieval-augmented generation for large language models: A survey, 2024.

\bibitem{yao_react_2023}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
\newblock {ReAct}: Synergizing reasoning and acting in language models.
\newblock In {\em International Conference on Learning Representations ({ICLR})}, 2023.

\bibitem{sim_ramp-up_1998}
S.E. Sim and R.C. Holt.
\newblock The ramp-up problem in software projects: a case study of how software immigrants naturalize.
\newblock In {\em Proceedings of the 20th International Conference on Software Engineering}, pages 361--370, 1998.

\bibitem{steinmacher_systematic_2015}
Igor Steinmacher, Marco Aurelio~Graciotto Silva, Marco~Aurelio Gerosa, and David~F. Redmiles.
\newblock A systematic literature review on the barriers faced by newcomers to open source software projects.
\newblock {\em Information and Software Technology}, 59:67--85, 2015.

\bibitem{ritz_artificial_2023}
Eva Ritz, Fabio Donisi, Edona Elshan, and Roman Rietsche.
\newblock Artificial socialization? how artificial intelligence applications can shape a new era of employee onboarding practices.
\newblock In {\em Hawaii International Conference on System Sciences 2023 ({HICSS}-56)}, pages 155--164. {IEEE} Computer Society, 2023.

\bibitem{azanza_can_2024}
Maider Azanza, Juanan Pereira, Arantza Irastorza, and Aritz Galdos.
\newblock Can {LLMs} facilitate onboarding software developers? an ongoing industrial case study.
\newblock In {\em 2024 36th International Conference on Software Engineering Education and Training ({CSEE}\&T)}, pages 1--6, 2024.
\newblock {ISSN}: 2377-570X.

\bibitem{cristian_ionescu_multi-agent_2025}
Andrei Cristian~Ionescu, Sergey Titov, and Maliheh Izadi.
\newblock A multi-agent onboarding assistant based on large language models, retrieval augmented generation, and chain-of-thought, 2025.
\newblock {ADS} Bibcode: 2025arXiv250323421C.

\bibitem{anthropic_model_2024}
{Anthropic}.
\newblock Model context protocol ({MCP}), 2024.
\newblock Disponible en: \url{https://docs.anthropic.com/en/docs/agents-and-tools/mcp}.

\bibitem{zhu_retrieving_2021}
Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua.
\newblock Retrieving and reading: A comprehensive survey on open-domain question answering.
\newblock {\em {CoRR}}, abs/2101.00774, 2021.

\bibitem{khattab_demonstrate-search-predict_2022}
Omar Khattab, Keshav Santhanam, Xiang~Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia.
\newblock Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive {NLP}.
\newblock {\em {arXiv} preprint {arXiv}:2212.14024}, 2022.

\bibitem{shao_enhancing_2023}
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen.
\newblock Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, {\em Findings of the Association for Computational Linguistics: {EMNLP} 2023}, pages 9248--9274. Association for Computational Linguistics, 2023.

\bibitem{qi-etal-2021-answering}
Peng Qi, Haejun Lee, Tg~Sido, and Christopher Manning.
\newblock Answering open-domain questions of varying reasoning steps from text.
\newblock In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, {\em Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 3599--3614, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

\bibitem{zheng_take_2023}
Huaixiu~Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed~H. Chi, Quoc~V. Le, and Denny Zhou.
\newblock Take a step back: Evoking reasoning via abstraction in large language models.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{trivedi_interleaving_2023}
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.
\newblock Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 10014--10037. Association for Computational Linguistics, 2023.

\bibitem{wang_survey_2024}
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhi-Yuan Chen, Jiakai Tang, Xu~Chen, Yankai Lin, Wayne~Xin Zhao, Zhewei Wei, and Jirong Wen.
\newblock A survey on large language model based autonomous agents.
\newblock {\em Frontiers Comput. Sci.}, 2024.

\bibitem{zhang_building_2023}
Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua~B. Tenenbaum, Tianmin Shu, and Chuang Gan.
\newblock Building cooperative embodied agents modularly with large language models.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{fischer_reflective_2023}
Kevin~A. Fischer.
\newblock Reflective linguistic programming ({RLP}): A stepping stone in socially-aware {AGI} ({SocialAGI}), 2023.

\bibitem{liang_unleashing_2023}
Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu~Lu, Zejun Ma, and Zhoujun Li.
\newblock {\em Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System}.
\newblock 2023.

\bibitem{zhao_expel_2024}
Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang.
\newblock {ExpeL}: {LLM} agents are experiential learners.
\newblock In {\em Proceedings of the Thirty-Eighth {AAAI} Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence}, {AAAI}'24/{IAAI}'24/{EAAI}'24. {AAAI} Press, 2024.

\bibitem{zhong_memorybank_2024}
Wanjun Zhong, Lianghong Guo, Qiqi Gao, He~Ye, and Yanlin Wang.
\newblock {MemoryBank}: Enhancing large language models with long-term memory.
\newblock {\em Proceedings of the {AAAI} Conference on Artificial Intelligence}, 38(17):19724--19731, 2024.
\newblock Number: 17.

\bibitem{park_generative_2023}
Joon~Sung Park, Joseph O'Brien, Carrie~Jun Cai, Meredith~Ringel Morris, Percy Liang, and Michael~S. Bernstein.
\newblock Generative agents: Interactive simulacra of human behavior.
\newblock In {\em Proceedings of the 36th Annual {ACM} Symposium on User Interface Software and Technology}, {UIST} '23, pages 1--22. Association for Computing Machinery, 2023.

\bibitem{wei_chain--thought_2023}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed~Chi, Quoc~V Le, and Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, editors, {\em Advances in Neural Information Processing Systems}, volume~35, pages 24824--24837. Curran Associates, Inc., 2022.

\bibitem{yao_tree_2023}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.
\newblock Tree of thoughts: Deliberate problem solving with large language models.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, {\em Advances in Neural Information Processing Systems}, volume~36, pages 11809--11822. Curran Associates, Inc., 2023.

\bibitem{wang_recmind_2024}
Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, and Yingzhen Yang.
\newblock {RecMind}: Large language model powered agent for recommendation, 2024.

\bibitem{shinn_reflexion_2023}
Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.
\newblock Reflexion: language agents with verbal reinforcement learning.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, {\em Advances in Neural Information Processing Systems}, volume~36, pages 8634--8652. Curran Associates, Inc., 2023.

\bibitem{madaan_self-refine_2023}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa~Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, {\em Advances in Neural Information Processing Systems}, volume~36, pages 46534--46594. Curran Associates, Inc., 2023.

\bibitem{miao_selfcheck_2023}
Ning Miao, Yee~Whye Teh, and Tom Rainforth.
\newblock {SelfCheck}: Using {LLMs} to zero-shot check their own step-by-step reasoning.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{lin_swiftsage_2023}
Bill~Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang Ren.
\newblock {SwiftSage}: A generative agent with fast and slow thinking for complex interactive tasks.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, {\em Advances in Neural Information Processing Systems}, volume~36, pages 23813--23825. Curran Associates, Inc., 2023.

\bibitem{huang_language_2022}
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
\newblock Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.
\newblock In {\em International Conference on Machine Learning}, pages 9118--9147. {PMLR}, 2022.
\newblock {ISSN}: 2640-3498.

\bibitem{wang_describe_2023}
Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang.
\newblock Describe, explain, plan and select: Interactive planning with {LLMs} enables open-world multi-task agents.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{zhu_ghost_2023}
Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu~Qiao, Zhaoxiang Zhang, and Jifeng Dai.
\newblock Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory, 2023.

\bibitem{song_llm-planner_2023}
Chan~Hee Song, Jiaman Wu, Clayton Washington, Brian~M. Sadler, Wei-Lun Chao, and Yu~Su.
\newblock {LLM}-planner: Few-shot grounded planning for embodied agents with large language models.
\newblock In {\em Proceedings of the {IEEE}/{CVF} International Conference on Computer Vision}, pages 2998--3009, 2023.

\bibitem{wang_voyager_2023}
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.
\newblock Voyager: An open-ended embodied agent with large language models.
\newblock {\em Transactions on Machine Learning Research}, 2023.

\bibitem{liu_odyssey_2025}
Shunyu Liu, Yaoru Li, Kongcheng Zhang, Zhenyu Cui, Wenkai Fang, Yuxuan Zheng, Tongya Zheng, and Mingli Song.
\newblock Odyssey: Empowering minecraft agents with open-world skills, 2025.

\bibitem{deepseek-ai_deepseek-r1_2025}
{DeepSeek-AI}.
\newblock {DeepSeek}-r1: Incentivizing reasoning capability in {LLMs} via reinforcement learning, 2025.
\newblock \_eprint: 2501.12948.

\bibitem{karpas_mrkl_2022}
Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, and Moshe Tenenholtz.
\newblock {MRKL} systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning, 2022.

\bibitem{ge_openagi_2023}
Yingqiang Ge, Wenyue Hua, Kai Mei, jianchao ji, Juntao Tan, Shuyuan Xu, Zelong Li, and Yongfeng Zhang.
\newblock {OpenAGI}: When {LLM} meets domain experts.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, {\em Advances in Neural Information Processing Systems}, volume~36, pages 5539--5568. Curran Associates, Inc., 2023.

\bibitem{zhuge_mindstorms_2025}
Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan~R. Ashley, R√≥bert Csord√°s, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al~Kader Hammoud, Vincent Herrmann, Kazuki Irie, Louis Kirsch, Bing Li, Guohao Li, Shuming Liu, Jinjie Mai, Piotr Piƒôkos, Aditya~A. Ramesh, Imanol Schlag, Weimin Shi, Aleksandar Staniƒá, Wenyi Wang, Yuhui Wang, Mengmeng Xu, Deng-Ping Fan, Bernard Ghanem, and J√ºrgen Schmidhuber.
\newblock Mindstorms in natural language-based societies of mind.
\newblock {\em Computational Visual Media}, 11(1):29--81, 2025.

\bibitem{du_improving_2024}
Yilun Du, Shuang Li, Antonio Torralba, Joshua~B. Tenenbaum, and Igor Mordatch.
\newblock Improving factuality and reasoning in language models through multiagent debate.
\newblock In {\em Forty-first International Conference on Machine Learning}, 2024.

\bibitem{hong_metagpt_2024}
Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka~Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and J√ºrgen Schmidhuber.
\newblock {MetaGPT}: Meta programming for a multi-agent collaborative framework.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{jimenez_swe-bench_2023}
Carlos~E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik~R. Narasimhan.
\newblock {SWE}-bench: Can language models resolve real-world github issues?
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{liu_agentbench_2023}
Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu~Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu~Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang.
\newblock {AgentBench}: Evaluating {LLMs} as agents.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{zhou_webarena_2023}
Shuyan Zhou, Frank~F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig.
\newblock {WebArena}: A realistic web environment for building autonomous agents.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{qin_toolllm_2023}
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun.
\newblock {ToolLLM}: Facilitating large language models to master 16000+ real-world {APIs}.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{li_api-bank_2023}
Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li.
\newblock {API}-bank: A comprehensive benchmark for tool-augmented {LLMs}.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, {\em Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 3102--3116. Association for Computational Linguistics, 2023.

\bibitem{song_restgpt_2023}
Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke~Wang, Rong Yao, Ye~Tian, and Sujian Li.
\newblock {RestGPT}: Connecting large language models with real-world {RESTful} {APIs}, 2023.

\bibitem{kalliamvakou_research_2022}
Eirini Kalliamvakou.
\newblock Research: quantifying {GitHub} copilot‚Äôs impact on developer productivity and happiness.
\newblock {\em The {GitHub} Blog}, 2022.
\newblock Disponible en: \url{https://github.blog/news-insights/research/research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/}.

\bibitem{sourcegraph2024cody}
Sourcegraph.
\newblock How cody understands your codebase, 2024.
\newblock Disponible en: \url{https://sourcegraph.com/blog/how-cody-understands-your-codebase}.

\bibitem{gauthier_building_2023}
Paul Gauthier.
\newblock Building a better repository map with tree sitter, 2023.
\newblock Disponible en: \url{https://aider.chat/2023/10/22/repomap.html}.

\bibitem{noauthor_number_2024}
Josh Howarth.
\newblock Number of parameters in {GPT}-4 (latest data), 2024.
\newblock Disponible en: \url{https://explodingtopics.com/blog/gpt-parameters}.

\bibitem{chen_fireact_2023}
Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao.
\newblock {FireAct}: Toward language agent fine-tuning, 2023.

\bibitem{zeng_agenttuning_2024}
Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang.
\newblock {AgentTuning}: Enabling generalized agent abilities for {LLMs}.
\newblock In {\em Findings of the Association for Computational Linguistics {ACL} 2024}, pages 3053--3077. Association for Computational Linguistics, 2024.

\bibitem{chen_agent-flan_2024}
Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao.
\newblock Agent-{FLAN}: Designing data and methods of effective agent tuning for large language models.
\newblock In {\em Findings of the Association for Computational Linguistics {ACL} 2024}, pages 9354--9366. Association for Computational Linguistics, 2024.

\bibitem{brown_c4_2018}
Simon Brown.
\newblock The c4 model for software architecture, 2018.
\newblock Publisher: {InfoQ}. Disponible en: \url{https://www.infoq.com/articles/C4-architecture-model}.

\bibitem{luo_repoagent_2024}
Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, Xiaoyin Che, Zhiyuan Liu, and Maosong Sun.
\newblock {RepoAgent}: An {LLM}-powered open-source framework for repository-level code documentation generation.
\newblock In Delia~Irazu Hernandez~Farias, Tom Hope, and Manling Li, editors, {\em Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pages 436--464. Association for Computational Linguistics, 2024.

\bibitem{tam_let_2024}
Zhi~Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-Yen Lin, Hung-yi Lee, and Yun-Nung Chen.
\newblock Let me speak freely? a study on the impact of format restrictions on large language model performance.
\newblock In Franck Dernoncourt, Daniel Preo≈£iuc-Pietro, and Anastasia Shimorina, editors, {\em {EMNLP} 2024}, pages 1218--1236. Association for Computational Linguistics, 2024.

\bibitem{brown_language_2020}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam {McCandlish}, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin, editors, {\em Advances in Neural Information Processing Systems}, volume~33, pages 1877--1901. Curran Associates, Inc., 2020.

\bibitem{jeong_adaptive-rag_2024}
Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung~Ju Hwang, and Jong Park.
\newblock Adaptive-{RAG}: Learning to adapt retrieval-augmented large language models through question complexity.
\newblock In Kevin Duh, Helena Gomez, and Steven Bethard, editors, {\em Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 7036--7050. Association for Computational Linguistics, 2024.

\bibitem{wei_eda_2019}
Jason Wei and Kai Zou.
\newblock {EDA}: Easy data augmentation techniques for boosting performance on text classification tasks.
\newblock In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, {\em Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})}, pages 6382--6388. Association for Computational Linguistics, 2019.

\bibitem{liu_roberta_2019}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock {RoBERTa}: A robustly optimized {BERT} pretraining approach, 2019.

\bibitem{gutierrez-fandino_maria_2022}
Asier Guti√©rrez-Fandi√±o, Jordi Armengol-Estap√©, Marc P√†mies, Joan Llop-Palao, Joaquin Silveira-Ocampo, Casimiro~Pio Carrino, Carme Armentano-Oller, Carlos Rodriguez-Penagos, Aitor Gonzalez-Agirre, and Marta Villegas.
\newblock {MarIA}: Spanish language models.
\newblock {\em Procesamiento del Lenguaje Natural}, 68(0):39--60, 2022.
\newblock Number: 0.

\bibitem{mistral_codestral_2025}
{Mistral AI}.
\newblock Codestral embed {\textbar} mistral {AI}.
\newblock Disponible en: \url{https://mistral.ai/news/codestral-embed}.

\bibitem{khattab_relevance-guided_2021}
Omar Khattab, Christopher Potts, and Matei Zaharia.
\newblock Relevance-guided supervision for {OpenQA} with {ColBERT}.
\newblock {\em Transactions of the Association for Computational Linguistics}, 9:929--944, 2021.

\bibitem{xiong_approximate_2020}
Lee Xiong, Chenyan Xiong, Ye~Li, Kwok-Fung Tang, Jialin Liu, Paul~N. Bennett, Junaid Ahmed, and Arnold Overwijk.
\newblock Approximate nearest neighbor negative contrastive learning for dense text retrieval.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{yu_augmentation-adapted_2023}
Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu.
\newblock Augmentation-adapted retriever improves generalization of language models as generic plug-in.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 2421--2436. Association for Computational Linguistics, 2023.

\end{thebibliography}
