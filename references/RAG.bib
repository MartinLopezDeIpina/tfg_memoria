
@misc{fischer_reflective_2023,
	title = {Reflective Linguistic Programming ({RLP}): A Stepping Stone in Socially-Aware {AGI} ({SocialAGI})},
	url = {http://arxiv.org/abs/2305.12647},
	doi = {10.48550/arXiv.2305.12647},
	shorttitle = {Reflective Linguistic Programming ({RLP})},
	abstract = {This paper presents Reﬂective Linguistic Programming ({RLP}), a unique approach to conversational {AI} that emphasizes self-awareness and strategic planning. {RLP} encourages models to introspect on their own predeﬁned personality traits, emotional responses to incoming messages, and planned strategies, enabling contextually rich, coherent, and engaging interactions. A striking illustration of {RLP}’s potential involves a toy example, an {AI} persona with an adversarial orientation, a demon named ‘Bogus’ inspired by the children’s fairy tale Hansel \& Gretel. Bogus exhibits sophisticated behaviors, such as strategic deception and sensitivity to user discomfort, that spontaneously arise from the model’s introspection and strategic planning. These behaviors are not pre-programmed or prompted, but emerge as a result of the model’s advanced cognitive modeling. The potential applications of {RLP} in socially-aware {AGI} (Social {AGI}) are vast, from nuanced negotiations and mental health support systems to the creation of diverse and dynamic {AI} personas. Our exploration of deception serves as a stepping stone towards a new frontier in {AGI}, one ﬁlled with opportunities for advanced cognitive modeling and the creation of truly human ‘digital souls’.},
	number = {{arXiv}:2305.12647},
	publisher = {{arXiv}},
	author = {Fischer, Kevin A.},
	urldate = {2025-03-04},
	date = {2023-05-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2305.12647 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {PDF:/home/martin/Zotero/storage/ZKUMXSLY/Fischer - 2023 - Reflective Linguistic Programming (RLP) A Stepping Stone in Socially-Aware AGI (SocialAGI).pdf:application/pdf},
}

@online{noauthor_httpssimgbaaiaccnpaperfile25a43194-c74c-4cd3-b60f-0a1f27f8b8afpdf_nodate,
	title = {https://simg.baai.ac.cn/paperfile/25a43194-c74c-4cd3-b60f-0a1f27f8b8af.pdf},
	url = {https://simg.baai.ac.cn/paperfile/25a43194-c74c-4cd3-b60f-0a1f27f8b8af.pdf},
	urldate = {2025-03-04},
	file = {https\://simg.baai.ac.cn/paperfile/25a43194-c74c-4cd3-b60f-0a1f27f8b8af.pdf:/home/martin/Zotero/storage/8ZCP7REC/25a43194-c74c-4cd3-b60f-0a1f27f8b8af.pdf:application/pdf},
}

@misc{gao_retrieval-augmented_2024,
	title = {Retrieval-Augmented Generation for Large Language Models: A Survey},
	url = {http://arxiv.org/abs/2312.10997},
	doi = {10.48550/arXiv.2312.10997},
	shorttitle = {Retrieval-Augmented Generation for Large Language Models},
	abstract = {Large Language Models ({LLMs}) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation ({RAG}) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. {RAG} synergistically merges {LLMs}' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of {RAG} paradigms, encompassing the Naive {RAG}, the Advanced {RAG}, and the Modular {RAG}. It meticulously scrutinizes the tripartite foundation of {RAG} frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in {RAG} systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
	number = {{arXiv}:2312.10997},
	publisher = {{arXiv}},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	urldate = {2025-03-04},
	date = {2024-03-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2312.10997 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {PDF:/home/martin/Zotero/storage/J9W8RP5P/Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language Models A Survey.pdf:application/pdf},
}

@misc{wang_knowledgpt_2023,
	title = {{KnowledGPT}: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases},
	url = {http://arxiv.org/abs/2308.11761},
	doi = {10.48550/arXiv.2308.11761},
	shorttitle = {{KnowledGPT}},
	abstract = {Large language models ({LLMs}) have demonstrated impressive impact in the field of natural language processing, but they still struggle with several issues regarding, such as completeness, timeliness, faithfulness and adaptability. While recent efforts have focuses on connecting {LLMs} with external knowledge sources, the integration of knowledge bases ({KBs}) remains understudied and faces several challenges. In this paper, we introduce {KnowledGPT}, a comprehensive framework to bridge {LLMs} with various knowledge bases, facilitating both the retrieval and storage of knowledge. The retrieval process employs the program of thought prompting, which generates search language for {KBs} in code format with pre-defined functions for {KB} operations. Besides retrieval, {KnowledGPT} offers the capability to store knowledge in a personalized {KB}, catering to individual user demands. With extensive experiments, we show that by integrating {LLMs} with {KBs}, {KnowledGPT} properly answers a broader range of questions requiring world knowledge compared with vanilla {LLMs}, utilizing both knowledge existing in widelyknown {KBs} and extracted into personalized {KBs}.},
	number = {{arXiv}:2308.11761},
	publisher = {{arXiv}},
	author = {Wang, Xintao and Yang, Qianwen and Qiu, Yongting and Liang, Jiaqing and He, Qianyu and Gu, Zhouhong and Xiao, Yanghua and Wang, Wei},
	urldate = {2025-03-04},
	date = {2023-08-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2308.11761 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {PDF:/home/martin/Zotero/storage/IXF4XF9J/Wang et al. - 2023 - KnowledGPT Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases.pdf:application/pdf},
}

@misc{modarressi_ret-llm_2024,
	title = {{RET}-{LLM}: Towards a General Read-Write Memory for Large Language Models},
	url = {http://arxiv.org/abs/2305.14322},
	doi = {10.48550/arXiv.2305.14322},
	shorttitle = {{RET}-{LLM}},
	abstract = {Large language models ({LLMs}) have significantly advanced the field of natural language processing ({NLP}) through their extensive parameters and comprehensive data utilization. However, existing {LLMs} lack a dedicated memory unit, limiting their ability to explicitly store and retrieve knowledge for various tasks. In this paper, we propose {RET}-{LLM} a novel framework that equips {LLMs} with a general write-read memory unit, allowing them to extract, store, and recall knowledge from the text as needed for task performance. Inspired by Davidsonian semantics theory, we extract and save knowledge in the form of triplets. The memory unit is designed to be scalable, aggregatable, updatable, and interpretable. Through qualitative evaluations, we demonstrate the superiority of our proposed framework over baseline approaches in question answering tasks. Moreover, our framework exhibits robust performance in handling temporal-based question answering tasks, showcasing its ability to effectively manage time-dependent information.},
	number = {{arXiv}:2305.14322},
	publisher = {{arXiv}},
	author = {Modarressi, Ali and Imani, Ayyoob and Fayyaz, Mohsen and Schütze, Hinrich},
	urldate = {2025-03-04},
	date = {2024-10-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2305.14322 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/martin/Zotero/storage/2P4W7S7S/Modarressi et al. - 2024 - RET-LLM Towards a General Read-Write Memory for Large Language Models.pdf:application/pdf},
}

@misc{trivedi_interleaving_2023,
	title = {Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions},
	url = {http://arxiv.org/abs/2212.10509},
	doi = {10.48550/arXiv.2212.10509},
	abstract = {Prompting-based large language models ({LLMs}) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts ({CoT}) for multi-step question answering ({QA}). They struggle, however, when the necessary knowledge is either unavailable to the {LLM} or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps {LLMs}, we observe that this one-step retrieve-and-read approach is insufficient for multi-step {QA}. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose {IRCoT}, a new approach for multi-step {QA} that interleaves retrieval with steps (sentences) in a {CoT}, guiding the retrieval with {CoT} and in turn using retrieved results to improve {CoT}. Using {IRCoT} with {GPT}3 substantially improves retrieval (up to 21 points) as well as downstream {QA} (up to 15 points) on four datasets: {HotpotQA}, 2WikiMultihopQA, {MuSiQue}, and {IIRC}. We observe similar substantial gains in out-ofdistribution ({OOD}) settings as well as with much smaller models such as Flan-T5-large without additional training. {IRCoT} reduces model hallucination, resulting in factually more accurate {CoT} reasoning.1.},
	number = {{arXiv}:2212.10509},
	publisher = {{arXiv}},
	author = {Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
	urldate = {2025-03-03},
	date = {2023-06-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2212.10509 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/martin/Zotero/storage/L2E6KQU4/Trivedi et al. - 2023 - Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions.pdf:application/pdf},
}

@inproceedings{feng_retrieval-generation_2024,
	title = {Retrieval-Generation Synergy Augmented Large Language Models},
	url = {https://ieeexplore.ieee.org/document/10448015/},
	doi = {10.1109/ICASSP48485.2024.10448015},
	abstract = {Large language models augmented with task-relevant documents have demonstrated impressive performance on knowledge-intensive tasks. However, regarding how to obtain effective documents, the existing methods are mainly divided into two categories. One is to retrieve from an external knowledge base, and the other is to utilize large language models to generate documents. We propose an iterative retrieval-generation collaborative framework. It is not only able to leverage both parametric and non-parametric knowledge, but also helps to find the correct reasoning path through retrieval-generation interactions, which is very important for tasks that require multi-step reasoning. We conduct experiments on four question answering datasets, including single-hop {QA} and multi-hop {QA} tasks. Empirical results show that our method significantly improves the reasoning ability of large language models and outperforms previous baselines.},
	eventtitle = {{ICASSP} 2024 - 2024 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {11661--11665},
	booktitle = {{ICASSP} 2024 - 2024 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Feng, Zhangyin and Feng, Xiaocheng and Zhao, Dezhi and Yang, Maojin and Qin, Bing},
	urldate = {2025-03-03},
	date = {2024-04},
	note = {{ISSN}: 2379-190X},
	keywords = {Cognition, Knowledge based systems, Acoustics, Collaboration, Iterative methods, large language models, question answering, Question answering (information retrieval), retrieval augmented, Signal processing},
	file = {IEEE Xplore Abstract Record:/home/martin/Zotero/storage/MHUSH6EH/10448015.html:text/html;Submitted Version:/home/martin/Zotero/storage/DQB8JTLZ/Feng et al. - 2024 - Retrieval-Generation Synergy Augmented Large Language Models.pdf:application/pdf},
}

@misc{li_classification_2023,
	title = {From Classification to Generation: Insights into Crosslingual Retrieval Augmented {ICL}},
	url = {http://arxiv.org/abs/2311.06595},
	doi = {10.48550/arXiv.2311.06595},
	shorttitle = {From Classification to Generation},
	abstract = {The remarkable ability of Large Language Models ({LLMs}) to understand and follow instructions has sometimes been limited by their in-context learning ({ICL}) performance in low-resource languages. To address this, we introduce a novel approach that leverages cross-lingual retrieval-augmented in-context learning ({CREA}-{ICL}). By extracting semantically similar prompts from high-resource languages, we aim to improve the zero-shot performance of multilingual pre-trained language models ({MPLMs}) across diverse tasks. Though our approach yields steady improvements in classification tasks, it faces challenges in generation tasks. Our evaluation offers insights into the performance dynamics of retrieval-augmented in-context learning across both classification and generation domains.},
	number = {{arXiv}:2311.06595},
	publisher = {{arXiv}},
	author = {Li, Xiaoqian and Nie, Ercong and Liang, Sheng},
	urldate = {2025-03-03},
	date = {2023-12-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2311.06595 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/martin/Zotero/storage/API3KY5J/Li et al. - 2023 - From Classification to Generation Insights into Crosslingual Retrieval Augmented ICL.pdf:application/pdf},
}

@article{lin_ra-dit_2024,
	title = {{RA}-{DIT}: {RETRIEVAL}-{AUGMENTED} {DUAL} {INSTRUC}- {TION} {TUNING}},
	abstract = {Retrieval-augmented language models ({RALMs}) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to {LM} pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning ({RA}-{DIT}), a lightweight fine-tuning methodology that provides a third option by retrofitting any {LLM} with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained {LM} to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the {LM}. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, {RA}-{DIT} 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context {RALM} approaches by up to +8.9\% in 0-shot setting and +1.4\% in 5-shot setting on average.},
	author = {Lin, Xi Victoria and Chen, Xilun and Chen, Mingda and Shi, Weijia and Lomeli, Maria and James, Rich and Rodriguez, Pedro and Kahn, Jacob and Szilvasy, Gergely and Lewis, Mike and Zettlemoyer, Luke and Yih, Scott},
	date = {2024},
	langid = {english},
	file = {PDF:/home/martin/Zotero/storage/M9AXRI65/Lin et al. - 2024 - RA-DIT RETRIEVAL-AUGMENTED DUAL INSTRUC- TION TUNING.pdf:application/pdf},
}

@misc{guu_realm_2020,
	title = {{REALM}: Retrieval-Augmented Language Model Pre-Training},
	url = {http://arxiv.org/abs/2002.08909},
	doi = {10.48550/arXiv.2002.08909},
	shorttitle = {{REALM}},
	abstract = {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for {NLP} tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pretraining with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, ﬁne-tuning and inference. For the ﬁrst time, we show how to pretrain such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training ({REALM}) by ﬁne-tuning on the challenging task of Open-domain Question Answering (Open-{QA}). We compare against state-of-theart models for both explicit and implicit knowledge storage on three popular Open-{QA} benchmarks, and ﬁnd that we outperform all previous methods by a signiﬁcant margin (4-16\% absolute accuracy), while also providing qualitative beneﬁts such as interpretability and modularity.},
	number = {{arXiv}:2002.08909},
	publisher = {{arXiv}},
	author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
	urldate = {2025-03-03},
	date = {2020-02-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2002.08909 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/home/martin/Zotero/storage/U5AELYKD/Guu et al. - 2020 - REALM Retrieval-Augmented Language Model Pre-Training.pdf:application/pdf},
}

@article{borgeaud_improving_nodate,
	title = {Improving Language Models by Retrieving from Trillions of Tokens},
	abstract = {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer ({RETRO}) obtains comparable performance to {GPT}-3 and Jurassic-1 on the Pile, despite using 25× fewer parameters. After ﬁne-tuning, {RETRO} performance translates to downstream knowledge-intensive tasks such as question answering. {RETRO} combines a frozen {BERT} retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train {RETRO} from scratch, yet can also rapidly {RETROﬁt} pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.},
	author = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie},
	langid = {english},
	file = {PDF:/home/martin/Zotero/storage/HPVJJ367/Borgeaud et al. - Improving Language Models by Retrieving from Trillions of Tokens.pdf:application/pdf},
}

@misc{kang_knowledge_2023,
	title = {Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation},
	url = {http://arxiv.org/abs/2305.18846},
	doi = {10.48550/arXiv.2305.18846},
	abstract = {Language models have achieved impressive performances on dialogue generation tasks. However, when generating responses for a conversation that requires factual knowledge, they are far from perfect, due to an absence of mechanisms to retrieve, encode, and reflect the knowledge in the generated responses. Some knowledgegrounded dialogue generation methods tackle this problem by leveraging facts from Knowledge Graphs ({KGs}); however, they do not guarantee that the model utilizes a relevant piece of knowledge from the {KG}. To overcome this limitation, we propose {SUbgraph} Retrieval-augmented {GEneration} ({SURGE}), a framework for generating context-relevant and knowledge-grounded dialogues with the {KG}. Specifically, our {SURGE} framework first retrieves the relevant subgraph from the {KG}, and then enforces consistency across facts by perturbing their word embeddings conditioned by the retrieved subgraph. Then, we utilize contrastive learning to ensure that the generated texts have high similarity to the retrieved subgraphs. We validate our {SURGE} framework on {OpendialKG} and {KOMODIS} datasets, showing that it generates high-quality dialogues that faithfully reflect the knowledge from {KG}.},
	number = {{arXiv}:2305.18846},
	publisher = {{arXiv}},
	author = {Kang, Minki and Kwak, Jin Myung and Baek, Jinheon and Hwang, Sung Ju},
	urldate = {2025-03-03},
	date = {2023-05-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2305.18846 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {PDF:/home/martin/Zotero/storage/QUVPGSVQ/Kang et al. - 2023 - Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation.pdf:application/pdf},
}

@article{cheng_lift_nodate,
	title = {Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory},
	abstract = {With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation (we define this as primal problem). The traditional approach for memory retrieval involves selecting memory that exhibits the highest similarity to the input. However, this method is constrained by the quality of the fixed corpus from which memory is retrieved. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a novel framework, Selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round. This enables the model to leverage its own output, referred to as self-memory, for improved generation. We evaluate the effectiveness of Selfmem on three distinct text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation, under two generation paradigms: fine-tuned small model and few-shot {LLM}. Our approach achieves state-of-the-art results in four directions in {JRC}-Acquis translation dataset, 50.3 {ROUGE}-1 in {XSum}, and 62.9 {ROUGE}-1 in {BigPatent}, demonstrating the potential of self-memory in enhancing retrieval-augmented generation models. Furthermore, we conduct thorough analyses of each component in the Selfmem framework to identify current system bottlenecks and provide insights for future research1.},
	author = {Cheng, Xin and Luo, Di and Chen, Xiuying and Liu, Lemao and Zhao, Dongyan and Yan, Rui},
	langid = {english},
	file = {PDF:/home/martin/Zotero/storage/JZ3D7XNV/Cheng et al. - Lift Yourself Up Retrieval-augmented Text Generation with Self-Memory.pdf:application/pdf},
}

@inproceedings{ma_large_2023,
	title = {Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!},
	url = {http://arxiv.org/abs/2303.08559},
	doi = {10.18653/v1/2023.findings-emnlp.710},
	abstract = {Large Language Models ({LLMs}) have made remarkable strides in various tasks. Whether {LLMs} are competitive few-shot solvers for information extraction ({IE}) tasks, however, remains an open problem. In this work, we aim to provide a thorough answer to this question. Through extensive experiments on nine datasets across four {IE} tasks, we demonstrate that current advanced {LLMs} consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned {SLMs} under most settings. Therefore, we conclude that {LLMs} are not effective few-shot information extractors in general 1. Nonetheless, we illustrate that with appropriate prompting strategies, {LLMs} can effectively complement {SLMs} and tackle challenging samples that {SLMs} struggle with. And moreover, we propose an adaptive filter-thenrerank paradigm to combine the strengths of {LLMs} and {SLMs}. In this paradigm, {SLMs} serve as filters and {LLMs} serve as rerankers. By prompting {LLMs} to rerank a small portion of difficult samples identified by {SLMs}, our preliminary system consistently achieves promising improvements (2.4\% F1-gain on average) on various {IE} tasks, with an acceptable time and cost investment. Our code is available at https://github.com/mayubo2333/{LLM}-{IE}.},
	pages = {10572--10601},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2023},
	author = {Ma, Yubo and Cao, Yixin and Hong, {YongChing} and Sun, Aixin},
	urldate = {2025-03-03},
	date = {2023},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2303.08559 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {PDF:/home/martin/Zotero/storage/5MWRULD9/Ma et al. - 2023 - Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samp.pdf:application/pdf},
}

@misc{luo_augmented_2023,
	title = {Augmented Large Language Models with Parametric Knowledge Guiding},
	url = {http://arxiv.org/abs/2305.04757},
	doi = {10.48550/arXiv.2305.04757},
	abstract = {Large Language Models ({LLMs}) have signiﬁcantly advanced natural language processing ({NLP}) with their impressive language understanding and generation capabilities. However, their performance may be suboptimal for domain-speciﬁc tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art ({SOTA}) {LLMs}, which can only be accessed via {APIs}, impedes further ﬁne-tuning with domain custom data. Moreover, providing private data to the {LLMs}’ owner leads to data privacy problems. To address these challenges, we propose the novel Parametric Knowledge Guiding ({PKG}) framework, which equips {LLMs} with a knowledge-guiding module to access relevant knowledge without altering the {LLMs}’ parameters. Our {PKG} is based on open-source "white-box" language models, allowing ofﬂine memory of any knowledge that {LLMs} require. We demonstrate that our {PKG} framework can enhance the performance of "black-box" {LLMs} on a range of domain knowledge-intensive tasks that require factual (+7.9\%), tabular (+11.9\%), medical (+3.0\%), and multimodal (+8.1\%) knowledge.},
	number = {{arXiv}:2305.04757},
	publisher = {{arXiv}},
	author = {Luo, Ziyang and Xu, Can and Zhao, Pu and Geng, Xiubo and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
	urldate = {2025-03-02},
	date = {2023-05-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2305.04757 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {PDF:/home/martin/Zotero/storage/BRVQSI4C/Luo et al. - 2023 - Augmented Large Language Models with Parametric Knowledge Guiding.pdf:application/pdf},
}

@misc{berchansky_optimizing_2023,
	title = {Optimizing Retrieval-augmented Reader Models via Token Elimination},
	url = {http://arxiv.org/abs/2310.13682},
	doi = {10.48550/arXiv.2310.13682},
	abstract = {Fusion-in-Decoder ({FiD}) is an effective retrieval-augmented language model applied across a variety of open-domain tasks, such as question answering, fact checking, etc. In {FiD}, supporting passages are first retrieved and then processed using a generative model (Reader), which can cause a significant bottleneck in decoding time, particularly with long outputs. In this work, we analyze the contribution and necessity of all the retrieved passages to the performance of reader models, and propose eliminating some of the retrieved information, at the token level, that might not contribute essential information to the answer generation process. We demonstrate that our method can reduce run-time by up to 62.2\%, with only a 2\% reduction in performance, and in some cases, even improve the performance results.},
	number = {{arXiv}:2310.13682},
	publisher = {{arXiv}},
	author = {Berchansky, Moshe and Izsak, Peter and Caciularu, Avi and Dagan, Ido and Wasserblat, Moshe},
	urldate = {2025-03-02},
	date = {2023-11-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2310.13682 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {PDF:/home/martin/Zotero/storage/6VQVKPYK/Berchansky et al. - 2023 - Optimizing Retrieval-augmented Reader Models via Token Elimination.pdf:application/pdf},
}

@misc{yang_prca_2023,
	title = {{PRCA}: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter},
	url = {http://arxiv.org/abs/2310.18347},
	doi = {10.48550/arXiv.2310.18347},
	shorttitle = {{PRCA}},
	abstract = {The Retrieval Question Answering ({ReQA}) task employs the retrieval-augmented framework, composed of a retriever and generator. The generator formulates the answer based on the documents retrieved by the retriever. Incorporating Large Language Models ({LLMs}) as generators is beneficial due to their advanced {QA} capabilities, but they are typically too large to be fine-tuned with budget constraints while some of them are only accessible via {APIs}. To tackle this issue and further improve {ReQA} performance, we propose a trainable Pluggable Reward-Driven Contextual Adapter ({PRCA}), keeping the generator as a black box. Positioned between the retriever and generator in a Pluggable manner, {PRCA} refines the retrieved information by operating in a tokenautoregressive strategy via maximizing rewards of the reinforcement learning phase. Our experiments validate {PRCA}’s effectiveness in enhancing {ReQA} performance on three datasets by up to 20\% improvement to fit black-box {LLMs} into existing frameworks, demonstrating its considerable potential in the {LLMs} era.},
	number = {{arXiv}:2310.18347},
	publisher = {{arXiv}},
	author = {Yang, Haoyan and Li, Zhitao and Zhang, Yong and Wang, Jianzong and Cheng, Ning and Li, Ming and Xiao, Jing},
	urldate = {2025-03-02},
	date = {2023-10-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2310.18347 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/martin/Zotero/storage/GDXKQCC6/Yang et al. - 2023 - PRCA Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-.pdf:application/pdf},
}

@misc{xu_recomp_2023,
	title = {{RECOMP}: Improving Retrieval-Augmented {LMs} with Compression and Selective Augmentation},
	url = {http://arxiv.org/abs/2310.04408},
	doi = {10.48550/arXiv.2310.04408},
	shorttitle = {{RECOMP}},
	abstract = {Retrieving documents and prepending them in-context at inference time improves performance of language model ({LMs}) on a wide range of tasks. However, these documents, often spanning hundreds of words, make inference substantially more expensive. We propose compressing the retrieved documents into textual summaries prior to in-context integration. This not only reduces the computational costs but also relieves the burden of {LMs} to identify relevant information in long retrieved documents. We present two compressors -- an extractive compressor which selects useful sentences from retrieved documents and an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve {LMs}' performance on end tasks when the generated summaries are prepended to the {LMs}' input, while keeping the summary concise.If the retrieved documents are irrelevant to the input or offer no additional information to {LM}, our compressor can return an empty string, implementing selective augmentation.We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6\% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summarization models. We show that our compressors trained for one {LM} can transfer to other {LMs} on the language modeling task and provide summaries largely faithful to the retrieved documents.},
	number = {{arXiv}:2310.04408},
	publisher = {{arXiv}},
	author = {Xu, Fangyuan and Shi, Weijia and Choi, Eunsol},
	urldate = {2025-03-02},
	date = {2023-10-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2310.04408 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/martin/Zotero/storage/EIU4K7E8/Xu et al. - 2023 - RECOMP Improving Retrieval-Augmented LMs with Compression and Selective Augmentation.pdf:application/pdf},
}

@misc{shi_replug_2023,
	title = {{REPLUG}: Retrieval-Augmented Black-Box Language Models},
	url = {http://arxiv.org/abs/2301.12652},
	doi = {10.48550/arXiv.2301.12652},
	shorttitle = {{REPLUG}},
	abstract = {We introduce {REPLUG}, a retrieval-augmented language modeling framework that treats the language model ({LM}) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented {LMs} that train language models with special cross attention mechanisms to encode the retrieved text, {REPLUG} simply prepends retrieved documents to the input for the frozen black-box {LM}. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the {LM} can be used to supervise the retrieval model, which can then find documents that help the {LM} make better predictions. Our experiments demonstrate that {REPLUG} with the tuned retriever significantly improves the performance of {GPT}-3 (175B) on language modeling by 6.3\%, as well as the performance of Codex on five-shot {MMLU} by 5.1\%.},
	number = {{arXiv}:2301.12652},
	publisher = {{arXiv}},
	author = {Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
	urldate = {2025-03-01},
	date = {2023-05-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2301.12652 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/martin/Zotero/storage/6BG7UD8Q/Shi et al. - 2023 - REPLUG Retrieval-Augmented Black-Box Language Models.pdf:application/pdf},
}

@misc{yu_augmentation-adapted_2023,
	title = {Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In},
	url = {http://arxiv.org/abs/2305.17331},
	doi = {10.48550/arXiv.2305.17331},
	abstract = {Retrieval augmentation can aid language models ({LMs}) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the {LM}, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target {LMs} that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target {LMs}, we propose augmentation-adapted retriever ({AAR}), which learns {LM}’s preferences obtained from a known source {LM}. Experiments on the {MMLU} and {PopQA} datasets demonstrate that our {AAR} trained with a small source {LM} is able to significantly improve the zero-shot generalization of larger target {LMs} ranging from 250M Flan-T5 to 175B {InstructGPT}. Further analysis indicates that the preferences of different {LMs} overlap, enabling {AAR} trained with a single source {LM} to serve as a generic plug-in for various target {LMs}. Our code is open-sourced at https://github.com/{OpenMatch}/{AugmentationAdapted}-Retriever.},
	number = {{arXiv}:2305.17331},
	publisher = {{arXiv}},
	author = {Yu, Zichun and Xiong, Chenyan and Yu, Shi and Liu, Zhiyuan},
	urldate = {2025-03-01},
	date = {2023-05-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2305.17331 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/home/martin/Zotero/storage/RI2KWISV/Yu et al. - 2023 - Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In.pdf:application/pdf},
}

@misc{li_structure-aware_2023,
	title = {Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data},
	url = {http://arxiv.org/abs/2305.19912},
	doi = {10.48550/arXiv.2305.19912},
	abstract = {This paper presents Structure Aware {DeNse} {ReTrievAl} ({SANTA}) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. {SANTA} proposes two pretraining methods to make language models structureaware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains language models to represent multi-modal text data and teaches models to distinguish matched structured data for unstructured texts. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities. Our experiments show that {SANTA} achieves state-of-the-art on code search and product search and conducts convincing results in the zero-shot setting. {SANTA} learns tailored representations for multi-modal text data by aligning structured and unstructured data pairs and capturing structural semantics by masking and predicting entities in the structured data. All codes are available at https: //github.com/{OpenMatch}/{OpenMatch}.},
	number = {{arXiv}:2305.19912},
	publisher = {{arXiv}},
	author = {Li, Xinze and Liu, Zhenghao and Xiong, Chenyan and Yu, Shi and Gu, Yu and Liu, Zhiyuan and Yu, Ge},
	urldate = {2025-02-28},
	date = {2023-05-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2305.19912 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {PDF:/home/martin/Zotero/storage/62U2UUZW/Li et al. - 2023 - Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data.pdf:application/pdf},
}

@misc{zheng_take_2024,
	title = {Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models},
	url = {http://arxiv.org/abs/2310.06117},
	doi = {10.48550/arXiv.2310.06117},
	shorttitle = {Take a Step Back},
	abstract = {We present {STEP}-{BACK} {PROMPTING}, a simple prompting technique that enables {LLMs} to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, {LLMs} significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of {STEP}-{BACK} {PROMPTING} with {PaLM}-2L, {GPT}-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including {STEM}, Knowledge {QA}, and Multi-Hop Reasoning. For instance, {STEP}-{BACK} {PROMPTING} improves {PaLM}-2L performance on {MMLU} (Physics and Chemistry) by 7\% and 11\% respectively, {TimeQA} by 27\%, and {MuSiQue} by 7\%.},
	number = {{arXiv}:2310.06117},
	publisher = {{arXiv}},
	author = {Zheng, Huaixiu Steven and Mishra, Swaroop and Chen, Xinyun and Cheng, Heng-Tze and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
	urldate = {2025-02-28},
	date = {2024-03-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2310.06117 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {PDF:/home/martin/Zotero/storage/QQQH3T2M/Zheng et al. - 2024 - Take a Step Back Evoking Reasoning via Abstraction in Large Language Models.pdf:application/pdf},
}

@inproceedings{gao_precise_2023,
	location = {Toronto, Canada},
	title = {Precise Zero-Shot Dense Retrieval without Relevance Labels},
	url = {https://aclanthology.org/2023.acl-long.99},
	doi = {10.18653/v1/2023.acl-long.99},
	eventtitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages = {1762--1777},
	booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie},
	urldate = {2025-02-28},
	date = {2023},
	langid = {english},
	file = {PDF:/home/martin/Zotero/storage/J33MWIH7/Gao et al. - 2023 - Precise Zero-Shot Dense Retrieval without Relevance Labels.pdf:application/pdf},
}

@article{asai_self-rag_2024,
	title = {{SELF}-{RAG}: {LEARNING} {TO} {RETRIEVE}, {GENERATE}, {AND} {CRITIQUE} {THROUGH} {SELF}-{REFLECTION}},
	author = {Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
	date = {2024},
	langid = {english},
	file = {PDF:/home/martin/Zotero/storage/ZPU8G9HH/Asai et al. - 2024 - SELF-RAG LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION.pdf:application/pdf},
}

@misc{yoran_answering_2024,
	title = {Answering Questions by Meta-Reasoning over Multiple Chains of Thought},
	url = {http://arxiv.org/abs/2304.13007},
	doi = {10.48550/arXiv.2304.13007},
	abstract = {Modern systems for multi-hop question answering ({QA}) typically break questions into a sequence of reasoning steps, termed chain-ofthought ({CoT}), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce {MultiChain} Reasoning ({MCR}), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregate their answers. {MCR} examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. {MCR} outperforms strong baselines on 7 multi-hop {QA} datasets. Moreover, our analysis reveals that {MCR} explanations exhibit high quality, enabling humans to verify its answers.},
	number = {{arXiv}:2304.13007},
	publisher = {{arXiv}},
	author = {Yoran, Ori and Wolfson, Tomer and Bogin, Ben and Katz, Uri and Deutch, Daniel and Berant, Jonathan},
	urldate = {2025-02-27},
	date = {2024-08-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2304.13007 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {PDF:/home/martin/Zotero/storage/VC5PX4F3/Yoran et al. - 2024 - Answering Questions by Meta-Reasoning over Multiple Chains of Thought.pdf:application/pdf},
}

@misc{press_measuring_2023,
	title = {Measuring and Narrowing the Compositionality Gap in Language Models},
	url = {http://arxiv.org/abs/2210.03350},
	doi = {10.48550/arXiv.2210.03350},
	abstract = {We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the {GPT}-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.},
	number = {{arXiv}:2210.03350},
	publisher = {{arXiv}},
	author = {Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A. and Lewis, Mike},
	urldate = {2025-02-27},
	date = {2023-10-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2210.03350 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/martin/Zotero/storage/7PEWBJII/Press et al. - 2023 - Measuring and Narrowing the Compositionality Gap in Language Models.pdf:application/pdf},
}

@misc{khattab_demonstrate-search-predict_2023,
	title = {Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive {NLP}},
	url = {http://arxiv.org/abs/2212.14024},
	doi = {10.48550/arXiv.2212.14024},
	shorttitle = {Demonstrate-Search-Predict},
	abstract = {Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models ({LM}) and retrieval models ({RM}). Existing work has combined these in simple “retrievethen-read” pipelines in which the {RM} retrieves passages that are inserted into the {LM} prompt. To begin to fully realize the potential of frozen {LMs} and {RMs}, we propose {DEMONSTRATE}–{SEARCH}–{PREDICT} ({DSP}), a framework that relies on passing natural language texts in sophisticated pipelines between an {LM} and an {RM}. {DSP} can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the {LM} and {RM} can handle more reliably. We have written novel {DSP} programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art incontext learning results and delivering 37–120\%, 8–39\%, and 80–290\% relative gains against the vanilla {LM} ({GPT}-3.5), a standard retrieve-thenread pipeline, and a contemporaneous self-ask pipeline, respectively. We release {DSP} at https: //github.com/stanfordnlp/dsp.},
	number = {{arXiv}:2212.14024},
	publisher = {{arXiv}},
	author = {Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
	urldate = {2025-02-27},
	date = {2023-01-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2212.14024 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {PDF:/home/martin/Zotero/storage/LKZXUC6I/Khattab et al. - 2023 - Demonstrate-Search-Predict Composing retrieval and language models for knowledge-intensive NLP.pdf:application/pdf},
}

@misc{levine_standing_2022,
	title = {Standing on the Shoulders of Giant Frozen Language Models},
	url = {http://arxiv.org/abs/2204.10019},
	doi = {10.48550/arXiv.2204.10019},
	abstract = {Huge pretrained language models ({LMs}) have demonstrated surprisingly good zero-shot capabilities on a wide variety of tasks. This gives rise to the appealing vision of a single, versatile model with a wide range of functionalities across disparate applications. However, current leading techniques for leveraging a “frozen” {LM}—i.e., leaving its weights untouched—still often underperform ﬁne-tuning approaches which modify these weights in a task-dependent way. Those, in turn, suffer forgetfulness and compromise versatility, suggesting a tradeoff between performance and versatility. The main message of this paper is that current frozenmodel techniques such as prompt tuning are only the tip of the iceberg, and more powerful methods for leveraging frozen {LMs} can do just as well as ﬁne tuning in challenging domains without sacriﬁcing the underlying model’s versatility. To demonstrate this, we introduce three novel methods for leveraging frozen models: input-dependent prompt tuning, frozen readers, and recursive {LMs}, each of which vastly improves on current frozen-model approaches. Indeed, some of our methods even outperform ﬁne-tuning approaches in domains currently dominated by the latter. The computational cost of each method is higher than that of existing frozen model methods, but still negligible relative to a single pass through a huge frozen {LM}. Each of these methods constitutes a meaningful contribution in its own right, but by presenting these contributions together we aim to convince the reader of a broader message that goes beyond the details of any given method: that frozen models have untapped potential and that ﬁne-tuning is often unnecessary.},
	number = {{arXiv}:2204.10019},
	publisher = {{arXiv}},
	author = {Levine, Yoav and Dalmedigos, Itay and Ram, Ori and Zeldes, Yoel and Jannai, Daniel and Muhlgay, Dor and Osin, Yoni and Lieber, Opher and Lenz, Barak and Shalev-Shwartz, Shai and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
	urldate = {2025-02-27},
	date = {2022-04-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2204.10019 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {PDF:/home/martin/Zotero/storage/JDABXUWT/Levine et al. - 2022 - Standing on the Shoulders of Giant Frozen Language Models.pdf:application/pdf},
}

@article{khattab_baleen_nodate,
	title = {Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval},
	abstract = {Multi-hop reasoning (i.e., reasoning across two or more documents) is a key ingredient for {NLP} models that leverage large corpora to exhibit broad knowledge. To retrieve evidence passages, multi-hop models must contend with a fast-growing search space across the hops, represent complex queries that combine multiple information needs, and resolve ambiguity about the best order in which to hop between training passages. We tackle these problems via Baleen, a system that improves the accuracy of multi-hop retrieval while learning robustly from weak training signals in the many-hop setting. To tame the search space, we propose condensed retrieval, a pipeline that summarizes the retrieved passages after each hop into a single compact context. To model complex queries, we introduce a focused late interaction retriever that allows different parts of the same query representation to match disparate relevant passages. Lastly, to infer the hopping dependencies among unordered training passages, we devise latent hop ordering, a weak-supervision strategy in which the trained retriever itself selects the sequence of hops. We evaluate Baleen on retrieval for two-hop question answering and many-hop claim veriﬁcation, establishing state-of-the-art performance.},
	author = {Khattab, Omar and Potts, Christopher and Zaharia, Matei},
	langid = {english},
	file = {PDF:/home/martin/Zotero/storage/5JH82E3W/Khattab et al. - Baleen Robust Multi-Hop Reasoning at Scale via Condensed Retrieval.pdf:application/pdf},
}

@misc{khattab_relevance-guided_2021,
	title = {Relevance-guided Supervision for {OpenQA} with {ColBERT}},
	url = {http://arxiv.org/abs/2007.00814},
	doi = {10.48550/arXiv.2007.00814},
	abstract = {Systems for Open-Domain Question Answering ({OpenQA}) generally depend on a retriever for ﬁnding candidate passages in a large corpus and a reader for extracting answers from those passages. In much recent work, the retriever is a learned component that uses coarse-grained vector representations of questions and passages. We argue that this modeling choice is insufﬁciently expressive for dealing with the complexity of natural language questions. To address this, we deﬁne {ColBERT}-{QA}, which adapts the scalable neural retrieval model {ColBERT} to {OpenQA}. {ColBERT} creates ﬁne-grained interactions between questions and passages. We propose an efﬁcient weak supervision strategy that iteratively uses {ColBERT} to create its own training data. This greatly improves {OpenQA} retrieval on Natural Questions, {SQuAD}, and {TriviaQA}, and the resulting system attains state-of-the-art extractive {OpenQA} performance on all three datasets.},
	number = {{arXiv}:2007.00814},
	publisher = {{arXiv}},
	author = {Khattab, Omar and Potts, Christopher and Zaharia, Matei},
	urldate = {2025-02-26},
	date = {2021-08-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2007.00814 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {PDF:/home/martin/Zotero/storage/U9QYDVIP/Khattab et al. - 2021 - Relevance-guided Supervision for OpenQA with ColBERT.pdf:application/pdf},
}

@misc{qi_answering_2021,
	title = {Answering Open-Domain Questions of Varying Reasoning Steps from Text},
	url = {http://arxiv.org/abs/2010.12527},
	doi = {10.48550/arXiv.2010.12527},
	abstract = {We develop a uniﬁed system to answer directly from text open-domain questions that may require a varying number of retrieval steps. We employ a single multi-task transformer model to perform all the necessary subtasks—retrieving supporting facts, reranking them, and predicting the answer from all retrieved documents—in an iterative fashion. We avoid crucial assumptions of previous work that do not transfer well to real-world settings, including exploiting knowledge of the ﬁxed number of retrieval steps required to answer each question or using structured metadata like knowledge bases or web links that have limited availability. Instead, we design a system that can answer open-domain questions on any text collection without prior knowledge of reasoning complexity. To emulate this setting, we construct a new benchmark, called {BeerQA}, by combining existing one- and twostep datasets with a new collection of 530 questions that require three Wikipedia pages to answer, unifying Wikipedia corpora versions in the process. We show that our model demonstrates competitive performance on both existing benchmarks and this new benchmark. We make the new benchmark available at https: //beerqa.github.io/.},
	number = {{arXiv}:2010.12527},
	publisher = {{arXiv}},
	author = {Qi, Peng and Lee, Haejun and Sido, Oghenetegiri "{TG}" and Manning, Christopher D.},
	urldate = {2025-02-26},
	date = {2021-10-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2010.12527 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/martin/Zotero/storage/UKXQ9JBI/Qi et al. - 2021 - Answering Open-Domain Questions of Varying Reasoning Steps from Text.pdf:application/pdf},
}

@misc{khattab_demonstrate-search-predict_2023-1,
	title = {Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive {NLP}},
	url = {http://arxiv.org/abs/2212.14024},
	doi = {10.48550/arXiv.2212.14024},
	shorttitle = {Demonstrate-Search-Predict},
	abstract = {Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models ({LM}) and retrieval models ({RM}). Existing work has combined these in simple “retrievethen-read” pipelines in which the {RM} retrieves passages that are inserted into the {LM} prompt. To begin to fully realize the potential of frozen {LMs} and {RMs}, we propose {DEMONSTRATE}–{SEARCH}–{PREDICT} ({DSP}), a framework that relies on passing natural language texts in sophisticated pipelines between an {LM} and an {RM}. {DSP} can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the {LM} and {RM} can handle more reliably. We have written novel {DSP} programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art incontext learning results and delivering 37–120\%, 8–39\%, and 80–290\% relative gains against the vanilla {LM} ({GPT}-3.5), a standard retrieve-thenread pipeline, and a contemporaneous self-ask pipeline, respectively. We release {DSP} at https: //github.com/stanfordnlp/dsp.},
	number = {{arXiv}:2212.14024},
	publisher = {{arXiv}},
	author = {Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
	urldate = {2025-02-26},
	date = {2023-01-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2212.14024 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {PDF:/home/martin/Zotero/storage/9PZKH3P7/Khattab et al. - 2023 - Demonstrate-Search-Predict Composing retrieval and language models for knowledge-intensive NLP.pdf:application/pdf},
}

@misc{sun_recitation-augmented_2023,
	title = {Recitation-Augmented Language Models},
	url = {http://arxiv.org/abs/2210.01296},
	doi = {10.48550/arXiv.2210.01296},
	abstract = {We propose a new paradigm to help Large Language Models ({LLMs}) generate more accurate factual knowledge without retrieving from an external corpus, called {RECITation}-augmented {gEneration} ({RECITE}). Different from retrievalaugmented language models that retrieve relevant documents before generating the outputs, given an input, {RECITE} ﬁrst recites one or several relevant passages from {LLMs}’ own memory via sampling, and then produces the ﬁnal answers. We show that {RECITE} is a powerful paradigm for knowledge-intensive {NLP} tasks. Speciﬁcally, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering ({CBQA}) tasks. In experiments, we verify the effectiveness of {RECITE} on four pre-trained models ({PaLM}, {UL}2, {OPT}, and Codex) and three {CBQA} tasks (Natural Questions, {TriviaQA}, and {HotpotQA}). Our code is available at https://github.com/Edward-Sun/{RECITE}.},
	number = {{arXiv}:2210.01296},
	publisher = {{arXiv}},
	author = {Sun, Zhiqing and Wang, Xuezhi and Tay, Yi and Yang, Yiming and Zhou, Denny},
	urldate = {2025-02-25},
	date = {2023-02-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2210.01296 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {PDF:/home/martin/Zotero/storage/E5A5T33Z/Sun et al. - 2023 - Recitation-Augmented Language Models.pdf:application/pdf},
}

@misc{yu_generate_2023,
	title = {Generate rather than Retrieve: Large Language Models are Strong Context Generators},
	url = {http://arxiv.org/abs/2209.10063},
	doi = {10.48550/arXiv.2209.10063},
	shorttitle = {Generate rather than Retrieve},
	abstract = {Knowledge-intensive tasks, such as open-domain question answering ({QA}), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that ﬁrst retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read ({GENREAD}), which ﬁrst prompts a large language model to generate contextual documents based on a given question, and then reads the generated documents to produce the ﬁnal answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, in order to generate diverse documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain {QA}, fact checking, and dialogue system. Notably, {GENREAD} achieves 71.6 and 54.4 exact match scores on {TriviaQA} and {WebQ}, signiﬁcantly outperforming the state-of-the-art retrieve-thenread pipeline {DPR}-{FiD} by +4.0 and +3.9, without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation. Our code and generated documents can be found at https://github.com/wyu97/{GenRead}.},
	number = {{arXiv}:2209.10063},
	publisher = {{arXiv}},
	author = {Yu, Wenhao and Iter, Dan and Wang, Shuohang and Xu, Yichong and Ju, Mingxuan and Sanyal, Soumya and Zhu, Chenguang and Zeng, Michael and Jiang, Meng},
	urldate = {2025-02-25},
	date = {2023-01-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2209.10063 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {PDF:/home/martin/Zotero/storage/U2BGESZH/Yu et al. - 2023 - Generate rather than Retrieve Large Language Models are Strong Context Generators.pdf:application/pdf},
}

@article{ma_query_nodate,
	title = {Query Rewriting for Retrieval-Augmented Large Language Models},
	abstract = {Large Language Models ({LLMs}) play powerful, black-box readers in the retrieve-thenread pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-{RetrieveRead} instead of the previous retrieve-then-read for the retrieval-augmented {LLMs} from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval. We first prompt an {LLM} to generate the query, then use a web search engine to retrieve contexts. Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline. A small language model is adopted as a trainable rewriter to cater to the black-box {LLM} reader. The rewriter is trained using the feedback of the {LLM} reader by reinforcement learning. Evaluation is conducted on downstream tasks, open-domain {QA} and multiple-choice {QA}. Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented {LLM} 1.},
	author = {Ma, Xinbei and Gong, Yeyun and He, Pengcheng and Zhao, Hai and Duan, Nan},
	langid = {english},
	file = {PDF:/home/martin/Zotero/storage/N69RXH44/Ma et al. - Query Rewriting for Retrieval-Augmented Large Language Models.pdf:application/pdf},
}

@misc{cheng_uprise_2023,
	title = {{UPRISE}: Universal Prompt Retrieval for Improving Zero-Shot Evaluation},
	url = {http://arxiv.org/abs/2303.08518},
	doi = {10.48550/arXiv.2303.08518},
	shorttitle = {{UPRISE}},
	abstract = {Large Language Models ({LLMs}) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose {UPRISE} (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a crosstask and cross-model scenario: the retriever is tuned on diverse tasks, but tested on unseen task types; we use a small frozen {LLM}, {GPT}-Neo-2.7B, for tuning the retriever, but test the retriever on different {LLMs} of much larger scales, such as {BLOOM}-7.1B, {OPT}-66B and {GPT}3-175B. Additionally, we show that {UPRISE} mitigates the hallucination problem in our experiments with {ChatGPT}, suggesting its potential to improve even the strongest {LLMs}. Our model and code are available at https://github.com/microsoft/{LMOps}.},
	number = {{arXiv}:2303.08518},
	publisher = {{arXiv}},
	author = {Cheng, Daixuan and Huang, Shaohan and Bi, Junyu and Zhan, Yuefeng and Liu, Jianfeng and Wang, Yujing and Sun, Hao and Wei, Furu and Deng, Denvy and Zhang, Qi},
	urldate = {2025-02-25},
	date = {2023-12-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2303.08518 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/home/martin/Zotero/storage/AJVBXC8L/Cheng et al. - 2023 - UPRISE Universal Prompt Retrieval for Improving Zero-Shot Evaluation.pdf:application/pdf},
}

@misc{gao_retrieval-augmented_2024-1,
	title = {Retrieval-Augmented Generation for Large Language Models: A Survey},
	url = {http://arxiv.org/abs/2312.10997},
	doi = {10.48550/arXiv.2312.10997},
	shorttitle = {Retrieval-Augmented Generation for Large Language Models},
	abstract = {Large Language Models ({LLMs}) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation ({RAG}) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. {RAG} synergistically merges {LLMs}' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of {RAG} paradigms, encompassing the Naive {RAG}, the Advanced {RAG}, and the Modular {RAG}. It meticulously scrutinizes the tripartite foundation of {RAG} frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in {RAG} systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
	number = {{arXiv}:2312.10997},
	publisher = {{arXiv}},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	urldate = {2025-02-25},
	date = {2024-03-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2312.10997 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {PDF:/home/martin/Zotero/storage/T362V5C7/Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language Models A Survey.pdf:application/pdf},
}

@misc{cho_improving_2023,
	title = {Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering},
	url = {http://arxiv.org/abs/2310.17490},
	doi = {10.48550/arXiv.2310.17490},
	abstract = {Large language models ({LLMs}) enable zeroshot approaches in open-domain question answering ({ODQA}), yet with limited advancements as the reader is compared to the retriever. This study aims at the feasibility of a zero-shot reader that addresses the challenges of computational cost and the need for labeled data. We find that {LLMs} are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers. To tackle these problems, we mitigate the impact of such documents via Distraction-aware Answer Selection ({DAS}) with a negation-based instruction and score adjustment for proper answer selection. Experimental results show that our approach successfully handles distraction across diverse scenarios, enhancing the performance of zeroshot readers. Furthermore, unlike supervised readers struggling with unseen data, zero-shot readers demonstrate outstanding transferability without any training.},
	number = {{arXiv}:2310.17490},
	publisher = {{arXiv}},
	author = {Cho, Sukmin and Seo, Jeongyeon and Jeong, Soyeong and Park, Jong C.},
	urldate = {2025-02-25},
	date = {2023-11-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2310.17490 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {PDF:/home/martin/Zotero/storage/Z4L7ZZFU/Cho et al. - 2023 - Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Questio.pdf:application/pdf},
}

@misc{zhu_retrieving_2021,
	title = {Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering},
	url = {http://arxiv.org/abs/2101.00774},
	doi = {10.48550/arXiv.2101.00774},
	shorttitle = {Retrieving and Reading},
	abstract = {Open-domain Question Answering ({OpenQA}) is an important task in Natural Language Processing ({NLP}), which aims to answer a question in the form of natural language based on large-scale unstructured documents. Recently, there has been a surge in the amount of research literature on {OpenQA}, particularly on techniques that integrate with neural Machine Reading Comprehension ({MRC}). While these research works have advanced performance to new heights on benchmark datasets, they have been rarely covered in existing surveys on {QA} systems. In this work, we review the latest research trends in {OpenQA}, with particular attention to systems that incorporate neural {MRC} techniques. Speciﬁcally, we begin with revisiting the origin and development of {OpenQA} systems. We then introduce modern {OpenQA} architecture named “Retriever-Reader” and analyze the various systems that follow this architecture as well as the speciﬁc techniques adopted in each of the components. We then discuss key challenges to developing {OpenQA} systems and offer an analysis of benchmarks that are commonly used. We hope our work would enable researchers to be informed of the recent advancement and also the open challenges in {OpenQA} research, so as to stimulate further progress in this ﬁeld.},
	number = {{arXiv}:2101.00774},
	publisher = {{arXiv}},
	author = {Zhu, Fengbin and Lei, Wenqiang and Wang, Chao and Zheng, Jianming and Poria, Soujanya and Chua, Tat-Seng},
	urldate = {2025-02-24},
	date = {2021-05-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2101.00774 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {PDF:/home/martin/Zotero/storage/RVPC8M8X/Zhu et al. - 2021 - Retrieving and Reading A Comprehensive Survey on Open-domain Question Answering.pdf:application/pdf},
}

@misc{jeong_adaptive-rag_2024,
	title = {Adaptive-{RAG}: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity},
	url = {http://arxiv.org/abs/2403.14403},
	doi = {10.48550/arXiv.2403.14403},
	shorttitle = {Adaptive-{RAG}},
	abstract = {Retrieval-Augmented Large Language Models ({LLMs}), which incorporate the non-parametric knowledge from external knowledge bases into {LLMs}, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering ({QA}). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive {QA} framework that can dynamically select the most suitable strategy for (retrieval-augmented) {LLMs} from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller {LM} trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented {LLMs}, as well as the noretrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain {QA} datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of {QA} systems, compared to relevant baselines including the adaptive retrieval approaches. Code is available at: https:// github.com/starsuzi/Adaptive-{RAG}.},
	number = {{arXiv}:2403.14403},
	publisher = {{arXiv}},
	author = {Jeong, Soyeong and Baek, Jinheon and Cho, Sukmin and Hwang, Sung Ju and Park, Jong C.},
	urldate = {2025-02-23},
	date = {2024-03-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2403.14403 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {PDF:/home/martin/Zotero/storage/KYAWSIJU/Jeong et al. - 2024 - Adaptive-RAG Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexit.pdf:application/pdf},
}
