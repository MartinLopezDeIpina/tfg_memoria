@inproceedings{vaswani_attention_2017,
	title = {Attention is All you Need},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	doi = {10.48550/arXiv.1706.03762},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@article{dong_self-collaboration_2024,
	title = {Self-Collaboration Code Generation via {ChatGPT}},
	volume = {33},
	issn = {1049-331X},
	url = {https://dl.acm.org/doi/10.1145/3672459},
	doi = {10.1145/3672459},
	abstract = {Although large language models ({LLMs}) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing {LLMs}, exemplified by {ChatGPT}. Specifically, through role instructions, (1) Multiple {LLM} agents act as distinct “experts,” each responsible for a specific subtask within a complex task; (2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other’s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three {LLM} roles (i.e., analyst, coder, and tester) responsible for software development’s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9–47.1\% Pass@1 compared to the base {LLM} agent. Moreover, we showcase that self-collaboration could potentially enable {LLMs} to efficiently handle complex repository-level tasks that are not readily solved by the single {LLM} agent.},
	pages = {189:1--189:38},
	number = {7},
	journal = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Dong, Yihong and Jiang, Xue and Jin, Zhi and Li, Ge},
	urlyear = {2025},
	year = {2024,
	file = {Full Text PDF:/home/martin/Zotero/storage/2XUHKKID/Dong et al. - 2024 - Self-Collaboration Code Generation via ChatGPT.pdf:application/pdf},
}
}

@misc{acharya_devin_2025,
	title = {Devin: A Cautionary Tale of the Autonomous {AI} Engineer},
	url = {https://medium.com/@lotussavy/devin-a-cautionary-tale-of-the-autonomous-ai-engineer-e1339ede8f8a},
	shorttitle = {Devin},
	abstract = {The emergence of Devin, an {AI} promising to revolutionize software development as a fully autonomous engineer, captured the tech world’s…},
	author = {Acharya, Kamal},
	urlyear = {2025},
	year = {2025},
	file = {Snapshot:/home/martin/Zotero/storage/BPVVXKYF/devin-a-cautionary-tale-of-the-autonomous-ai-engineer-e1339ede8f8a.html:text/html},
}

@inproceedings{qian_chatdev_2024,
	location = {Bangkok, Thailand},
	title = {{ChatDev}: Communicative Agents for Software Development},
	doi = {10.18653/v1/2024.acl-long.810},
	pages = {15174--15186},
	booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Qian, Chen and Liu, Wei and Liu, Hongzhang and Chen, Nuo and Dang, Yufan and Li, Jiahao and Yang, Cheng and Chen, Weize and Su, Yusheng and Cong, Xin and Xu, Juyuan and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
	year = {2024},
}

@misc{gao_retrieval-augmented_2024,
	title = {Retrieval-Augmented Generation for Large Language Models: A Survey},
	url = {http://arxiv.org/abs/2312.10997},
	doi = {10.48550/arXiv.2312.10997},
	shorttitle = {Retrieval-Augmented Generation for Large Language Models},
	abstract = {Large Language Models ({LLMs}) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation ({RAG}) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. {RAG} synergistically merges {LLMs}' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of {RAG} paradigms, encompassing the Naive {RAG}, the Advanced {RAG}, and the Modular {RAG}. It meticulously scrutinizes the tripartite foundation of {RAG} frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in {RAG} systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
	number = {{arXiv}:2312.10997},
	publisher = {{arXiv}},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	urlyear = {2025},
	year = {2024},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2312.10997 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {PDF:/home/martin/Zotero/storage/SV5MDEW5/Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language Models A Survey.pdf:application/pdf},
}

@article{steinmacher_systematic_2015,
	title = {A systematic literature review on the barriers faced by newcomers to open source software projects},
	volume = {59},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584914002390},
	doi = {https://doi.org/10.1016/j.infsof.2014.11.001},
	abstract = {Context Numerous open source software projects are based on volunteers collaboration and require a continuous influx of newcomers for their continuity. Newcomers face barriers that can lead them to give up. These barriers hinder both developers willing to make a single contribution and those willing to become a project member. Objective This study aims to identify and classify the barriers that newcomers face when contributing to open source software projects. Method We conducted a systematic literature review of papers reporting empirical evidence regarding the barriers that newcomers face when contributing to open source software ({OSS}) projects. We retrieved 291 studies by querying 4 digital libraries. Twenty studies were identified as primary. We performed a backward snowballing approach, and searched for other papers published by the authors of the selected papers to identify potential studies. Then, we used a coding approach inspired by open coding and axial coding procedures from Grounded Theory to categorize the barriers reported by the selected studies. Results We identified 20 studies providing empirical evidence of barriers faced by newcomers to {OSS} projects while making a contribution. From the analysis, we identified 15 different barriers, which we grouped into five categories: social interaction, newcomers’ previous knowledge, finding a way to start, documentation, and technical hurdles. We also classified the problems with regard to their origin: newcomers, community, or product. Conclusion The results are useful to researchers and {OSS} practitioners willing to investigate or to implement tools to support newcomers. We mapped technical and non-technical barriers that hinder newcomers’ first contributions. The most evidenced barriers are related to socialization, appearing in 75\% (15 out of 20) of the studies analyzed, with a high focus on interactions in mailing lists (receiving answers and socialization with other members). There is a lack of in-depth studies on technical issues, such as code issues. We also noticed that the majority of the studies relied on historical data gathered from software repositories and that there was a lack of experiments and qualitative studies in this area.},
	pages = {67--85},
	journal = {Information and Software Technology},
	author = {Steinmacher, Igor and Silva, Marco Aurelio Graciotto and Gerosa, Marco Aurelio and Redmiles, David F.},
	year = {2015},
	keywords = {Barriers to entry, Joining, Newcomers, Onboarding, Open source software, Systematic literature review},
}

@inproceedings{azanza_can_2024,
	title = {Can {LLMs} Facilitate Onboarding Software Developers? An Ongoing Industrial Case Study},
	url = {https://ieeexplore.ieee.org/abstract/document/10662989},
	doi = {10.1109/CSEET62301.2024.10662989},
	shorttitle = {Can {LLMs} Facilitate Onboarding Software Developers?},
	abstract = {Onboarding new software developers presents per-sistent challenges for teams, with newcomers facing steep learning curves and senior staff burdened by providing training and mentoring. This research explores leveraging Large Language Models ({LLMs}) to streamline onboarding, mitigating productivity losses from disrupted workflows. We collaborated with {LKS} Next, an {IT} consulting firm that has experienced first hand the challenges onboarding new developers brings. This paper presents an ongoing case study within {LKS} Next, conducted using action design research methodology. Two cycles have been completed, gathering feedback from newcomers, team leaders, and managers on issues like the need to prioritize self-directed learning resources over “burdening” mentors, and considerations around using third-party {LLMs}. The third cycle will focus on evaluating open source {LLMs} to maintain control within the company. Our envisioned goal is to develop an {LLM}-powered conversational agent, delivering tailored onboarding support while avoiding privacy risks.},
	booktitle = {2024 36th International Conference on Software Engineering Education and Training ({CSEE}\&T)},
	pages = {1--6},
	booktitle = {2024 36th International Conference on Software Engineering Education and Training ({CSEE}\&T)},
	author = {Azanza, Maider and Pereira, Juanan and Irastorza, Arantza and Galdos, Aritz},
	urlyear = {2025},
	year = {2024},
	note = {{ISSN}: 2377-570X},
	keywords = {Large language models, Companies, Pressing, Privacy, Productivity, Prototypes, Training},
	file = {Full Text PDF:/home/martin/Zotero/storage/RMRCFS8P/Azanza et al. - 2024 - Can LLMs Facilitate Onboarding Software Developers An Ongoing Industrial Case Study.pdf:application/pdf},
}

@misc{cristian_ionescu_multi-agent_2025,
	title = {A Multi-agent Onboarding Assistant based on Large Language Models, Retrieval Augmented Generation, and Chain-of-Thought},
	url = {https://ui.adsabs.harvard.edu/abs/2025arXiv250323421C},
	doi = {10.48550/arXiv.2503.23421},
	abstract = {Effective onboarding in software engineering is crucial but difficult due to the fast-paced evolution of technologies. Traditional methods, like exploration and workshops, are costly, time-consuming, and quickly outdated in large projects. We propose the Onboarding Buddy system, which leverages large language models, retrieval augmented generation, and an automated chain-of-thought approach to improve onboarding. It integrates dynamic, context-specific support within the development environment, offering natural language explanations, code insights, and project guidance. Our solution is agent-based and provides customized assistance with minimal human intervention. Our study results among the eight participants show an average helpfulness rating of (M=3.26, {SD}=0.86) and ease of onboarding at (M=3.0, {SD}=0.96) out of four. While similar to tools like {GitHub} Copilot, Onboarding Buddy uniquely integrates a chain-of-thought reasoning mechanism with retrieval-augmented generation, tailored specifically for dynamic onboarding contexts. While our initial evaluation is based on eight participants within one project, we will explore larger teams and multiple real-world codebases in the company to demonstrate broader applicability. Overall, Onboarding Buddy holds great potential for enhancing developer productivity and satisfaction. Our tool, source code, and demonstration video are publicly available},
	publisher = {{arXiv}},
	author = {Cristian Ionescu, Andrei and Titov, Sergey and Izadi, Maliheh},
	urlyear = {2025},
	year = {2025},
	note = {{ADS} Bibcode: 2025arXiv250323421C},
	keywords = {Software Engineering},
	file = {Full Text PDF:/home/martin/Zotero/storage/SI36ERQU/Cristian Ionescu et al. - 2025 - A Multi-agent Onboarding Assistant based on Large Language Models, Retrieval Augmented Generation, a.pdf:application/pdf},
}

@inproceedings{zheng_take_2023,
	title = {Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models},
	url = {https://openreview.net/forum?id=3bq3jsvcQ1},
	shorttitle = {Take a Step Back},
	abstract = {We present {STEP}-{BACK} {PROMPTING}, a simple prompting technique that enables {LLMs} to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, {LLMs} significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of {STEP}-{BACK} {PROMPTING} with {PaLM}-2L, {GPT}-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including {STEM}, Knowledge {QA}, and Multi-Hop Reasoning. For instance, {STEP}-{BACK} {PROMPTING} improves {PaLM}-2L performance on {MMLU} (Physics and Chemistry) by 7\% and 11\% respectively, {TimeQA} by 27\%, and {MuSiQue} by 7\%.},
	booktitle = {The Twelfth International Conference on Learning Representations},
	author = {Zheng, Huaixiu Steven and Mishra, Swaroop and Chen, Xinyun and Cheng, Heng-Tze and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
	urlyear = {2025},
	year = {2023},
	langid = {english},
	file = {Full Text PDF:/home/martin/Zotero/storage/F7XCTQWT/Zheng et al. - 2023 - Take a Step Back Evoking Reasoning via Abstraction in Large Language Models.pdf:application/pdf},
}

@inproceedings{zhang_building_2023,
	title = {Building Cooperative Embodied Agents Modularly with Large Language Models},
	url = {https://openreview.net/forum?id=EnXJfQqy0K},
	abstract = {In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of {LLMs} and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent {CoELA}, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-{WAH} and {TDW}-{MAT} demonstrate that {CoELA} driven by {GPT}-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open {LMs} like {LLAMA}-2 still underperform, we fine-tune a {CoELA} with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that {CoELA} communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of {LLMs} for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-{LLM}-Agents/.},
	booktitle = {The Twelfth International Conference on Learning Representations},
	author = {Zhang, Hongxin and Du, Weihua and Shan, Jiaming and Zhou, Qinhong and Du, Yilun and Tenenbaum, Joshua B. and Shu, Tianmin and Gan, Chuang},
	urlyear = {2025},
	year = {2023},
	langid = {english},
	file = {Full Text PDF:/home/martin/Zotero/storage/PLCJK6LY/Zhang et al. - 2023 - Building Cooperative Embodied Agents Modularly with Large Language Models.pdf:application/pdf},
}

@misc{fischer_reflective_2023,
	title = {Reflective Linguistic Programming ({RLP}): A Stepping Stone in Socially-Aware {AGI} ({SocialAGI})},
	url = {http://arxiv.org/abs/2305.12647},
	doi = {10.48550/arXiv.2305.12647},
	shorttitle = {Reflective Linguistic Programming ({RLP})},
	abstract = {This paper presents Reflective Linguistic Programming ({RLP}), a unique approach to conversational {AI} that emphasizes self-awareness and strategic planning. {RLP} encourages models to introspect on their own predefined personality traits, emotional responses to incoming messages, and planned strategies, enabling contextually rich, coherent, and engaging interactions. A striking illustration of {RLP}'s potential involves a toy example, an {AI} persona with an adversarial orientation, a demon named `Bogus' inspired by the children's fairy tale Hansel \& Gretel. Bogus exhibits sophisticated behaviors, such as strategic deception and sensitivity to user discomfort, that spontaneously arise from the model's introspection and strategic planning. These behaviors are not pre-programmed or prompted, but emerge as a result of the model's advanced cognitive modeling. The potential applications of {RLP} in socially-aware {AGI} (Social {AGI}) are vast, from nuanced negotiations and mental health support systems to the creation of diverse and dynamic {AI} personas. Our exploration of deception serves as a stepping stone towards a new frontier in {AGI}, one filled with opportunities for advanced cognitive modeling and the creation of truly human `digital souls'.},
	number = {{arXiv}:2305.12647},
	publisher = {{arXiv}},
	author = {Fischer, Kevin A.},
	urlyear = {2025},
	year = {2023},
	eprinttype = {arxiv},
	eprint = {2305.12647 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {Preprint PDF:/home/martin/Zotero/storage/DLKS7ZXF/Fischer - 2023 - Reflective Linguistic Programming (RLP) A Stepping Stone in Socially-Aware AGI (SocialAGI).pdf:application/pdf;Snapshot:/home/martin/Zotero/storage/UL9CJGBI/2305.html:text/html},
}

@article{zhong_memorybank_2024,
	title = {{MemoryBank}: Enhancing Large Language Models with Long-Term Memory},
	volume = {38},
	rights = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29946},
	doi = {10.1609/aaai.v38i17.29946},
	shorttitle = {{MemoryBank}},
	abstract = {Large Language Models ({LLMs}) have drastically reshaped our interactions with artificial intelligence ({AI}) systems, showcasing impressive performance across an extensive array of tasks. Despite this, a notable hindrance remains—the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems, psychological counseling, and secretarial assistance. Recognizing the necessity for long-term memory, we propose {MemoryBank}, a novel memory mechanism tailored for {LLMs}. {MemoryBank} enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user's personality over time by synthesizing information from previous interactions. To mimic anthropomorphic behaviors and selectively preserve memory, {MemoryBank} incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory. This mechanism permits the {AI} to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a more human-like memory mechanism and enriched user experience. {MemoryBank} is versatile in accommodating both closed-source models like {ChatGPT} and open-source models such as {ChatGLM}. To validate {MemoryBank}'s effectiveness, we exemplify its application through the creation of an {LLM}-based chatbot named {SiliconFriend} in a long-term {AI} Companion scenario. Further tuned with psychological dialog data, {SiliconFriend} displays heightened empathy and discernment in its interactions. Experiment involves both qualitative analysis with real-world user dialogs and quantitative analysis with simulated dialogs. In the latter, {ChatGPT} acts as multiple users with diverse characteristics and generates long-term dialog contexts covering a wide array of topics. The results of our analysis reveal that {SiliconFriend}, equipped with {MemoryBank}, exhibits a strong capability for long-term companionship as it can provide emphatic response, recall relevant memories and understand user personality.},
	pages = {19724--19731},
	number = {17},
	journal = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Zhong, Wanjun and Guo, Lianghong and Gao, Qiqi and Ye, He and Wang, Yanlin},
	urlyear = {2025},
	year = {2024},
	langid = {english},
	note = {Number: 17},
	keywords = {{NLP}: Generation},
	file = {Full Text PDF:/home/martin/Zotero/storage/CXH6GJPY/Zhong et al. - 2024 - MemoryBank Enhancing Large Language Models with Long-Term Memory.pdf:application/pdf},
}

@inproceedings{park_generative_2023,
	location = {New York, {NY}, {USA}},
	title = {Generative Agents: Interactive Simulacra of Human Behavior},
	isbn = {979-8-4007-0132-0},
	url = {https://dl.acm.org/doi/10.1145/3586183.3606763},
	doi = {10.1145/3586183.3606763},
	series = {{UIST} '23},
	shorttitle = {Generative Agents},
	abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
	pages = {1--22},
	booktitle = {Proceedings of the 36th Annual {ACM} Symposium on User Interface Software and Technology},
	publisher = {Association for Computing Machinery},
	author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	urlyear = {2025},
	year = {2023},
	file = {Full Text PDF:/home/martin/Zotero/storage/PH2NNW8Y/Park et al. - 2023 - Generative Agents Interactive Simulacra of Human Behavior.pdf:application/pdf},
}

@inproceedings{wei_chain--thought_2023,
	title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
	pages = {24824--24837},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
}

@inproceedings{yao_tree_2023,
	title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf},
	pages = {11809--11822},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
}

@misc{wang_recmind_2024,
	title = {{RecMind}: Large Language Model Powered Agent For Recommendation},
	url = {http://arxiv.org/abs/2308.14296},
	doi = {10.48550/arXiv.2308.14296},
	shorttitle = {{RecMind}},
	abstract = {While the recommendation system ({RS}) has advanced significantly through deep learning, current {RS} approaches usually train and fine-tune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an {LLM}-powered autonomous recommender agent, {RecMind}, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the {LLM} self-inspires to consider all previously explored states to plan for the next step. This mechanism greatly improves the model's ability to comprehend and utilize historical information in planning for recommendation. We evaluate {RecMind}'s performance in various recommendation scenarios. Our experiment shows that {RecMind} outperforms existing zero/few-shot {LLM}-based recommendation baseline methods in various tasks and achieves comparable performance to a fully trained recommendation model P5.},
	number = {{arXiv}:2308.14296},
	publisher = {{arXiv}},
	author = {Wang, Yancheng and Jiang, Ziyan and Chen, Zheng and Yang, Fan and Zhou, Yingxue and Cho, Eunah and Fan, Xing and Huang, Xiaojiang and Lu, Yanbin and Yang, Yingzhen},
	urlyear = {2025},
	year = {2024},
	eprinttype = {arxiv},
	eprint = {2308.14296 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {Preprint PDF:/home/martin/Zotero/storage/L8VXFUW9/Wang et al. - 2024 - RecMind Large Language Model Powered Agent For Recommendation.pdf:application/pdf;Snapshot:/home/martin/Zotero/storage/AUG5WV28/2308.html:text/html},
}

@inproceedings{shinn_reflexion_2023,
	title = {Reflexion: language agents with verbal reinforcement learning},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1b44b878bb782e6954cd888628510e90-Paper-Conference.pdf},
	pages = {8634--8652},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
}

@inproceedings{madaan_self-refine_2023,
	title = {Self-Refine: Iterative Refinement with Self-Feedback},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf},
	pages = {46534--46594},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
}

@inproceedings{lin_swiftsage_2023,
	title = {{SwiftSage}: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/4b0eea69deea512c9e2c469187643dc2-Paper-Conference.pdf},
	pages = {23813--23825},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Lin, Bill Yuchen and Fu, Yicheng and Yang, Karina and Brahman, Faeze and Huang, Shiyu and Bhagavatula, Chandra and Ammanabrolu, Prithviraj and Choi, Yejin and Ren, Xiang},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
}

@inproceedings{huang_language_2022,
	title = {Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents},
	url = {https://proceedings.mlr.press/v162/huang22a.html},
	shorttitle = {Language Models as Zero-Shot Planners},
	abstract = {Can world knowledge learned by large language models ({LLMs}) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. “make breakfast”), to a chosen set of actionable steps (e.g. “open fridge”). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained {LMs} are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by {LLMs} often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent {VirtualHome} environment shows that the resulting method substantially improves executability over the {LLM} baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models.},
	booktitle = {International Conference on Machine Learning},
	pages = {9118--9147},
	booktitle = {Proceedings of the 39th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
	urlyear = {2025},
	year = {2022},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/home/martin/Zotero/storage/I5X7KFEB/Huang et al. - 2022 - Language Models as Zero-Shot Planners Extracting Actionable Knowledge for Embodied Agents.pdf:application/pdf},
}

@misc{zhu_ghost_2023,
	title = {Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory},
	url = {http://arxiv.org/abs/2305.17144},
	doi = {10.48550/arXiv.2305.17144},
	shorttitle = {Ghost in the Minecraft},
	abstract = {The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular "{ObtainDiamond}" task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the "{ObtainDiamond}" task stands at around 20\%, highlighting the limitations of Reinforcement Learning ({RL}) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft ({GITM}), a novel framework integrates Large Language Models ({LLMs}) with text-based knowledge and memory, aiming to create Generally Capable Agents ({GCAs}) in Minecraft. These agents, equipped with the logic and common sense capabilities of {LLMs}, can skillfully navigate complex, sparse-reward environments with text-based interactions. We develop a set of structured actions and leverage {LLMs} to generate action plans for the agents to execute. The resulting {LLM}-based agent markedly surpasses previous methods, achieving a remarkable improvement of +47.5\% in success rate on the "{ObtainDiamond}" task, demonstrating superior robustness compared to traditional {RL}-based controllers. Notably, our agent is the first to procure all items in the Minecraft Overworld technology tree, demonstrating its extensive capabilities. {GITM} does not need any {GPU} for training, but a single {CPU} node with 32 {CPU} cores is enough. This research shows the potential of {LLMs} in developing capable agents for handling long-horizon, complex tasks and adapting to uncertainties in open-world environments. See the project website at https://github.com/{OpenGVLab}/{GITM}.},
	number = {{arXiv}:2305.17144},
	publisher = {{arXiv}},
	author = {Zhu, Xizhou and Chen, Yuntao and Tian, Hao and Tao, Chenxin and Su, Weijie and Yang, Chenyu and Huang, Gao and Li, Bin and Lu, Lewei and Wang, Xiaogang and Qiao, Yu and Zhang, Zhaoxiang and Dai, Jifeng},
	urlyear = {2025},
	year = {2023},
	eprinttype = {arxiv},
	eprint = {2305.17144 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/martin/Zotero/storage/NSGAEZ9P/Zhu et al. - 2023 - Ghost in the Minecraft Generally Capable Agents for Open-World Environments via Large Language Mode.pdf:application/pdf;Snapshot:/home/martin/Zotero/storage/XQ7V6EQH/2305.html:text/html},
}

@inproceedings{song_llm-planner_2023,
	title = {{LLM}-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Song_LLM-Planner_Few-Shot_Grounded_Planning_for_Embodied_Agents_with_Large_Language_ICCV_2023_paper.html},
	shorttitle = {{LLM}-Planner},
	booktitle = {Proceedings of the {IEEE}/{CVF} International Conference on Computer Vision},
	pages = {2998--3009},
	author = {Song, Chan Hee and Wu, Jiaman and Washington, Clayton and Sadler, Brian M. and Chao, Wei-Lun and Su, Yu},
	urlyear = {2025},
	year = {2023},
	langid = {english},
	file = {Full Text PDF:/home/martin/Zotero/storage/8DK8FYUZ/Song et al. - 2023 - LLM-Planner Few-Shot Grounded Planning for Embodied Agents with Large Language Models.pdf:application/pdf},
}

@misc{liu_odyssey_2025,
	title = {Odyssey: Empowering Minecraft Agents with Open-World Skills},
	url = {http://arxiv.org/abs/2407.15325},
	doi = {10.48550/arXiv.2407.15325},
	shorttitle = {Odyssey},
	abstract = {Recent studies have delved into constructing generalist agents for open-world environments like Minecraft. Despite the encouraging results, existing efforts mainly focus on solving basic programmatic tasks, e.g., material collection and tool-crafting following the Minecraft tech-tree, treating the {ObtainDiamond} task as the ultimate goal. This limitation stems from the narrowly defined set of actions available to agents, requiring them to learn effective long-horizon strategies from scratch. Consequently, discovering diverse gameplay opportunities in the open world becomes challenging. In this work, we introduce Odyssey, a new framework that empowers Large Language Model ({LLM})-based agents with open-world skills to explore the vast Minecraft world. Odyssey comprises three key parts: (1) An interactive agent with an open-world skill library that consists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned {LLaMA}-3 model trained on a large question-answering dataset with 390k+ instruction entries derived from the Minecraft Wiki. (3) A new agent capability benchmark includes the long-term planning task, the dynamic-immediate planning task, and the autonomous exploration task. Extensive experiments demonstrate that the proposed Odyssey framework can effectively evaluate different capabilities of {LLM}-based agents. All datasets, model weights, and code are publicly available to motivate future research on more advanced autonomous agent solutions.},
	number = {{arXiv}:2407.15325},
	publisher = {{arXiv}},
	author = {Liu, Shunyu and Li, Yaoru and Zhang, Kongcheng and Cui, Zhenyu and Fang, Wenkai and Zheng, Yuxuan and Zheng, Tongya and Song, Mingli},
	urlyear = {2025},
	year = {2025},
	eprinttype = {arxiv},
	eprint = {2407.15325 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/home/martin/Zotero/storage/UVNSAY2J/Liu et al. - 2025 - Odyssey Empowering Minecraft Agents with Open-World Skills.pdf:application/pdf;Snapshot:/home/martin/Zotero/storage/ULEZJ8AH/2407.html:text/html},
}

@inproceedings{raman_planning_2022,
	title = {Planning With Large Language Models Via Corrective Re-Prompting},
	url = {https://openreview.net/forum?id=cMDMRBe1TKs},
	abstract = {Extracting knowledge from Large Language Models ({LLM}) offers a path to designing intelligent, embodied agents that takes advantage of the common sense knowledge present in large language datasets. Related works have queried {LLMs} with a wide-range of contextual information, such as goals, sensor observations and scene descriptions, to generate high-level action plans for a specific task. In this work, we propose a prompting-based strategy for extracting executable plans from a {LLM} that leverages a novel and readily-accessible source of information: precondition errors. Our approach assumes that actions are only afforded execution in certain contexts (i.e. implicit preconditions must be met for an action to execute), and that the embodied agent has the ability to determine if the action is not executable in the current context (e.g: a precondition error is present). When an agent is unable to execute an action in a plan, our approach re-prompts the {LLM} with precondition error information to extract a useful and executable action to achieve the intended goal in the current context. We evaluate our approach in the {VirtualHome} simulation environment on 88 different tasks and 7 scenes. We evaluate different prompt templates and compare to methods that naively re-sample actions from the {LLM}. We find that our approach using precondition errors improves the executability and semantic correctness of plans, while also reducing the number of corrective re-prompts for querying actions.},
	booktitle = {{NeurIPS} 2022 Foundation Models for Decision Making Workshop},
	author = {Raman, Shreyas Sundara and Cohen, Vanya and Rosen, Eric and Idrees, Ifrah and Paulius, David and Tellex, Stefanie},
	urlyear = {2025},
	year = {2022},
	langid = {english},
	file = {Full Text PDF:/home/martin/Zotero/storage/CB48AAIA/Raman et al. - 2022 - Planning With Large Language Models Via Corrective Re-Prompting.pdf:application/pdf},
}

@misc{liu_llmp_2023,
	title = {{LLM}+P: Empowering Large Language Models with Optimal Planning Proficiency},
	url = {http://arxiv.org/abs/2304.11477},
	doi = {10.48550/arXiv.2304.11477},
	shorttitle = {{LLM}+P},
	abstract = {Large language models ({LLMs}) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, {LLMs} cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces {LLM}+P, the first framework that incorporates the strengths of classical planners into {LLMs}. {LLM}+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. {LLM}+P does so by first converting the language description into a file written in the planning domain definition language ({PDDL}), then leveraging classical planners to quickly find a solution, and then translating the found solution back into natural language. Along with {LLM}+P, we define a diverse set of different benchmark problems taken from common planning scenarios. Via a comprehensive set of experiments on these benchmark problems, we find that {LLM}+P is able to provide optimal solutions for most problems, while {LLMs} fail to provide even feasible plans for most problems.{\textbackslash}footnote\{The code and results are publicly available at https://github.com/Cranial-{XIX}/llm-pddl.git.},
	number = {{arXiv}:2304.11477},
	publisher = {{arXiv}},
	author = {Liu, Bo and Jiang, Yuqian and Zhang, Xiaohan and Liu, Qiang and Zhang, Shiqi and Biswas, Joydeep and Stone, Peter},
	urlyear = {2025},
	year = {2023},
	eprinttype = {arxiv},
	eprint = {2304.11477 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {Preprint PDF:/home/martin/Zotero/storage/CC3NLVRG/Liu et al. - 2023 - LLM+P Empowering Large Language Models with Optimal Planning Proficiency.pdf:application/pdf;Snapshot:/home/martin/Zotero/storage/Z8548YWG/2304.html:text/html},
}
}

@misc{karpas_mrkl_2022,
	title = {{MRKL} Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning},
	url = {http://arxiv.org/abs/2205.00445},
	doi = {10.48550/arXiv.2205.00445},
	shorttitle = {{MRKL} Systems},
	abstract = {Huge language models ({LMs}) have ushered in a new era for {AI}, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern {AI}, {LMs} are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language ({MRKL}, pronounced "miracle") system, some of the technical challenges in implementing it, and Jurassic-X, {AI}21 Labs' {MRKL} system implementation.},
	number = {{arXiv}:2205.00445},
	publisher = {{arXiv}},
	author = {Karpas, Ehud and Abend, Omri and Belinkov, Yonatan and Lenz, Barak and Lieber, Opher and Ratner, Nir and Shoham, Yoav and Bata, Hofit and Levine, Yoav and Leyton-Brown, Kevin and Muhlgay, Dor and Rozen, Noam and Schwartz, Erez and Shachaf, Gal and Shalev-Shwartz, Shai and Shashua, Amnon and Tenenholtz, Moshe},
	urlyear = {2025},
	year = {2022},
	eprinttype = {arxiv},
	eprint = {2205.00445 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/home/martin/Zotero/storage/LXEZT4QP/Karpas et al. - 2022 - MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external k.pdf:application/pdf;Snapshot:/home/martin/Zotero/storage/HNLAT5JQ/2205.html:text/html},
}

@inproceedings{ge_openagi_2023,
	title = {{OpenAGI}: When {LLM} Meets Domain Experts},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1190733f217404edc8a7f4e15a57f301-Paper-Datasets_and_Benchmarks.pdf},
	pages = {5539--5568},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Ge, Yingqiang and Hua, Wenyue and Mei, Kai and ji, jianchao and Tan, Juntao and Xu, Shuyuan and Li, Zelong and Zhang, Yongfeng},
	editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
	year = {2023},
}

@inproceedings{du_improving_2024,
	title = {Improving Factuality and Reasoning in Language Models through Multiagent Debate},
	url = {https://openreview.net/forum?id=zj7YuTE4t8},
	abstract = {Large language models ({LLMs}) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such "society of minds" approach has the potential to significantly advance the capabilities of {LLMs} and pave the way for further breakthroughs in language generation and understanding.},
	booktitle = {Forty-first International Conference on Machine Learning},
	author = {Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B. and Mordatch, Igor},
	urlyear = {2025},
	year = {2024},
	langid = {english},
	file = {Full Text PDF:/home/martin/Zotero/storage/A9JZW7RR/Du et al. - 2024 - Improving Factuality and Reasoning in Language Models through Multiagent Debate.pdf:application/pdf},
}

@misc{song_restgpt_2023,
	title = {{RestGPT}: Connecting Large Language Models with Real-World {RESTful} {APIs}},
	url = {http://arxiv.org/abs/2306.06624},
	doi = {10.48550/arXiv.2306.06624},
	shorttitle = {{RestGPT}},
	abstract = {Tool-augmented large language models ({LLMs}) have achieved remarkable progress in tackling a broad range of tasks. However, existing methods are mainly restricted to specifically designed tools and fail to fulfill complex instructions, having great limitations when confronted with real-world scenarios. In this paper, we explore a more realistic scenario by connecting {LLMs} with {RESTful} {APIs}, which adhere to the widely adopted {REST} software architectural style for web service development. To address the practical challenges of tackling complex instructions, we propose {RestGPT}, which exploits the power of {LLMs} and conducts a coarse-to-fine online planning mechanism to enhance the abilities of task decomposition and {API} selection. {RestGPT} also contains an {API} executor tailored for calling {RESTful} {APIs}, which can meticulously formulate parameters and parse {API} responses. To fully evaluate the performance of {RestGPT}, we propose {RestBench}, a high-quality benchmark which consists of two real-world scenarios and human-annotated instructions with gold solution paths. Experiments show that {RestGPT} is able to achieve impressive results in complex tasks and has strong robustness, which paves a new way towards {AGI}. {RestGPT} and {RestBench} is publicly available at https://restgpt.github.io/.},
	number = {{arXiv}:2306.06624},
	publisher = {{arXiv}},
	author = {Song, Yifan and Xiong, Weimin and Zhu, Dawei and Wu, Wenhao and Qian, Han and Song, Mingbo and Huang, Hailiang and Li, Cheng and Wang, Ke and Yao, Rong and Tian, Ye and Li, Sujian},
	urlyear = {2025},
	year = {2023},
	eprinttype = {arxiv},
	eprint = {2306.06624 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/martin/Zotero/storage/TQRLQ895/Song et al. - 2023 - RestGPT Connecting Large Language Models with Real-World RESTful APIs.pdf:application/pdf;Snapshot:/home/martin/Zotero/storage/97VQEX8Z/2306.html:text/html},
}

@misc{chen_fireact_2023,
	title = {{FireAct}: Toward Language Agent Fine-tuning},
	url = {http://arxiv.org/abs/2310.05915},
	doi = {10.48550/arXiv.2310.05915},
	shorttitle = {{FireAct}},
	abstract = {Recent efforts have augmented language models ({LMs}) with external tools or environments, leading to the development of language agents that can reason and act. However, most of these agents rely on few-shot prompting techniques with off-the-shelf {LMs}. In this paper, we investigate and argue for the overlooked direction of fine-tuning {LMs} to obtain language agents. Using a setup of question answering ({QA}) with a Google search {API}, we explore a variety of base {LMs}, prompting methods, fine-tuning data, and {QA} tasks, and find language agents are consistently improved after fine-tuning their backbone {LMs}. For example, fine-tuning Llama2-7B with 500 agent trajectories generated by {GPT}-4 leads to a 77\% {HotpotQA} performance increase. Furthermore, we propose {FireAct}, a novel approach to fine-tuning {LMs} with trajectories from multiple tasks and prompting methods, and show having more diverse fine-tuning data can further improve agents. Along with other findings regarding scaling effects, robustness, generalization, efficiency and cost, our work establishes comprehensive benefits of fine-tuning {LMs} for agents, and provides an initial set of experimental designs, insights, as well as open questions toward language agent fine-tuning.},
	number = {{arXiv}:2310.05915},
	publisher = {{arXiv}},
	author = {Chen, Baian and Shu, Chang and Shareghi, Ehsan and Collier, Nigel and Narasimhan, Karthik and Yao, Shunyu},
	urlyear = {2025},
	year = {2023},
	eprinttype = {arxiv},
	eprint = {2310.05915 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/home/martin/Zotero/storage/E4GURW4B/Chen et al. - 2023 - FireAct Toward Language Agent Fine-tuning.pdf:application/pdf;Snapshot:/home/martin/Zotero/storage/EVCB68XE/2310.html:text/html},
}

@inproceedings{chen_agent-flan_2024,
	location = {Bangkok, Thailand and virtual meeting},
	title = {Agent-{FLAN}: Designing Data and Methods of Effective Agent Tuning for Large Language Models},
	url = {https://aclanthology.org/2024.findings-acl.557},
	doi = {10.18653/v1/2024.findings-acl.557},
	shorttitle = {Agent-{FLAN}},
	abstract = {Open-sourced Large Language Models ({LLMs}) have achieved great success in various {NLP} tasks, however, they are still far inferior to {APIbased} models when acting as agents. How to integrate agent ability into general {LLMs} becomes a crucial and urgent problem. This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) {LLMs} exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-{FLAN} to effectively Fine-tune {LANguage} models for Agents. Through careful decomposition and redesign of the training corpus, Agent-{FLAN} enables Llama2-7B to outperform prior best works by 3.5\% across various agent evaluation datasets. With comprehensively constructed negative samples, Agent-{FLAN} greatly alleviates the hallucination issues based on our established evaluation benchmark. Besides, it consistently improves the agent capability of {LLMs} when scaling model sizes while slightly enhancing the general capability of {LLMs}. The code and model are available at https://github.com/{InternLM}/Agent-{FLAN}.},
	booktitle = {Findings of the Association for Computational Linguistics {ACL} 2024},
	pages = {9354--9366},
	booktitle = {Findings of the Association for Computational Linguistics {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Zehui and Liu, Kuikun and Wang, Qiuchen and Zhang, Wenwei and Liu, Jiangning and Lin, Dahua and Chen, Kai and Zhao, Feng},
	urlyear = {2025},
	year = {2024},
	langid = {english},
	file = {PDF:/home/martin/Zotero/storage/IAD7NPXI/Chen et al. - 2024 - Agent-FLAN Designing Data and Methods of Effective Agent Tuning for Large Language Models.pdf:application/pdf},
}

@inproceedings{zeng_agenttuning_2024,
	location = {Bangkok, Thailand and virtual meeting},
	title = {{AgentTuning}: Enabling Generalized Agent Abilities for {LLMs}},
	url = {https://aclanthology.org/2024.findings-acl.181},
	doi = {10.18653/v1/2024.findings-acl.181},
	shorttitle = {{AgentTuning}},
	abstract = {Open large language models ({LLMs}) has thus far inferior to commercial {LLMs} when acting as agents to tackle complex tasks. These agent tasks employ {LLMs} as the central controller responsible for planning, memorization, and tool utilization. To date, there is lack of research focusing on improving the agent capabilities of {LLMs} themselves. In this work, we present {AgentTuning}, a simple and general method to enhance the agent abilities of {LLMs} while maintaining their general {LLM} capabilities. We construct {AgentInstruct}, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hybrid instruction-tuning strategy by combining {AgentInstruct} with open-source instructions from general domains. {AgentTuning} is used to instruction-tune the Llama 2 series, resulting in {AgentLM}. Evaluations show that {AgentTuning} enables {LLMs}’ agent capabilities without compromising general abilities. The {AgentLM}-70B is comparable to {GPT}-3.5turbo on unseen agent tasks, demonstrating generalized agent capabilities. We open source the {AgentInstruct} dataset and {AgentLM}-7B, 13B, and 70B models at https://anonymous. 4open.science/r/{AgentTuning}.},
	booktitle = {Findings of the Association for Computational Linguistics {ACL} 2024},
	pages = {3053--3077},
	booktitle = {Findings of the Association for Computational Linguistics {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Zeng, Aohan and Liu, Mingdao and Lu, Rui and Wang, Bowen and Liu, Xiao and Dong, Yuxiao and Tang, Jie},
	urlyear = {2025},
	year = {2024},
	langid = {english},
	file = {PDF:/home/martin/Zotero/storage/I2YFCVCC/Zeng et al. - 2024 - AgentTuning Enabling Generalized Agent Abilities for LLMs.pdf:application/pdf},
}

@inproceedings{tam_let_2024,
	location = {Miami, Florida, {US}},
	title = {Let Me Speak Freely? A Study On The Impact Of Format Restrictions On Large Language Model Performance.},
	url = {https://aclanthology.org/2024.emnlp-industry.91/},
	doi = {10.18653/v1/2024.emnlp-industry.91},
	shorttitle = {Let Me Speak Freely?},
	abstract = {Structured generation, the process of producing content in standardized formats like {JSON} and {XML}, is widely utilized in real-world applications to extract key output information from large language models ({LLMs}).This study investigates whether such constraints on generation space impact {LLMs}' abilities, including reasoning and domain knowledge comprehension. Specifically, we evaluate {LLMs}' performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks. Surprisingly, we observe a significant decline in {LLMs}' reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks.},
	booktitle = {{EMNLP} 2024},
	pages = {1218--1236},
	booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},
	publisher = {Association for Computational Linguistics},
	author = {Tam, Zhi Rui and Wu, Cheng-Kuang and Tsai, Yi-Lin and Lin, Chieh-Yen and Lee, Hung-yi and Chen, Yun-Nung},
	editor = {Dernoncourt, Franck and Preoţiuc-Pietro, Daniel and Shimorina, Anastasia},
	urlyear = {2025},
	year = {2024},
	file = {Full Text PDF:/home/martin/Zotero/storage/F8IUCU96/Tam et al. - 2024 - Let Me Speak Freely A Study On The Impact Of Format Restrictions On Large Language Model Performanc.pdf:application/pdf},
}

@inproceedings{brown_language_2020,
	title = {Language Models are Few-Shot Learners},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	pages = {1877--1901},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
	url = {http://arxiv.org/abs/1907.11692},
	doi = {10.48550/arXiv.1907.11692},
	shorttitle = {{RoBERTa}},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of {BERT} pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that {BERT} was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on {GLUE}, {RACE} and {SQuAD}. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	number = {{arXiv}:1907.11692},
	publisher = {{arXiv}},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	urlyear = {2025},
	year = {2019},
	eprinttype = {arxiv},
	eprint = {1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/martin/Zotero/storage/CMA8IVT7/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining Approach.pdf:application/pdf;Snapshot:/home/martin/Zotero/storage/JTHD8D58/1907.html:text/html},
}

@article{gutierrez-fandino_maria_2022,
	title = {{MarIA}: Spanish Language Models},
	volume = {68},
	rights = {Copyright (c) 2022 Procesamiento del Lenguaje Natural},
	issn = {1989-7553},
	url = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6405},
	shorttitle = {{MarIA}},
	abstract = {This work presents {MarIA}, a family of Spanish language models and associated resources made available to the industry and the research community. Currently, {MarIA} includes {RoBERTa}-base, {RoBERTa}-large, {GPT}2 and {GPT}2-large Spanish language models, which can arguably be presented as the largest and most proficient language models in Spanish. The models were pretrained using a massive corpus of 570GB of clean and deduplicated texts with 135 billion words extracted from the Spanish Web Archive crawled by the National Library of Spain between 2009 and 2019. We assessed the performance of the models with nine existing evaluation datasets and with a novel extractive Question Answering dataset created ex novo. Overall, {MarIA} models outperform the existing Spanish models across a variety of {NLU} tasks and training settings.},
	pages = {39--60},
	number = {0},
	journal = {Procesamiento del Lenguaje Natural},
	author = {Gutiérrez-Fandiño, Asier and Armengol-Estapé, Jordi and Pàmies, Marc and Llop-Palao, Joan and Silveira-Ocampo, Joaquin and Carrino, Casimiro Pio and Armentano-Oller, Carme and Rodriguez-Penagos, Carlos and Gonzalez-Agirre, Aitor and Villegas, Marta},
	urlyear = {2025},
	year = {2022},
	langid = {spanish},
	note = {Number: 0},
	file = {Full Text PDF:/home/martin/Zotero/storage/JQJKLZIU/Gutiérrez-Fandiño et al. - 2022 - MarIA Spanish Language Models.pdf:application/pdf},
}

@article{khattab_relevance-guided_2021,
	title = {Relevance-guided Supervision for {OpenQA} with {ColBERT}},
	volume = {9},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00405},
	doi = {10.1162/tacl_a_00405},
	abstract = {Systems for Open-Domain Question Answering ({OpenQA}) generally depend on a retriever for finding candidate passages in a large corpus and a reader for extracting answers from those passages. In much recent work, the retriever is a learned component that uses coarse-grained vector representations of questions and passages. We argue that this modeling choice is insufficiently expressive for dealing with the complexity of natural language questions. To address this, we define {ColBERT}-{QA}, which adapts the scalable neural retrieval model {ColBERT} to {OpenQA}. {ColBERT} creates fine-grained interactions between questions and passages. We propose an efficient weak supervision strategy that iteratively uses {ColBERT} to create its own training data. This greatly improves {OpenQA} retrieval on Natural Questions, {SQuAD}, and {TriviaQA}, and the resulting system attains state-of-the-art extractive {OpenQA} performance on all three datasets.},
	pages = {929--944},
	journal = {Transactions of the Association for Computational Linguistics},
	shortjournal = {Transactions of the Association for Computational Linguistics},
	author = {Khattab, Omar and Potts, Christopher and Zaharia, Matei},
	urlyear = {2025},
	year = {2021},
	file = {Full Text PDF:/home/martin/Zotero/storage/LGRZLF4S/Khattab et al. - 2021 - Relevance-guided Supervision for OpenQA with ColBERT.pdf:application/pdf;Snapshot:/home/martin/Zotero/storage/JRZYSW88/Relevance-guided-Supervision-for-OpenQA-with.html:text/html},
}

@inproceedings{xiong_approximate_2020,
	title = {Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval},
	url = {https://openreview.net/forum?id=zeFrfgyZln},
	abstract = {Conducting text retrieval in a learned dense representation space has many intriguing advantages. Yet dense retrieval ({DR}) often underperforms word-based sparse retrieval. In this paper, we first theoretically show the bottleneck of dense retrieval is the domination of uninformative negatives sampled in mini-batch training, which yield diminishing gradient norms, large gradient variances, and slow convergence. We then propose Approximate nearest neighbor Negative Contrastive Learning ({ANCE}), which selects hard training negatives globally from the entire corpus. Our experiments demonstrate the effectiveness of {ANCE} on web search, question answering, and in a commercial search engine, showing {ANCE} dot-product retrieval nearly matches the accuracy of {BERT}-based cascade {IR} pipeline. We also empirically validate our theory that negative sampling with {ANCE} better approximates the oracle importance sampling procedure and improves learning convergence.},
	booktitle = {International Conference on Learning Representations},
	author = {Xiong, Lee and Xiong, Chenyan and Li, Ye and Tang, Kwok-Fung and Liu, Jialin and Bennett, Paul N. and Ahmed, Junaid and Overwijk, Arnold},
	urlyear = {2025},
	year = {2020},
	langid = {english},
	file = {Full Text PDF:/home/martin/Zotero/storage/8GPIICS2/Xiong et al. - 2020 - Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval.pdf:application/pdf},
}

@inproceedings{yu_augmentation-adapted_2023,
	location = {Toronto, Canada},
	title = {Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In},
	url = {https://aclanthology.org/2023.acl-long.136},
	doi = {10.18653/v1/2023.acl-long.136},
	abstract = {Retrieval augmentation can aid language models ({LMs}) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the {LM}, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target {LMs} that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target {LMs}, we propose augmentation-adapted retriever ({AAR}), which learns {LM}’s preferences obtained from a known source {LM}. Experiments on the {MMLU} and {PopQA} datasets demonstrate that our {AAR} trained with a small source {LM} is able to significantly improve the zero-shot generalization of larger target {LMs} ranging from 250M Flan-T5 to 175B {InstructGPT}. Further analysis indicates that the preferences of different {LMs} overlap, enabling {AAR} trained with a single source {LM} to serve as a generic plug-in for various target {LMs}. Our code is open-sourced at https://github.com/{OpenMatch}/{AugmentationAdapted}-Retriever.},
	booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages = {2421--2436},
	booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Yu, Zichun and Xiong, Chenyan and Yu, Shi and Liu, Zhiyuan},
	urlyear = {2025},
	year = {2023},
	langid = {english},
	file = {PDF:/home/martin/Zotero/storage/J8M4GSCU/Yu et al. - 2023 - Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In.pdf:application/pdf},
}

@software{noauthor_aider-aiaider_2025,
	title = {Aider-{AI}/aider},
	url = {https://github.com/Aider-AI/aider},
	abstract = {aider is {AI} pair programming in your terminal},
	publisher = {Aider {AI}},
	urlyear = {2025},
	year = {2025},
	keywords = {anthropic, chatgpt, claude-3, cli, command-line, gemini, gpt-3, gpt-35-turbo, gpt-4, gpt-4o, llama, openai, sonnet},
}

@inproceedings{yao_react_2023,
	title = {{ReAct}: Synergizing Reasoning and Acting in Language Models},
	booktitle = {International Conference on Learning Representations ({ICLR})},
	author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
	year = {2023},
}

@inproceedings{sim_ramp-up_1998,
	title = {The ramp-up problem in software projects: a case study of how software immigrants naturalize},
	doi = {10.1109/ICSE.1998.671389},
	pages = {361--370},
	booktitle = {Proceedings of the 20th International Conference on Software Engineering},
	author = {Sim, S.E. and Holt, R.C.},
	year = {1998},
	keywords = {Computer aided software engineering, Computer science, Costs, Educational institutions, Management training, Personnel, Programming, Recruitment, Software maintenance, Software systems},
}

@inproceedings{ritz_artificial_2023,
	location = {Maui, Hawaii, {USA}},
	title = {Artificial Socialization? How Artificial Intelligence Applications Can Shape A New Era of Employee Onboarding Practices},
	url = {https://aisel.aisnet.org/hicss-56/cl/ai_and_future_work/2/},
	doi = {10.24251/HICSS.2023.020},
	pages = {155--164},
	booktitle = {Hawaii International Conference on System Sciences 2023 ({HICSS}-56)},
	publisher = {{IEEE} Computer Society},
	author = {Ritz, Eva and Donisi, Fabio and Elshan, Edona and Rietsche, Roman},
	year = {2023},
}

@book{anthropic_model_2024,
	title = {Model Context Protocol ({MCP})},
	url = {https://docs.anthropic.com/en/docs/agents-and-tools/mcp},
	publisher = {Anthropic},
	author = {{Anthropic}},
	urlyear = {2025},
	year = {2024},
}

@article{zhu_retrieving_2021,
	title = {Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering},
	volume = {abs/2101.00774},
	url = {https://arxiv.org/abs/2101.00774},
	journal = {{CoRR}},
	author = {Zhu, Fengbin and Lei, Wenqiang and Wang, Chao and Zheng, Jianming and Poria, Soujanya and Chua, Tat-Seng},
	year = {2021},
	eprinttype = {arxiv},
	eprint = {2101.00774},
}

@article{khattab_demonstrate-search-predict_2022,
	title = {Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},
	journal = {{arXiv} preprint {arXiv}:2212.14024},
	author = {Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
	year = {2022},
}

@inproceedings{shao_enhancing_2023,
	location = {Singapore},
	title = {Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy},
	url = {https://aclanthology.org/2023.findings-emnlp.620/},
	doi = {10.18653/v1/2023.findings-emnlp.620},
	abstract = {Retrieval-augmented generation has raise extensive attention as it is promising to address the limitations of large language models including outdated knowledge and hallucinations. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to guide retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-{RetGen}, which synergizes retrieval and generation in an iterative manner: a model's response to a task input shows what might be needed to finish the task, and thus can serve as an informative context for retrieving more relevant knowledge which in turn helps generate a better response in another iteration. Compared with recent work which interleaves retrieval with generation when completing a single output, Iter-{RetGen} processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-{RetGen} on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.},
	pages = {9248--9274},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Huang, Minlie and Duan, Nan and Chen, Weizhu},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	year = {2023},
}

@article{qi_retrieve_2020,
	title = {Retrieve, Rerank, Read, then Iterate: Answering Open-Domain Questions of Arbitrary Complexity from Text},
	volume = {abs/2010.12527},
	url = {https://arxiv.org/abs/2010.12527},
	journal = {{CoRR}},
	author = {Qi, Peng and Lee, Haejun and Sido, Oghenetegiri "{TG}" and Manning, Christopher D.},
	year = {2020},
	eprinttype = {arxiv},
	eprint = {2010.12527},
}

@inproceedings{qi-etal-2021-answering,
    title = "Answering Open-Domain Questions of Varying Reasoning Steps from Text",
    author = "Qi, Peng  and
      Lee, Haejun  and
      Sido, Tg  and
      Manning, Christopher",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = {2021},
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.292/",
    doi = "10.18653/v1/2021.emnlp-main.292",
    pages = "3599--3614",
    abstract = "We develop a unified system to answer directly from text open-domain questions that may require a varying number of retrieval steps. We employ a single multi-task transformer model to perform all the necessary subtasks{---}retrieving supporting facts, reranking them, and predicting the answer from all retrieved documents{---}in an iterative fashion. We avoid crucial assumptions of previous work that do not transfer well to real-world settings, including exploiting knowledge of the fixed number of retrieval steps required to answer each question or using structured metadata like knowledge bases or web links that have limited availability. Instead, we design a system that can answer open-domain questions on any text collection without prior knowledge of reasoning complexity. To emulate this setting, we construct a new benchmark, called BeerQA, by combining existing one- and two-step datasets with a new collection of 530 questions that require three Wikipedia pages to answer, unifying Wikipedia corpora versions in the process. We show that our model demonstrates competitive performance on both existing benchmarks and this new benchmark. We make the new benchmark available at \url{https://beerqa.github.io/}."
}

@inproceedings{trivedi_interleaving_2023,
	location = {Toronto, Canada},
	title = {Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions},
	url = {https://aclanthology.org/2023.acl-long.557},
	doi = {10.18653/v1/2023.acl-long.557},
	pages = {10014--10037},
	booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	year = {2023},
}

@book{liang_unleashing_2023,
	title = {Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System},
	abstract = {Large-scale Language Models ({LLMs}) are constrained by their inability to process lengthy inputs. To address this limitation, we propose the Self-Controlled Memory ({SCM}) system to unleash infinite-length input capacity for large-scale language models. Our {SCM} system is composed of three key modules: the language model agent, the memory stream, and the memory controller. The language model agent iteratively processes ultra-long inputs and stores all historical information in the memory stream. The memory controller provides the agent with both long-term memory (archived memory) and short-term memory (flash memory) to generate precise and coherent responses. The controller determines which memories from archived memory should be activated and how to incorporate them into the model input. Our {SCM} system can be integrated with any {LLMs} to enable them to process ultra-long texts without any modification or fine-tuning. Experimental results show that our {SCM} system enables {LLMs}, which are not optimized for multi-turn dialogue, to achieve multi-turn dialogue capabilities that are comparable to {ChatGPT}, and to outperform {ChatGPT} in scenarios involving ultra-long document summarization or long-term conversations. Additionally, we will supply a test set, which covers common long-text input scenarios, for evaluating the abilities of {LLMs} in processing long documents.{\textbackslash}textasciitilde{\textbackslash}textbackslashfootnote\{Working in progress.\}{\textbackslash}textbackslashfootnote\{{\textbackslash}textbackslashurl\{https://github.com/wbbeyourself/{SCM}4LLMs\}\}},
	author = {Liang, Xinnian and Wang, Bing and Huang, Hui and Wu, Shuangzhi and Wu, Peihao and Lu, Lu and Ma, Zejun and Li, Zhoujun},
	year = {2023},
	doi = {10.48550/arXiv.2304.13343},
}

@misc{deepseek-ai_deepseek-r1_2025,
	title = {{DeepSeek}-R1: Incentivizing Reasoning Capability in {LLMs} via Reinforcement Learning},
	url = {https://arxiv.org/abs/2501.12948},
	author = {{DeepSeek-AI}},
	year = {2025},
	note = {\_eprint: 2501.12948},
}

@inproceedings{hong_metagpt_2024,
	title = {{MetaGPT}: Meta Programming for A Multi-Agent Collaborative Framework},
	url = {https://openreview.net/forum?id=VtmBAGCN7o},
	booktitle = {The Twelfth International Conference on Learning Representations},
	author = {Hong, Sirui and Zhuge, Mingchen and Chen, Jonathan and Zheng, Xiawu and Cheng, Yuheng and Wang, Jinlin and Zhang, Ceyao and Wang, Zili and Yau, Steven Ka Shing and Lin, Zijuan and Zhou, Liyang and Ran, Chenyu and Xiao, Lingfeng and Wu, Chenglin and Schmidhuber, Jürgen},
	year = {2024},
}

@article{kalliamvakou_research_2022,
	title = {Research: quantifying {GitHub} Copilot’s impact on developer productivity and happiness},
	journal = {The {GitHub} Blog},
	author = {Kalliamvakou, Eirini},
	year = {2022},
}

@misc{noauthor_github_nodate,
	title = {{GitHub} Copilot features},
	url = {https://docs-internal.github.com/en/copilot/about-github-copilot/github-copilot-features},
	abstract = {{GitHub} Copilot offers a suite of features. Copilot also offers a suite of features for administrators.},
	urlyear = {2025},
	file = {Snapshot:/home/martin/Zotero/storage/M7W6SBNT/github-copilot-features.html:text/html},
}

@software{sourcegraph_scip_2023,
	title = {{SCIP} Code Intelligence Protocol},
	url = {https://github.com/sourcegraph/scip},
	author = {{Sourcegraph}},
	urlyear = {2025},
	year = {2023},
}

@misc{sourcegraph_sourcegraph_2024,
	title = {Sourcegraph Public Snapshot: Code {AI} platform with Code Search \& Cody},
	url = {https://github.com/sourcegraph/sourcegraph-public-snapshot},
	author = {{Sourcegraph}},
	urlyear = {2025},
	year = {2024},
}

@misc{brown_c4_2018,
	title = {The C4 Model for Software Architecture},
	url = {https://www.infoq.com/articles/C4-architecture-model/},
	author = {Brown, Simon},
	urlyear = {2025},
	year = {2018},
	note = {Publisher: {InfoQ}},
}

@inproceedings{luo_repoagent_2024,
	location = {Miami, Florida, {USA}},
	title = {{RepoAgent}: An {LLM}-Powered Open-Source Framework for Repository-level Code Documentation Generation},
	url = {https://aclanthology.org/2024.emnlp-demo.46/},
	doi = {10.18653/v1/2024.emnlp-demo.46},
	abstract = {Generative models have demonstrated considerable potential in software engineering, particularly in tasks such as code generation and debugging. However, their utilization in the domain of code documentation generation remains underexplored. To this end, we introduce {RepoAgent}, a large language model powered open-source framework aimed at proactively generating, maintaining, and updating code documentation. Through both qualitative and quantitative evaluations, we have validated the effectiveness of our approach, showing that {RepoAgent} excels in generating high-quality repository-level documentation. The code and results are publicly accessible at https://github.com/{OpenBMB}/{RepoAgent}.},
	pages = {436--464},
	booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
	publisher = {Association for Computational Linguistics},
	author = {Luo, Qinyu and Ye, Yining and Liang, Shihao and Zhang, Zhong and Qin, Yujia and Lu, Yaxi and Wu, Yesai and Cong, Xin and Lin, Yankai and Zhang, Yingli and Che, Xiaoyin and Liu, Zhiyuan and Sun, Maosong},
	editor = {Hernandez Farias, Delia Irazu and Hope, Tom and Li, Manling},
	year = {2024},
}

@inproceedings{jeong_adaptive-rag_2024,
	location = {Mexico City, Mexico},
	title = {Adaptive-{RAG}: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity},
	url = {https://aclanthology.org/2024.naacl-long.389/},
	doi = {10.18653/v1/2024.naacl-long.389},
	abstract = {Retrieval-Augmented Large Language Models ({LLMs}), which incorporate the non-parametric knowledge from external knowledge bases into {LLMs}, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering ({QA}). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive {QA} framework that can dynamically select the most suitable strategy for (retrieval-augmented) {LLMs} from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller {LM} trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented {LLMs}, as well as the no-retrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain {QA} datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of {QA} systems, compared to relevant baselines including the adaptive retrieval approaches. Code is available at: https://github.com/starsuzi/Adaptive-{RAG}.},
	pages = {7036--7050},
	booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Jeong, Soyeong and Baek, Jinheon and Cho, Sukmin and Hwang, Sung Ju and Park, Jong},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	year = {2024},
}

@inproceedings{wei_eda_2019,
	location = {Hong Kong, China},
	title = {{EDA}: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks},
	url = {https://aclanthology.org/D19-1670/},
	doi = {10.18653/v1/D19-1670},
	abstract = {We present {EDA}: easy data augmentation techniques for boosting performance on text classification tasks. {EDA} consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that {EDA} improves performance for both convolutional and recurrent neural networks. {EDA} demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with {EDA} while using only 50\% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.},
	pages = {6382--6388},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Wei, Jason and Zou, Kai},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	year = {2019},
}

@article{wang_survey_2024,
	title = {A survey on large language model based autonomous agents},
	url = {https://openreview.net/forum?id=XATpzINPJh&referrer=%5Bthe%20profile%20of%20Xin%20Zhao%5D(%2Fprofile%3Fid%3D~Xin_Zhao10)},
	abstract = {Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models ({LLMs}) have shown potential in human-level intelligence, leading to a surge in research on {LLM}-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of {LLM}-based autonomous agents from a holistic perspective. We first discuss the construction of {LLM}-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of {LLM}-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for {LLM}-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.},
	journal = {Frontiers Comput. Sci.},
	author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhi-Yuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Jirong},
	urlyear = {2025},
	year = {2024},
	langid = {english},
	file = {Full Text PDF:/home/martin/Zotero/storage/I9UXNH2M/Wang et al. - 2024 - A survey on large language model based autonomous agents.pdf:application/pdf},
}

@inproceedings{liu_agentbench_2023,
	title = {{AgentBench}: Evaluating {LLMs} as Agents},
	url = {https://openreview.net/forum?id=zAdUB0aCTQ},
	shorttitle = {{AgentBench}},
	abstract = {The potential of Large Language Model ({LLM}) as agents has been widely acknowledged recently. Thus, there is an urgent need to quantitatively evaluate {LLMs} as agents on challenging tasks in interactive environments. We present {AgentBench}, a multi-dimensional benchmark that consists of 8 distinct environments to assess {LLM}-as-Agent's reasoning and decision-making abilities. Our extensive test over 29 {API}-based and open-sourced ({OSS}) {LLMs} shows that, while top commercial {LLMs} present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many {OSS} competitors that are no larger than 70B. We identify the typical reasons of failures in environments and {LLMs}, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable {LLM} agents. Improving instruction following and training on high quality multi-round alignment data could improve agent performance. And different from existing assumptions, training on code present ambivalent impacts on different agent tasks. Datasets, environments, and an integrated evaluation package for {AgentBench} are released at https://github.com/{THUDM}/{AgentBench}.},
	booktitle = {The Twelfth International Conference on Learning Representations},
	author = {Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and Zhang, Shudan and Deng, Xiang and Zeng, Aohan and Du, Zhengxiao and Zhang, Chenhui and Shen, Sheng and Zhang, Tianjun and Su, Yu and Sun, Huan and Huang, Minlie and Dong, Yuxiao and Tang, Jie},
	urlyear = {2025},
	year = {2023},
	langid = {english},
	file = {Full Text PDF:/home/martin/Zotero/storage/NYY5TX7S/Liu et al. - 2023 - AgentBench Evaluating LLMs as Agents.pdf:application/pdf},
}

@inproceedings{wang_describe_2023,
	title = {Describe, Explain, Plan and Select: Interactive Planning with {LLMs} Enables Open-World Multi-Task Agents},
	url = {https://openreview.net/forum?id=KtvPdGb31Z},
	shorttitle = {Describe, Explain, Plan and Select},
	abstract = {In this paper, we study the problem of planning in Minecraft, a popular, democratized yet challenging open-ended environment for developing multi-task embodied agents. We've found two primary challenges of empowering such agents with planning: 1) planning in an open-ended world like Minecraft requires precise and multi-step reasoning due to the long-term nature of the tasks, and 2) as vanilla planners do not consider the achievability of the current agent when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient. To this end, we propose ``\${\textbackslash}underline\{D\}\$escribe, \${\textbackslash}underline\{E\}\$xplain, \${\textbackslash}underline\{P\}\$lan and \${\textbackslash}underline\{S\}\$elect'' (\${\textbackslash}textbf\{{DEPS}\}\$), an interactive planning approach based on Large Language Models ({LLMs}). Our approach helps with better error correction from the feedback during the long-haul planning, while also bringing the sense of proximity via goal \${\textbackslash}textbf\{Selector\}\$, a learnable module that ranks parallel sub-goals based on the estimated steps of completion and improves the original plan accordingly. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., {ALFWorld} and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the \${\textbackslash}texttt\{{ObtainDiamond}\}\$ grand challenge with our approach.},
	booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
	author = {Wang, Zihao and Cai, Shaofei and Chen, Guanzhou and Liu, Anji and Ma, Xiaojian and Liang, Yitao},
	urlyear = {2025},
	year = {2023},
	langid = {english},
	file = {Full Text PDF:/home/martin/Zotero/storage/9BBW9LB9/Wang et al. - 2023 - Describe, Explain, Plan and Select Interactive Planning with LLMs Enables Open-World Multi-Task Age.pdf:application/pdf},
}

@inproceedings{dagan_dynamic_2024,
	title = {Dynamic Planning with a {LLM}},
	url = {https://openreview.net/forum?id=ewx2RFiEYR},
	abstract = {While Large Language Models ({LLMs}) can solve many {NLP} tasks in zero-shot settings, applications involving embodied agents remain problematic. In particular, plans that require multi-step reasoning become difficult and too costly as the context window grows. Planning requires understanding the likely effects of actions and identifying whether the current environment satisfies the goal. While symbolic planners can often find optimal solutions quickly, their capacity to handle noisy observations and uncertainty is relatively rudimentary, severely limiting their practical use. In contrast, Large Language Models ({LLMs}) cope with noisy observations and high levels of uncertainty. This paper presents {LLM} Dynamic Planner ({LLM}-{DP}): a neuro-symbolic framework where an {LLM} works hand-in-hand with a traditional planner to solve an embodied task. Given action-descriptions, {LLM}-{DP} solves Alfworld more successfully and efficiently than a {LLM}-only {ReAct} baseline.},
	booktitle = {Language Gamification - {NeurIPS} 2024 Workshop},
	author = {Dagan, Gautier and Keller, Frank and Lascarides, Alex},
	urlyear = {2025},
	year = {2024},
	langid = {english},
	file = {Full Text PDF:/home/martin/Zotero/storage/IIGVDMVT/Dagan et al. - 2024 - Dynamic Planning with a LLM.pdf:application/pdf},
}

@article{zhuge_mindstorms_2025,
	title = {Mindstorms in Natural Language-Based Societies of Mind},
	volume = {11},
	issn = {2096-0662},
	url = {https://ieeexplore.ieee.org/document/10903668},
	doi = {10.26599/CVM.2025.9450460},
	abstract = {Inspired by Minsky's Society of Mind, Schmidhuber's Learning to Think, and other more recent works, this paper proposes and advocates for the concept of natural language-based societies of mind ({NLSOMs}). We imagine these societies as consisting of a collection of multimodal neural networks, including large language models, which engage in a “mindstorm” to solve problems using a shared natural language interface. Here, we work to identify and discuss key questions about the social structure, governance, and economic principles for {NLSOMs}, emphasizing their impact on the future of {AI}. Our demonstrations with {NLSOMs}-which feature up to 129 agents-show their effectiveness in various tasks, including visual question answering, image captioning, and prompt generation for text-to-image synthesis.},
	pages = {29--81},
	number = {1},
	journal = {Computational Visual Media},
	author = {Zhuge, Mingchen and Liu, Haozhe and Faccio, Francesco and Ashley, Dylan R. and Csordás, Róbert and Gopalakrishnan, Anand and Hamdi, Abdullah and Hammoud, Hasan Abed Al Kader and Herrmann, Vincent and Irie, Kazuki and Kirsch, Louis and Li, Bing and Li, Guohao and Liu, Shuming and Mai, Jinjie and Piękos, Piotr and Ramesh, Aditya A. and Schlag, Imanol and Shi, Weimin and Stanić, Aleksandar and Wang, Wenyi and Wang, Yuhui and Xu, Mengmeng and Fan, Deng-Ping and Ghanem, Bernard and Schmidhuber, Jürgen},
	urlyear = {2025},
	year = {2025},
	keywords = {Artificial intelligence, Artificial neural networks, Brain modeling, Electronic mail, Large language models, large language models ({LLMs}), learning to think, mindstorm, multimodal learning, Natural languages, Question answering (information retrieval), society of mind ({SOM}), Text to image, Vectors, Visualization},
	file = {Full Text PDF:/home/martin/Zotero/storage/GSQWKUB8/Zhuge et al. - 2025 - Mindstorms in Natural Language-Based Societies of Mind.pdf:application/pdf},
}

@inproceedings{miao_selfcheck_2023,
	title = {{SelfCheck}: Using {LLMs} to Zero-Shot Check Their Own Step-by-Step Reasoning},
	url = {https://openreview.net/forum?id=pTHfApDakA},
	shorttitle = {{SelfCheck}},
	abstract = {The recent progress in large language models ({LLMs}), especially the invention of chain-of-thought prompting, has made it possible to automatically answer questions by stepwise reasoning. However, when faced with more complicated problems that require non-linear thinking, even the strongest {LLMs} make mistakes. To address this, we explore whether {LLMs} are able to recognize errors in their own step-by-step reasoning, without resorting to external resources. To this end, we propose {SelfCheck}, a general-purpose zero-shot verification schema for recognizing such errors. We then use the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question. We test {SelfCheck} on math- and logic-based datasets and find that it successfully recognizes errors and, in turn, increases final answer accuracies.},
	booktitle = {The Twelfth International Conference on Learning Representations},
	author = {Miao, Ning and Teh, Yee Whye and Rainforth, Tom},
	urlyear = {2025},
	year = {2023},
	langid = {english},
	file = {Full Text PDF:/home/martin/Zotero/storage/AZ6VSUEL/Miao et al. - 2023 - SelfCheck Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning.pdf:application/pdf},
}

@inproceedings{jimenez_swe-bench_2023,
	title = {{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},
	url = {https://openreview.net/forum?id=VTF8yNQM66},
	shorttitle = {{SWE}-bench},
	abstract = {Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce {SWE}-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real {GitHub} issues and corresponding pull requests across 12 popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in {SWE}-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model {SWE}-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96\% of the issues. Advances on {SWE}-bench represent steps towards {LMs} that are more practical, intelligent, and autonomous.},
	booktitle = {The Twelfth International Conference on Learning Representations},
	author = {Jimenez, Carlos E. and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik R.},
	urlyear = {2025},
	year = {2023},
	langid = {english},
	file = {Full Text PDF:/home/martin/Zotero/storage/KHIKW8LK/Jimenez et al. - 2023 - SWE-bench Can Language Models Resolve Real-world Github Issues.pdf:application/pdf},
}

@inproceedings{qin_toolllm_2023,
	title = {{ToolLLM}: Facilitating Large Language Models to Master 16000+ Real-world {APIs}},
	url = {https://openreview.net/forum?id=dHng2O0Jjr},
	shorttitle = {{ToolLLM}},
	abstract = {Despite the advancements of open-source large language models ({LLMs}), e.g., {LLaMA}, they remain significantly limited in tool-use capabilities, i.e., using external tools ({APIs}) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art ({SOTA}) closed-source {LLMs}, e.g., {ChatGPT}. To bridge this gap, we introduce {ToolLLM}, a general tool-use framework encompassing data construction, model training, and evaluation. We first present {ToolBench}, an instruction-tuning dataset for tool use, which is constructed automatically using {ChatGPT}. Specifically, the construction can be divided into three stages: (i) {API} collection: we collect 16,464 real-world {RESTful} {APIs} spanning 49 categories from {RapidAPI} Hub; (ii) instruction generation: we prompt {ChatGPT} to generate diverse instructions involving these {APIs}, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use {ChatGPT} to search for a valid solution path (chain of {API} calls) for each instruction. To enhance the reasoning capabilities of {LLMs}, we develop a novel depth-first search-based decision tree algorithm. It enables {LLMs} to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of {LLMs}, we develop an automatic evaluator: {ToolEval}. Based on {ToolBench}, we fine-tune {LLaMA} to obtain an {LLM} {ToolLLaMA}, and equip it with a neural {API} retriever to recommend appropriate {APIs} for each instruction. Experiments show that {ToolLLaMA} demonstrates a remarkable ability to execute complex instructions and generalize to unseen {APIs}, and exhibits comparable performance to {ChatGPT}. Our {ToolLLaMA} also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: {APIBench}.},
	booktitle = {The Twelfth International Conference on Learning Representations},
	author = {Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and Zhao, Sihan and Hong, Lauren and Tian, Runchu and Xie, Ruobing and Zhou, Jie and Gerstein, Mark and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
	urlyear = {2025},
	year = {2023},
	langid = {english},
	file = {Full Text PDF:/home/martin/Zotero/storage/8PTTGSS9/Qin et al. - 2023 - ToolLLM Facilitating Large Language Models to Master 16000+ Real-world APIs.pdf:application/pdf},
}

@article{wang_voyager_2023,
	title = {Voyager: An Open-Ended Embodied Agent with Large Language Models},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=ehfRiF0R3a},
	shorttitle = {Voyager},
	abstract = {We introduce Voyager, the first {LLM}-powered embodied lifelong learning agent in an open-ended world that continuously explores, acquires diverse skills, and makes novel discoveries without human intervention in Minecraft. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with {GPT}-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent’s capability rapidly and alleviates catastrophic forgetting. Empirically, Voyager demonstrates strong in-context lifelong learning capabilities. It outperforms prior {SOTA} by obtaining 3.1x more unique items, unlocking tech tree milestones up to 15.3x faster, and traveling 2.3x longer distances. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize.},
	journal = {Transactions on Machine Learning Research},
	author = {Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
	urlyear = {2025},
	year = {2023},
	langid = {english},
	file = {Full Text PDF:/home/martin/Zotero/storage/IJCMRQNW/Wang et al. - 2023 - Voyager An Open-Ended Embodied Agent with Large Language Models.pdf:application/pdf},
}

@inproceedings{zhou_webarena_2023,
	title = {{WebArena}: A Realistic Web Environment for Building Autonomous Agents},
	url = {https://openreview.net/forum?id=oKn9c6ytLx},
	shorttitle = {{WebArena}},
	abstract = {With advances in generative {AI}, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best {GPT}-4-based agent only achieves an end-to-end task success rate of 14.41\%, significantly lower than the human performance of 78.24\%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that {\textbackslash}ours can be used to measure such progress.{\textbackslash}footnote\{Code, data, environment reproduction instructions, video demonstrations are available in the supplementary.\}},
	booktitle = {The Twelfth International Conference on Learning Representations},
	author = {Zhou, Shuyan and Xu, Frank F. and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Ou, Tianyue and Bisk, Yonatan and Fried, Daniel and Alon, Uri and Neubig, Graham},
	urlyear = {2025},
	year = {2023},
	langid = {english},
	file = {Full Text PDF:/home/martin/Zotero/storage/86LKJ2RS/Zhou et al. - 2023 - WebArena A Realistic Web Environment for Building Autonomous Agents.pdf:application/pdf},
}

@inproceedings{li_api-bank_2023,
	location = {Singapore},
	title = {{API}-Bank: A Comprehensive Benchmark for Tool-Augmented {LLMs}},
	url = {https://aclanthology.org/2023.emnlp-main.187/},
	doi = {10.18653/v1/2023.emnlp-main.187},
	abstract = {Recent research has demonstrated that Large Language Models ({LLMs}) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current {LLMs} in utilizing tools? (2) How can we enhance {LLMs}' ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce {API}-Bank, a groundbreaking benchmark, specifically designed for tool-augmented {LLMs}. For the first question, we develop a runnable evaluation system consisting of 73 {API} tools. We annotate 314 tool-use dialogues with 753 {API} calls to assess the existing {LLMs}' capabilities in planning, retrieving, and calling {APIs}. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 {APIs} spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented {LLM} initialized from Alpaca. Experimental results demonstrate that {GPT}-3.5 exhibits improved tool utilization compared to {GPT}-3, while {GPT}-4 excels in planning. However, there is still significant potential for further improvement. Moreover, Lynx surpasses Alpaca's tool utilization performance by more than 26 pts and approaches the effectiveness of {GPT}-3.5. Through error analysis, we highlight the key challenges for future research in this field to answer the third question.},
	pages = {3102--3116},
	booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Li, Minghao and Zhao, Yingxiu and Yu, Bowen and Song, Feifan and Li, Hangyu and Yu, Haiyang and Li, Zhoujun and Huang, Fei and Li, Yongbin},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	year = {2023},
}

@inproceedings{zhao_expel_2024,
	title = {{ExpeL}: {LLM} agents are experiential learners},
	isbn = {978-1-57735-887-9},
	url = {https://doi.org/10.1609/aaai.v38i17.29936},
	doi = {10.1609/aaai.v38i17.29936},
	series = {{AAAI}'24/{IAAI}'24/{EAAI}'24},
	abstract = {The recent surge in research interest in applying large language models ({LLMs}) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in {LLMs}. While there is a growing demand to tailor {LLMs} for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities. Moreover, state-of-the-art language models like {GPT}-4 and Claude are primarily accessible through {API} calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning ({ExpeL}) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the {ExpeL} agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the {ExpeL} agent through qualitative observations and additional experiments.},
	booktitle = {Proceedings of the Thirty-Eighth {AAAI} Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
	publisher = {{AAAI} Press},
	author = {Zhao, Andrew and Huang, Daniel and Xu, Quentin and Lin, Matthieu and Liu, Yong-Jin and Huang, Gao},
	year = {2024},
}

@inproceedings{li_exploring_2024,
	location = {New York, {NY}, {USA}},
	title = {Exploring the Use of Large Language Model-Driven Chatbots in Virtual Reality to Train Autistic Individuals in Job Communication Skills},
	isbn = {979-8-4007-0331-7},
	url = {https://doi.org/10.1145/3613905.3651996},
	doi = {10.1145/3613905.3651996},
	series = {{CHI} {EA} '24},
	abstract = {Autistic individuals commonly encounter challenges in communicating with others which can lead to difficulties in obtaining and maintaining jobs. Thus, job training programs have emphasized training the communication skills of autistic individuals to improve their employability. Hence, we developed a virtual reality application that features avatars as chatbots powered by Large Language Models ({LLMs}), such as {GPT}-3.5 Turbo, and employs speech-based interactions with users. The use of {LLM}-driven chatbots allows job coaches to create training scenarios for trainees using text prompts. We conducted a preliminary study with three autistic trainees and two job coaches to gather early-stage feedback on the application’s usability and user experience. In the study, the trainee participants were asked to interact with the application in two scenarios involving customer interactions. Our findings indicate that our application shows promise for training job communication. Furthermore, we discuss its user experience aspects from the trainees’ and job coaches’ perspectives.},
	booktitle = {Extended Abstracts of the {CHI} Conference on Human Factors in Computing Systems},
	publisher = {Association for Computing Machinery},
	author = {Li, Ziming and Babar, Pinaki Prasanna and Barry, Mike and Peiris, Roshan L},
	year = {2024},
	note = {event-place: Honolulu, {HI}, {USA}},
	keywords = {Accessibility, Autism Spectrum Disorder, Communication Skill Training, Generative {AI}, Job-related Soft Skill, Virtual Reality},
}

@article{xiaoliang_design_2024,
	title = {Design of a large language model for improving customer service in telecom operators},
	volume = {60},
	url = {https://digital-library.theiet.org/doi/abs/10.1049/ell2.13218},
	doi = {10.1049/ell2.13218},
	abstract = {Telecommunications operators are tasked with enhancing service quality, reducing operational costs, and preserving customer privacy. This study presents an innovative application of large language models ({LLMs}) integrated with the {LangChain} technology framework, aimed at revolutionizing customer service in the telecom sector. The {LangChain} framework features a Knowledge Organizing Module and a Knowledge Search Module, both designed to refine customer support operations. The research develops an {LLM}‐based approach to improve the segmentation and organization of knowledge bases, tailored for the telecommunications industry. This approach ensures seamless integration with existing {LLMs} while preserving distinct knowledge domains, crucial for search accuracy. Additionally, the framework includes an advanced information security protocol with a robust filtering system that effectively excludes sensitive data from the model's outputs, enhancing data security. Empirical findings indicate that the {ChatGLM}2‐6B+{LangChain} model outperforms the baseline {ChatGLM}2, demonstrating heightened effectiveness in telecom‐specific tasks and outstripping even more sophisticated models like {GPT}‐4. The implementation of this {LLM}‐based framework within telecom customer service systems has significantly sharpened the precision of knowledge recommendations, as reflected by a dramatic increase in acceptance rates from 15\% to 70\%. This study addresses the inefficiencies of traditional customer service systems in telecom operations, which need help with timely data retrieval and precision. A customized large language model ({LLM}) is developed using the {LangChain} framework tailored for telecom customer service. The model's performance is further enhanced through reinforcement learning, significantly reducing the dissemination of incorrect information. The experimental findings demonstrate a substantial increase in the acceptance rate of the model's recommendations, from 15\% to 70\%, indicating its efficacy and reliability in environments with limited resources. image image},
	pages = {e13218},
	number = {10},
	journal = {Electronics Letters},
	author = {Xiaoliang, Ma and {RuQiang}, Zhao and Ying, Liu and Congjian, Deng and Dequan, Du},
	year = {2024},
	note = {\_eprint: https://digital-library.theiet.org/doi/pdf/10.1049/ell2.13218},
}


@misc{sourcegraph2024cody,
  title = {How Cody understands your codebase},
  author = {Sourcegraph},
  year = {2024},
  url = {https://sourcegraph.com/blog/how-cody-understands-your-codebase},
  note = {Accessed June 17, 2025}
}

@misc{gauthier_building_2023,
	author = {Gauthier, Paul},
	title = {Building a better repository map with tree sitter},
	url = {https://aider.chat/2023/10/22/repomap.html},
	titleaddon = {aider},
	urldate = {2025-04-12},
	year = {2023},
	langid = {american},
	file = {Snapshot:/home/martin/Zotero/storage/SZFZ2QQB/repomap.html:text/html},
}

@misc{noauthor_number_2024,
  author = {Josh Howarth},
	title = {Number of Parameters in {GPT}-4 (Latest Data)},
	url = {https://explodingtopics.com/blog/gpt-parameters},
	abstract = {An extensive list of statistics covering the number of parameters in {ChatGPT}-4, {ChatGPT}-4o, and other {AI} models.},
	titleaddon = {Exploding Topics},
	urldate = {2025-04-13},
	year = {2024},
	langid = {english},
	file = {Snapshot:/home/martin/Zotero/storage/QU5JTKX5/gpt-parameters.html:text/html},
}

@misc{mistral_codestral_2025,
	author = {{Mistral AI}},
	title = {Codestral Embed {\textbar} Mistral {AI}},
	url = {https://mistral.ai/news/codestral-embed},
	abstract = {The new state-of-the-art embedding model for code.},
	urldate = {2025-05-31},
	date = {2025-05-28},
	langid = {english},
	file = {Snapshot:/home/martin/Zotero/storage/TITSXXMT/codestral-embed.html:text/html},
}
