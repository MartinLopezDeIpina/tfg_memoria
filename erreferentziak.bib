@article{Shahbaba2011,
author="Shahbaba, B and Shachaf, C M and Yu, Z",
title="A pathway analysis method for genome-wide association studies",
journal="Statistics in Medicine",
year="2012",
volume="31",
pages="988-1000"
}

@book{Efron1994,
author="Efron, B.
and Tibshirani, R.",
title="An Introduction to the Bootstrap (Chapman {\&} Hall/CRC Monographs on Statistics {\&} Applied Probability)",
year="1994",
publisher="Chapman and Hall/CRC",
address="London"
}

@manual{Rmanual,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2013},
    url = {http://www.R-project.org/},
  }


@article{Subramanian2005gene,
  title={Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles},
  author={Subramanian, Aravind and Tamayo, Pablo and Mootha, Vamsi K and Mukherjee, Sayan and Ebert, Benjamin L and Gillette, Michael A and Paulovich, Amanda and Pomeroy, Scott L and Golub, Todd R and Lander, Eric S and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={102},
  number={43},
  pages={15545--15550},
  year={2005},
  publisher={National Acad Sciences}
}

@online{wei_eda_2019,
	title = {{EDA}: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks},
	url = {https://arxiv.org/abs/1901.11196v2},
	shorttitle = {{EDA}},
	abstract = {We present {EDA}: easy data augmentation techniques for boosting performance on text classification tasks. {EDA} consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that {EDA} improves performance for both convolutional and recurrent neural networks. {EDA} demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with {EDA} while using only 50\% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.},
	titleaddon = {{arXiv}.org},
	author = {Wei, Jason and Zou, Kai},
	urldate = {2025-05-22},
	date = {2019-01-31},
	langid = {english},
	file = {Full Text PDF:/home/martin/Zotero/storage/2ZIAC6SR/Wei and Zou - 2019 - EDA Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks.pdf:application/pdf},
}

@misc{jeong_adaptive-rag_2024,
	title = {Adaptive-{RAG}: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity},
	url = {http://arxiv.org/abs/2403.14403},
	doi = {10.48550/arXiv.2403.14403},
	shorttitle = {Adaptive-{RAG}},
	abstract = {Retrieval-Augmented Large Language Models ({LLMs}), which incorporate the non-parametric knowledge from external knowledge bases into {LLMs}, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering ({QA}). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive {QA} framework, that can dynamically select the most suitable strategy for (retrieval-augmented) {LLMs} from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller {LM} trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented {LLMs}, as well as the no-retrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain {QA} datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of {QA} systems, compared to relevant baselines including the adaptive retrieval approaches. Code is available at: https://github.com/starsuzi/Adaptive-{RAG}.},
	number = {{arXiv}:2403.14403},
	publisher = {{arXiv}},
	author = {Jeong, Soyeong and Baek, Jinheon and Cho, Sukmin and Hwang, Sung Ju and Park, Jong C.},
	urldate = {2025-05-22},
	date = {2024-03-28},
	eprinttype = {arxiv},
	eprint = {2403.14403 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/martin/Zotero/storage/EZJJTZ9P/Jeong et al. - 2024 - Adaptive-RAG Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexit.pdf:application/pdf;Snapshot:/home/martin/Zotero/storage/RBWXMHLP/2403.html:text/html},
}

@online{gutierrez-fandino_maria_2021,
	title = {{MarIA}: Spanish Language Models},
	url = {https://arxiv.org/abs/2107.07253v5},
	shorttitle = {{MarIA}},
	abstract = {This work presents {MarIA}, a family of Spanish language models and associated resources made available to the industry and the research community. Currently, {MarIA} includes {RoBERTa}-base, {RoBERTa}-large, {GPT}2 and {GPT}2-large Spanish language models, which can arguably be presented as the largest and most proficient language models in Spanish. The models were pretrained using a massive corpus of 570GB of clean and deduplicated texts with 135 billion words extracted from the Spanish Web Archive crawled by the National Library of Spain between 2009 and 2019. We assessed the performance of the models with nine existing evaluation datasets and with a novel extractive Question Answering dataset created ex novo. Overall, {MarIA} models outperform the existing Spanish models across a variety of {NLU} tasks and training settings.},
	titleaddon = {{arXiv}.org},
	author = {Gutiérrez-Fandiño, Asier and Armengol-Estapé, Jordi and Pàmies, Marc and Llop-Palao, Joan and Silveira-Ocampo, Joaquín and Carrino, Casimiro Pio and Gonzalez-Agirre, Aitor and Armentano-Oller, Carme and Rodriguez-Penagos, Carlos and Villegas, Marta},
	urldate = {2025-05-22},
	date = {2021-07-15},
	langid = {english},
	doi = {10.26342/2022-68-3},
	file = {Full Text PDF:/home/martin/Zotero/storage/XBNQRPP5/Gutiérrez-Fandiño et al. - 2021 - MarIA Spanish Language Models.pdf:application/pdf},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
	url = {http://arxiv.org/abs/1907.11692},
	doi = {10.48550/arXiv.1907.11692},
	shorttitle = {{RoBERTa}},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of {BERT} pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that {BERT} was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on {GLUE}, {RACE} and {SQuAD}. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	number = {{arXiv}:1907.11692},
	publisher = {{arXiv}},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	urldate = {2025-05-22},
	date = {2019-07-26},
	eprinttype = {arxiv},
	eprint = {1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/martin/Zotero/storage/H34NHNAY/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining Approach.pdf:application/pdf;Snapshot:/home/martin/Zotero/storage/4GW8Q9LL/1907.html:text/html},
}

