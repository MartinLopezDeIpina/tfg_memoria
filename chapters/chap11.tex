Habiendo finalizado la implementación del sistema, este capítulo expone los principales desafíos que el proyecto ha presentado, abordando tanto retos de análisis y diseño como consideraciones técnicas.

\section{Análisis y diseño}
Los agentes LLM ofrecen múltiples aplicaciones posibles, con numerosos trabajos explorando técnicas diversas. Dado este extenso espectro, uno de los principales retos consistió en determinar qué objetivos concretos perseguir dentro del ámbito del onboarding y cómo abordar su desarrollo.

Este desafío se manifestó inicialmente al establecer el alcance, donde se delimitó el objetivo específico en torno a la asistencia con las diversas cuestiones que puede enfrentar un nuevo desarrollador. Posteriormente, surgió al seleccionar las arquitecturas: aunque el director empresarial estableció la necesidad de un agente coordinador que delegase tareas a componentes especializados, la elección del enfoque concreto formaba parte del problema a resolver. La solución implicó explorar diferentes enfoques que combinaran técnicas de orquestación y planificación, junto con la implementación de un sistema adaptativo y la integración de memoria de ejecución.

\paragraph{Diseño del software}
Con el fin de desarrollar un sistema extensible, se estableció un diseño premeditado antes de proceder con cualquier implementación. Destacan especialmente la arquitectura distribuida para superar las limitaciones de ventana de contexto mediante agentes especializados, el enfoque híbrido que combina composición mediante grafos de LangGraph con herencia orientada a objetos para evitar duplicación de código, y el sistema de citas dinámico que garantiza la validez de los documentos referenciados.

\section{Comportamiento de agentes}
Deducir la causa del comportamiento de los agentes tras su evaluación constituyó otra dificultad. A diferencia de las excepciones tradicionales, donde un error se localiza en un punto específico del código, la conducta de un LLM resulta de múltiples variables.

Aunque es posible inferir las causas específicas mediante el análisis del estado interno del modelo, los LLM utilizados son de caja negra. Por tanto, el proceso de depuración se asemejó más a un análisis forense, requiriendo reflexión ante comportamientos inesperados: ¿por qué no citas las fuentes?, ¿por qué llamas a esa herramienta?, ¿por qué \textit{NO} llamas a esa herramienta?, ¿por qué ignoras la memoria?

Respecto a la última cuestión, al evaluar el mecanismo de memoria, ciertos ejemplos no mostraban mejoras. El análisis reveló que el agente ignoraba las memorias incluidas en el prompt del sistema junto a las demás instrucciones, presumiblemente porque la extensión del prompt reducía la atención hacia estos fragmentos específicos. Para solucionar este problema, se reimplementaron las memorias como mensajes individuales, aprovechando así la optimización conversacional de estos modelos.

\section{Desafíos técnicos}
Se han utilizado una variedad de tecnologías que han conllevado también desafíos de implementación, dado que el uso de este conjunto en específico en el grado universitario es más bien limitado.

\paragraph{Manejo de conexiones}
La gestión de múltiples conexiones simultáneas presentó complejidades técnicas. Para la base de datos, se evaluaron varias alternativas tecnológicas, optando por un enfoque híbrido: el ORM SQLAlchemy para consultas complejas con PGVector, y las abstracciones de LangChain para operaciones básicas de lectura e inserción. Se descartaron tecnologías web como Flask al determinar que estas capacidades no eran necesarias para los objetivos del proyecto.

El protocolo MCP introdujo retos adicionales al requerir el análisis de las modalidades de comunicación SSE y STDIO para diferentes servidores. La sincronización entre las conexiones de base de datos y las conexiones MCP demandó una arquitectura coordinada, implementada mediante un contexto asíncrono global que gestiona de forma centralizada tanto el pool de conexiones de PostgreSQL (Anexo \ref{anexo:pool}) como las sesiones MCP, optimizando el uso de recursos compartidos.

\paragraph{Contexto asíncrono}
El costo computacional de los modelos LLM implica una latencia que requiere la optimización de ejecuciones. De este modo, en ciertas ocasiones han sido necesarios varios niveles de concurrencia asíncrona. Por ejemplo, a la hora de evaluar el sistema, se ejecutan 10 evaluaciones paralelas, en las cuales el orquestador puede decidir utilizar varios agentes concurrentes, los cuales a su vez pueden ejecutar varias herramientas a la vez. 

