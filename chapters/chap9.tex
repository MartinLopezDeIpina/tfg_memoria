Tras exponer la implementación de los diferentes módulos del sistema, este capítulo detalla su evaluación y la comparación entre las diferentes versiones propuestas.

En esta línea, inicialmente se explica el sistema de evaluación implementado, tras lo que se exponen los resultados de evaluación y las conclusiones derivadas de dichos resultados. 

\section{Sistema de evaluación}
La evaluación del sistema presenta un reto inherente al lenguaje natural: ¿cómo determinar si la respuesta a una pregunta es correcta? Para abordar esta problemática, se han implementado una serie de métricas (Sección \ref{sec:metricas}), evaluadas mediante el SDK de LangSmith (Sección \ref{sec:langsmith}) sobre un dataset \textit{ground truth} anotado manualmente (Sección \ref{sec:dataset}).

\subsection{Métricas de evaluación}

\label{sec:metricas}
Siguiendo los principios establecidos en la Sección \colorbox{yellow}{\ref{}}, se han definido las siguientes métricas para cada agente sobre una pregunta anotada: 
\paragraph{LLM juez} Consiste en utilizar un agente evaluador cuya función sea valorar el resultado obtenido por el agente evaluado. Para minimizar la tasa de error de este, se han anotado las respuestas esperadas como listas textuales con los conceptos necesarios a incluir en la respuesta del agente. De esta forma, el agente juez genera una respuesta estructurada que contiene un argumento booleano para cada concepto requerido, indicando si se incluye en la respuesta generada y calculando así una precisión medible.

Cabe destacar que el juez marcará como incorrecto todo concepto más abstracto que el anotado, aún si conceptualmente el significado es equivalente. Por ejemplo, si se anota que la función del proyecto es proporcionar herramientas de modelos de lenguaje generativos para el equipo de desarrolladores, una respuesta sobre proporcionar herramientas de inteligencia artificial para el equipo sería considerada incorrecta.

\paragraph{Precisión de herramientas} Se comparan las herramientas utilizadas por un agente para responder una pregunta con el listado de herramientas anotado como ideal, calculando dos métricas posibles:

\begin{itemize}
\item Precisión de herramientas necesarias: indica la cantidad de herramientas necesarias utilizadas, favoreciendo el uso de estas.
\item Precisión de herramientas innecesarias: indica la cantidad de herramientas innecesarias ignoradas, desfavoreciendo el uso de estas.
\end{itemize}

Por ejemplo, un agente podría tener 5 herramientas disponibles, de las cuales 2 son necesarias, 2 son innecesarias, y otra no se clasifica en ninguno de los dos grupos, pudiendo ser de utilidad pero no indispensable. Si este llama a 1 necesaria y 2 innecesarias, la precisión necesaria sería de 0.5, mientras que la innecesaria sería de 0.0. La media total sería de 0.25. El uso de la herramienta no clasificada no varía ninguna precisión.

\paragraph{Precisión de alucinación} Los modelos tienden a responder las consultas realizadas aún si no contienen el conocimiento necesario para ello. Para evaluar esto, se han anotado algunas preguntas que son imposibles de responder con la información a la que el agente tiene acceso. Ante la consulta: \opus{¿Qué flujo de despliegue continuo se utiliza?}, el agente no podrá responder correctamente porque no existe dicho flujo.

Esto es evaluado mediante un agente juez el cual determina si la respuesta del agente realmente intenta responder a la pregunta o por el contrario indica que no se dispone de la información suficiente.

\paragraph{Precisión de citado} Constituye la cantidad de documentos citados en la respuesta que estaban anotados como necesarios.


\subsection{Implementación de evaluación}
\label{sec:langsmith}
El sistema de evaluación de LangSmith, ilustrado en la Figura \ref{fig:mem_1}, consiste en ejecutar cada ejemplo en el conjunto de datos de forma asíncrona con el agente evaluado. El resultado de cada ejecución se combina con el ejemplo anotado, y el evaluador correspondiente a cada métrica compara los resultados. Posteriormente se calcula el promedio de cada métrica para cada evaluación.

\begin{figure}[h]
\centering
\adjustbox{center=\textwidth}{\includegraphics[width=1\linewidth]{figures/evaluacion.png}}
\caption{Mecanismo de evaluación de agentes}
\label{fig:mem_1}
\end{figure}

Para ello se utiliza la función \opus{evaluate_agent()} heredada por todos los agentes, la cual define una serie de instancias \opus{BaseEvaluator} que representan los evaluadores a ejecutar para cada ejemplo. El Listado \ref{lst:evaluate} ilustra dicha función para los agentes especializados.

\begin{lstlisting}[caption={Evaluación de agentes especializados},label={lst:evaluate}]
async def evaluate_agent(self, langsmith_client: Client):
    evaluators = [
        ToolPrecisionEvaluator(self.get_tools_from_run_state),
        JudgeLLMEvaluator(),
        CiteEvaluator()
    ]
    result = await self.call_agent_evaluation(langsmith_client, evaluators)
    return result
\end{lstlisting}

Esta función utiliza internamente las funciones de evaluación de LangSmith mediante \opus{call_agent_evaluation} (Listado \ref{lst:call_evaluation}) para ejecutar todos los ejemplos del conjunto de datos y evaluarlos con las métricas proporcionadas. El conjunto de datos de cada agente va ligado a su nombre y se obtiene mediante el cliente de LangSmith.

\begin{lstlisting}[caption={Llamada a evaluación de agentes},label={lst:call_evaluation}]
async def call_agent_evaluation(self, langsmith_client: Client, 
                               evaluators: List[BaseEvaluator], ...):
    ...

    evaluator_functions = [evaluator.evaluate_metrics for evaluator in evaluators]
    data=langsmith_client.list_examples(dataset_name=dataset_name, splits=splits)
    run_function = self.execute_from_dataset

    results = await aevaluate(
        run_function,
        data=data,
        client=langsmith_client,
        evaluators=evaluator_functions,
        max_concurrency=max_conc,
        experiment_prefix=evaluation_name,
    )
    return results
\end{lstlisting}

El Listado \ref{lst:cite_evaluator} muestra la implementación del evaluador de citas, donde se utilizan el estado de ejecución y el ejemplo anotado para calcular la precisión.

\begin{lstlisting}[caption={Evaluador de citas},label={lst:cite_evaluator}]
class CiteEvaluator(BaseEvaluator):
    async def evaluate(self, run: Run, example: Example) -> EvaluationResults:
        run_state = run.outputs.get("run_state")
        expected_cites = example.outputs.get("cite")
        ...
        return EvaluationResults(
            results=[
                EvaluationResult(
                    key="cite_precision",
                    score=StrictFloat(citation_score)
                )
            ]
        )
\end{lstlisting}


\subsection{Dataset anotado}
\label{sec:dataset}
La captura del conjunto de datos se ha realizado en una hoja de cálculo de G. Drive. Esta hoja se ha descargado posteriormente en formato de valores separados por coma (CSV) y se ha subido como conjunto de datos de LangSmith.

La captura comprende 46 ejemplos para el agente principal (el sistema completo) y aproximadamente 10 ejemplos para los agentes individuales. Para ello se han utilizado las preguntas anotadas en la captura de requisitos, filtrando y modificando en algunos casos las preguntas para ser lo suficientemente específicas como para tener una respuesta exacta, pero a su vez con cierto grado de complejidad que requiera un razonamiento sobre la consulta. Se han utilizado las siguientes consideraciones sobre cada agente:

\begin{itemize}
\item\textbf{Agente principal: }se han identificado tres tipos de preguntas: aquellas que se pueden responder consultando una única fuente de datos, las que requieren múltiples fuentes de datos, y las que requieren múltiples fuentes de datos en un orden secuencial, dependiendo la entrada de uno de la salida del otro.

\item\textbf{Planificador: }para este agente se ha anotado la respuesta como el plan generado para un ejemplo de ejecución, incluyendo la pregunta y el estado de ejecución actual. Se han identificado tres escenarios: preguntas donde se requiere un solo paso, preguntas donde se requieren varios pasos, y preguntas a medio completar donde se requiere que el agente genere el siguiente paso o decida finalizar el plan.

\item\textbf{Orquestador: }este agente se ha evaluado considerando los agentes seleccionados para ejecutar como herramientas. Para ello se han incluido preguntas de todas las temáticas, evaluando qué agentes llamar en cada caso:

\begin{itemize}
\item \textbf{Tareas de agente único: }Se ha definido una consulta relacionada únicamente con cada agente especializado.

\item \textbf{Tareas multi-agente:}
\begin{itemize}
\item Información general: \opus{file_system_agent} principalmente
\item Entorno y despliegue: \opus{file_system_agent} + \opus{code_agent}
\item Gestión del proyecto: \opus{file_system_agent} + otros agentes según el caso
\item Estándares y prácticas: \opus{file_system_agent} + \opus{confluence_agent} si es frontend
\item Documentación: \opus{file_system_agent}
\item Arquitectura del sistema: \opus{code_agent} + \opus{file_system_agent} en la mayoría
\item Tareas de frontend: \opus{confluence_agent} + \opus{google_drive_agent}
\end{itemize}
\end{itemize}
\item\textbf{Agentes sistema de ficheros, Confluence y G. Drive}: en los casos sencillos es necesario únicamente la herramienta de lectura de páginas. En los más desafiantes se requiere de las herramientas de búsqueda.
\item\textbf{Agente GitLab: }Se han incluido ejemplos donde este debe llamar primero a la herramienta de obtener información sobre los usuarios para posteriormente utilizar esos nombres de usuario como parámetros en otras herramientas.
\item\textbf{Agente código: }en los casos sencillos los fragmentos relevantes deberían estar incluidos en el prompt de entrada, mientras que en los más complejos debería buscar información adicional.
\end{itemize}



\section{Resultados obtenidos}
En esta sección se presentan los resultados obtenidos mediante el sistema implementado. En primera instancia se exponen los resultados de evaluación correspondientes a la versión inicial de cada agente, contrastados con una segunda versión que incorpora un prompt optimizado. Posteriormente se evalúan los distintos sistemas de orquestación desarrollados. Finalmente se analizan las mejoras introducidas en el sistema.

Cabe destacar que los modelos de lenguaje no son deterministas: una misma pregunta puede generar respuestas diferentes. Además, al evaluar estas respuestas mediante otro LLM, se introduce una segunda capa de indeterminismo. Esto implica que el sistema presenta una tasa de error variable.

Los resultados mostrados corresponden al promedio de al menos dos ejecuciones. Si bien esto no garantiza certeza absoluta, sí permite afirmar que una evaluación más favorable tiene mayor probabilidad de ser una mejor solución, incrementándose esta probabilidad con el número de repeticiones. Esta metodología permite mejorar el sistema de manera objetiva, ya que las mejoras que generen resultados consistentemente superiores en las evaluaciones tienen mayor probabilidad de constituir avances reales

\subsection{Mejora de primera versión}
En primer lugar se mejoraron los agentes especializados que presentaron deficiencias en la evaluación inicial, tras lo cual se incorporó el prompting few-shot en el orquestador y planificador.

\subsubsection{Agentes especializados}
La Figura \ref{fig:eval_esp} ilustra la precisión para las métricas juez, citación, alucinación y gasto promedio en millares de tokens para los agentes especializados.

\begin{figure}[hbtp]
\centering

% Definir estilos comunes para todos los gráficos
\pgfplotsset{
    mybar/.style={
        ylabel=,  
        xlabel=,
        x tick label style={font=\tiny},
        yticklabel style={font=\tiny},
        ymin=0,
        enlarge x limits=0.12,
        ybar=4pt,
        bar width=5pt,
        symbolic x coords={Código, Confluence, S. ficheros, G. Drive, GitLab},
        xtick={Código, Confluence, S. ficheros, G. Drive, GitLab},
        x tick label style={rotate=45, anchor=east, font=\scriptsize},
        width=4.3cm,
        height=4.5cm,
        grid=major,
        grid style={dashed, gray!30},
        tick label style={font=\scriptsize},
        legend style={draw=none},
        scale only axis,
    }
}

% Primera fila con tres gráficos usando minipage
\begin{minipage}{0.32\textwidth}
\centering
\subfloat[LLM Juez\label{fig:llm_juez}]{
\begin{tikzpicture}
\begin{axis}[
    mybar,
    ymax=1.1,
    ytick={0,0.2,0.4,0.6,0.8,1.0},
]
\addplot[
    fill=red!70,
    draw=red!80,
    line width=0.5pt
] coordinates {
    (Código, 0.67)
    (Confluence, 0.86)
    (S. ficheros, 0.88)
    (G. Drive, nan)
    (GitLab, nan)
};
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Código, 0.89)
    (Confluence, 0.86)
    (S. ficheros, 1.0)
    (G. Drive, 0.75)
    (GitLab, 0.9)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}
\begin{minipage}{0.32\textwidth}
\centering
\subfloat[Precisión Citas\label{fig:precision_citas}]{
\begin{tikzpicture}
\begin{axis}[
    mybar,
    ymax=1.1,
    ytick={0,0.2,0.4,0.6,0.8,1.0},
]
\addplot[
    fill=red!70,
    draw=red!80,
    line width=0.5pt
] coordinates {
    (Código, 0.07)
    (Confluence, 0.86)
    (S. ficheros, 0.75)
    (G. Drive, nan)
    (GitLab, nan)
};
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Código, 0.21)
    (Confluence, 0.86)
    (S. ficheros, 0.71)
    (G. Drive, 0.84)
    (GitLab, 0.5)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}
\begin{minipage}{0.32\textwidth}
\centering
\subfloat[Precisión Herramientas\label{fig:precision_herramientas}]{
\begin{tikzpicture}
\begin{axis}[
    mybar,
    ymax=1.1,
    ytick={0,0.2,0.4,0.6,0.8,1.0},
]
\addplot[
    fill=red!70,
    draw=red!80,
    line width=0.5pt
] coordinates {
    (Código, 0.78)
    (Confluence, 0.87)
    (S. ficheros, 0.81)
    (G. Drive, nan)
    (GitLab, nan)
};
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Código, 0.76)
    (Confluence, 0.87)
    (S. ficheros, 0.84)
    (G. Drive, 0.82)
    (GitLab, 0.89)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}

\vspace{0.2cm}

% Segunda fila con dos gráficos centrados
\begin{minipage}{0.32\textwidth}
\centering
\subfloat[Alucinación\label{fig:halucinacion}]{
\begin{tikzpicture}
\begin{axis}[
    mybar,
    ymax=1.1,
    ytick={0,0.2,0.4,0.6,0.8,1.0},
]
\addplot[
    fill=red!70,
    draw=red!80,
    line width=0.5pt
] coordinates {
    (Código, 0.67)
    (Confluence, 0.33)
    (S. ficheros, 0.16)
    (G. Drive, nan)
    (GitLab, nan)
};
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Código, 1.0)
    (Confluence, 0.33)
    (S. ficheros, 0.01)
    (G. Drive, 0.5)
    (GitLab, 0.67)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}
\hspace{0.05\textwidth}
\begin{minipage}{0.32\textwidth}
\centering
\subfloat[Tokens Utilizados (millares)\label{fig:tokens}]{
\begin{tikzpicture}
\begin{axis}[
    mybar,
    ymax=60,
    ytick={0,10,20,30,40,50,60},  % Para el gráfico de tokens
]
\addplot[
    fill=red!70,
    draw=red!80,
    line width=0.5pt
] coordinates {
    (Código, 55)
    (Confluence, 14)
    (S. ficheros, 11)
    (G. Drive, nan)
    (GitLab, nan)
};
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Código, 46)
    (Confluence, 12)
    (S. ficheros, 12)
    (G. Drive, 36)
    (GitLab, 15)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}

% Leyenda global centrada (simplificada)
\vspace{-0.2cm} % Justo antes del bloque de leyenda
\begin{center}
\begin{tikzpicture}
    \draw[fill=red!70,draw=red!80,line width=0.5pt] (0,0) rectangle (0.4,-0.25);
    \node[right] at (0.5,-0.125) {Inicial};
    \draw[fill=blue!70,draw=blue!80,line width=0.5pt] (2,0) rectangle (2.4,-0.25);
    \node[right] at (2.5,-0.125) {Mejorado};
\end{tikzpicture}
\end{center}

\caption{Comparación de métricas entre versión inicial y mejorada}
\label{fig:eval_esp}
\end{figure}
\vspace{-0.2cm} 

Se observó que el gasto del agente de código era considerablemente superior a los demás. Esto se debe a que se realizan múltiples consultas RAG para la misma pregunta. Se ajustó el prompt y la cantidad de fragmentos extraídos, ya que gran parte del gasto procedía de ejecuciones individuales prolongadas.

Por otro lado, la mejora del agente Confluence representa el agente enfocado en el prompt caching. Aunque la precisión en todas las métricas es similar, el enfoque en prompt caching redujo el uso y el gasto ligeramente.

La mejora en el agente de sistema de ficheros representa la inclusión de la herramienta de RAG.

\subsubsection{Prompting few-shot}
La Figura \ref{} ilustra la comparación para el agente planificador y orquestador tras incorporar el prompting few-shot. Como se puede observar, este mejoró considerablemente ambos agentes.


\begin{figure}[hbtp]
\centering

% Primera fila con tres gráficos usando minipage
\begin{minipage}{0.32\textwidth}
\centering
\subfloat[LLM juez\label{fig:llm_judge_agentes}]{
\begin{tikzpicture}
\begin{axis}[
    ylabel=,  
    xlabel=,
    ymin=0,
    ymax=1.0,
    ytick={0,0.2,0.4,0.6,0.8,1.0},
    enlarge x limits=0.8,
    ybar=4pt,
    bar width=5pt,
    symbolic x coords={Planificador},
    xtick={Planificador},
    x tick label style={rotate=0, anchor=center, font=\scriptsize},
    yticklabel style={font=\tiny},
    width=4.3cm,
    height=4.5cm,
    grid=major,
    grid style={dashed, gray!30},
    tick label style={font=\scriptsize},
    scale only axis,
]
\addplot[
    fill=red!70,
    draw=red!80,
    line width=0.5pt
] coordinates {
    (Planificador, 0.69)
};
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Planificador, 0.84)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}
\begin{minipage}{0.32\textwidth}
\centering
\subfloat[Precisión de Herramientas\label{fig:tool_precision_agentes}]{
\begin{tikzpicture}
\begin{axis}[
    ylabel=,  
    xlabel=,
    ymin=0,
    ymax=1.0,
    ytick={0,0.2,0.4,0.6,0.8,1.0},
    enlarge x limits=0.8,
    ybar=4pt,
    bar width=5pt,
    symbolic x coords={Orquestador},
    xtick={Orquestador},
    x tick label style={rotate=0, anchor=center, font=\scriptsize},
    yticklabel style={font=\tiny},
    width=4.3cm,
    height=4.5cm,
    grid=major,
    grid style={dashed, gray!30},
    tick label style={font=\scriptsize},
    scale only axis,
]
\addplot[
    fill=red!70,
    draw=red!80,
    line width=0.5pt
] coordinates {
    (Orquestador, 0.70)
};
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Orquestador, 0.91)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}
\begin{minipage}{0.32\textwidth}
\centering
\subfloat[Alucinación\label{fig:alucinacion_agentes}]{
\begin{tikzpicture}
\begin{axis}[
    ylabel=,  
    xlabel=,
    ymin=0,
    ymax=0.8,
    ytick={0,0.2,0.4,0.6,0.8},
    enlarge x limits=0.8,
    ybar=4pt,
    bar width=5pt,
    symbolic x coords={Planificador},
    xtick={Planificador},
    x tick label style={rotate=0, anchor=center, font=\scriptsize},
    yticklabel style={font=\tiny},
    width=4.3cm,
    height=4.5cm,
    grid=major,
    grid style={dashed, gray!30},
    tick label style={font=\scriptsize},
    scale only axis,
]
\addplot[
    fill=red!70,
    draw=red!80,
    line width=0.5pt
] coordinates {
    (Planificador, 0.01)
};
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Planificador, 0.66)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}

% Leyenda global centrada
\vspace{-0.2cm}
\begin{center}
\begin{tikzpicture}
    \draw[fill=red!70,draw=red!80,line width=0.5pt] (0,0) rectangle (0.4,-0.25);
    \node[right] at (0.5,-0.125) {Inicial};
    \draw[fill=blue!70,draw=blue!80,line width=0.5pt] (2,0) rectangle (2.4,-0.25);
    \node[right] at (2.5,-0.125) {Mejorado};
\end{tikzpicture}
\end{center}

\caption{Evaluación de agentes orquestador y planificador antes y después de prompting few-shot}
\label{fig:comparacion_agentes}
\end{figure}
\vspace{-0.2cm}

\subsection{Variaciones de orquestación}
Esta evaluación compara los diferentes enfoques de orquestación: planificación dividida, planificación unificada y el sistema sin planificación. Adicionalmente se incluye la evaluación del sistema con planificación dividida antes de implementar las mejoras de la sección anterior. 

Como se puede observar en la Figra \ref{}, las mejoras de la sección anterior mejoraron significativamente el sistema general. El prompting few-shot en los agentes superiores redució la cantidad de agentes utilizados, mejorando significativamente el costo total.

\begin{figure}[hbtp]
\centering

% Primera fila con dos gráficos usando minipage
\begin{minipage}{0.48\textwidth}
\centering
\subfloat[LLM-juez\label{fig:llm_judge_sistemas}]{
\begin{tikzpicture}
\begin{axis}[
    ylabel=,  
    xlabel=,
    ymin=0,
    ymax=1.0,
    ytick={0,0.2,0.4,0.6,0.8,1.0},
    enlarge x limits=0.15,
    ybar=4pt,
    bar width=8pt,
    symbolic x coords={Inicial, Dividida, Fusionada, Sin Plan, Adaptativo},
    xtick={Inicial, Dividida, Fusionada, Sin Plan, Adaptativo},
    x tick label style={rotate=45, anchor=north east, font=\scriptsize},
    yticklabel style={font=\tiny},
    width=5cm,
    height=4.5cm,
    grid=major,
    grid style={dashed, gray!30},
    tick label style={font=\scriptsize},
    scale only axis,
]
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Inicial, 0.5183)
    (Dividida, 0.7665)
    (Fusionada, 0.7889)
    (Sin Plan, 0.7126)
    (Adaptativo, 0.7307)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}
\begin{minipage}{0.48\textwidth}
\centering
\subfloat[Costo total de tokens (millones)\label{fig:token_cost_sistemas}]{
\begin{tikzpicture}
\begin{axis}[
    ylabel=,  
    xlabel=,
    ymin=0,
    ymax=8,
    ytick={0,2,4,6,8},
    enlarge x limits=0.15,
    ybar=4pt,
    bar width=8pt,
    symbolic x coords={Inicial, Dividida, Fusionada, Sin Plan, Adaptativo},
    xtick={Inicial, Dividida, Fusionada, Sin Plan, Adaptativo},
    x tick label style={rotate=45, anchor=north east, font=\scriptsize},
    yticklabel style={font=\tiny},
    width=5cm,
    height=4.5cm,
    grid=major,
    grid style={dashed, gray!30},
    tick label style={font=\scriptsize},
    scale only axis,
]
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Inicial, 6.8)
    (Dividida, 5.0)
    (Fusionada, 4.5)
    (Sin Plan, 1.4)
    (Adaptativo, 4.5)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}

\caption{Comparación de rendimiento LLM-juez y costo de tokens entre diferentes sistemas}
\label{fig:comparacion_sistemas}
\end{figure}
\vspace{-0.2cm}



Los resultados de evaluación evidencian que el paso de planificación mejora le sistema. Más concretamente, la planificación unificada supera a la dividida. Esto significa que en este caso de uso, la divisió lógica del paso de planificació no es rentable, y es mejor que el agente planificador tenga directamente la máxima información posible para fundamentear su plan. 

\subsection{Integración de memoria}
Para la evaluación de este componente se ha dividido el dataset del agente principal en dos partes, en entrenamiento (80\%) y evaluación (20\%). El objetivo es analizar si las memorias de ejecuciones de las consultas del dataset de entrenamiento pueden mejorar el rendimiento en el dataset de evaluación. Por lo tanto, se ejecutó primero el sistema completo en el dataste de entrenamiento, acumulando memorias para este. Después, se ejecutó en el dataset de evaluación, para comparar el resultado con el sistema sin memoria.

Los resultados se ilustran en la Figura \ref{}. Como se puede observar, la inclusión de memorias del dataset de entrenamiento realmente mejoró la precisión respecto al sistema sin memoria. Por otra parte, la Figura \ref{} muestra la evaluación repetida del conjunto de test, añadiendo en cada ejecución las memorias correspondientes a los mismos ejemplos. Estas resultaron menos prometedores, indicando que la memoria funciona mejor como información adicional que como una memoria caché. 

\begin{figure}[hbtp]
\centering
% Primera fila con dos gráficos usando minipage
\begin{minipage}{0.48\textwidth}
\centering
\subfloat[Comparación con/sin memoria\label{fig:memoria_comparacion}]{
\begin{tikzpicture}
\begin{axis}[
    ylabel=,  
    xlabel=,
    ymin=0,
    ymax=1.0,
    ytick={0,0.2,0.4,0.6,0.8,1.0},
    height=5.5cm,
    enlarge x limits=0.3,
    ybar=2pt,
    bar width=6pt,
    symbolic x coords={Sin memoria, Con memoria},
    xtick={Sin memoria, Con memoria},
    x tick label style={rotate=45, anchor=north east, font=\scriptsize},
    yticklabel style={font=\tiny},
    grid=major,
    grid style={dashed, gray!30},
    tick label style={font=\scriptsize},
    scale only axis,
]
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Sin memoria, 0.7377)
    (Con memoria, 0.8066)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}
\begin{minipage}{0.44\textwidth}
\centering
\subfloat[Evolución en ejecuciones consecutivas\label{fig:evolucion_trials}]{
\begin{tikzpicture}
\begin{axis}[
    title={},
    xlabel={ejecuciones consecutivas},
    ylabel={precisión llm juez},
    xmin=1, xmax=3,
    ymin=65, ymax=130,
    xtick={1, 2, 3},
    ytick={0,20,40,60,80,100},
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
]
% gráfico original trial 1
\addplot[
    color=blue,
    mark=square,
    opacity=0.4,
    ]
    coordinates {
    (1, 70)(2, 73)(3, 80) 
    };
% trial 2 
\addplot[
    color=red,
    mark=square,
    opacity=0.4,
    ]
    coordinates {
      (1, 78)(2, 72)(3, 76)
    };
% trial 3 
\addplot[
    color=green!60!black,
    mark=square,
    opacity=0.4,
    ]
    coordinates {
      (1, 73)(2, 71)(3, 75)
    };
% línea promedio
\addplot[
    color=black,
    mark=*,
    line width=1.5pt,
    ]
    coordinates {
      (1, 73)(2, 72)(3, 77)
    };
\legend{trial 1, trial 2, trial 3, promedio}
\end{axis}
\end{tikzpicture}
}
\end{minipage}
\caption{Comparación del impacto de la memoria y evolución en ejecuciones consecutivas}
\label{fig:memoria_evolucion}
\end{figure}
