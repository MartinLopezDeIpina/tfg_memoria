Tras exponer la implementación de los diferentes módulos del sistema, este capítulo detalla su evaluación y la comparación entre las diferentes versiones propuestas.

Con este propósito, inicialmente se explica el sistema de evaluación implementado, tras lo que se exponen los resultados de evaluación y las conclusiones derivadas de dichos resultados. 

\section{Sistema de evaluación}
La evaluación del sistema plantea un reto inherente al lenguaje natural: ¿cómo determinar si la respuesta a una pregunta es correcta? La solución adoptada comprende una serie de métricas (Sección \ref{sec:metricas}), evaluadas mediante el SDK de LangSmith (Sección \ref{sec:langsmith}) sobre un dataset \textit{ground truth} anotado manualmente (Sección \ref{sec:dataset}).

\subsection{Métricas de evaluación}

\label{sec:metricas}
Siguiendo los principios establecidos en la Sección \colorbox{yellow}{\ref{}}, se han implementado las siguientes métricas para cada agente sobre una pregunta anotada: 
\todo{Aquí creo que debería referenciar una sección de antecedentes que introduzca el tema de evaluación de agentes LLM y explique brevemente el TFG de Mikel Lonbide.}
\paragraph{LLM juez} Consiste en utilizar un agente evaluador cuya función sea valorar el resultado obtenido por el agente evaluado. Para minimizar la tasa de error de este, se han anotado las respuestas esperadas como listas textuales con los conceptos necesarios a incluir en la respuesta del agente. De esta forma, el agente juez genera una respuesta estructurada que contiene un argumento booleano para cada concepto requerido, indicando si se incluye en la respuesta generada y calculando así una precisión medible.

Cabe destacar que el juez marcará como incorrecto todo concepto más abstracto que el anotado, aún si conceptualmente el significado es equivalente. Por ejemplo, si se anota que la función del proyecto es proporcionar herramientas de modelos de lenguaje generativos para el equipo de desarrolladores, una respuesta sobre proporcionar herramientas de inteligencia artificial para el equipo sería considerada incorrecta.

\paragraph{Precisión de herramientas} Se comparan las herramientas utilizadas por un agente para responder una pregunta con el listado de herramientas anotado como ideal, calculando dos métricas posibles:

\begin{itemize}
\item Precisión de herramientas necesarias: indica la cantidad de herramientas necesarias utilizadas, favoreciendo el uso de estas.
\item Precisión de herramientas innecesarias: indica la cantidad de herramientas innecesarias ignoradas, desfavoreciendo el uso de estas.
\end{itemize}

A modo de ejemplo, un agente podría tener 5 herramientas disponibles, de las cuales 2 son necesarias, 2 son innecesarias, y otra no se clasifica en ninguno de los dos grupos, pudiendo ser de utilidad pero no indispensable. Si este llama a 1 necesaria y 2 innecesarias, la precisión necesaria sería de 0.5, mientras que la innecesaria sería de 0.0. La media total sería de 0.25. El uso de la herramienta no clasificada no varía ninguna precisión.

\paragraph{Precisión de alucinación} Los modelos tienden a responder las consultas realizadas aún si no contienen el conocimiento necesario para ello. Para evaluar esto, se han anotado algunas preguntas que son imposibles de responder con la información a la que el agente tiene acceso. Ante la consulta: \texttt{¿Qué flujo de despliegue continuo se utiliza?}, el agente no podrá responder correctamente porque no existe dicho flujo.

Esto es evaluado mediante un agente juez el cual determina si la respuesta del agente realmente intenta responder a la pregunta o por el contrario indica que no se dispone de la información suficiente.

\paragraph{Precisión de citación} Constituye la cantidad de documentos citados en la respuesta que estaban anotados como necesarios.


\subsection{Implementación de evaluación}
\label{sec:langsmith}
El sistema de evaluación de LangSmith, ilustrado en la Figura \ref{fig:mem_1}, consiste en ejecutar cada ejemplo en el conjunto de datos de forma asíncrona con el agente evaluado. El resultado de cada ejecución se combina con el ejemplo anotado, y el evaluador correspondiente a cada métrica compara los resultados. Posteriormente se calcula el promedio de cada métrica para cada evaluación.

\begin{figure}[h]
\centering
\adjustbox{center=\textwidth}{\includegraphics[width=1\linewidth]{figures/evaluacion.png}}
\caption{Mecanismo de evaluación de agentes}
\label{fig:mem_1}
\end{figure}

Para ello se utiliza la función \opus{evaluate_agent()} heredada por todos los agentes, la cual define una serie de instancias \opus{BaseEvaluator} que representan los evaluadores a ejecutar para cada ejemplo. El Listado \ref{lst:evaluate} ilustra dicha función para los agentes especializados.

\begin{lstlisting}[caption={Evaluación de agentes especializados},label={lst:evaluate}]
async def evaluate_agent(self, langsmith_client: Client):
    evaluators = [
        ToolPrecisionEvaluator(self.get_tools_from_run_state),
        JudgeLLMEvaluator(),
        CiteEvaluator()
    ]
    result = await self.call_agent_evaluation(langsmith_client, evaluators)
    return result
\end{lstlisting}

Esta función utiliza internamente las funciones de evaluación de LangSmith mediante \opus{call_agent_evaluation} (Listado \ref{lst:call_evaluation}) para ejecutar todos los ejemplos del conjunto de datos y evaluarlos con las métricas proporcionadas. El conjunto de datos de cada agente va ligado a su nombre y se obtiene mediante el cliente de LangSmith.

\begin{lstlisting}[caption={Llamada a evaluación de agentes},label={lst:call_evaluation}]
async def call_agent_evaluation(self, langsmith_client: Client, 
                               evaluators: List[BaseEvaluator], ...):
    ...
    evaluator_functions = [evaluator.evaluate_metrics for evaluator in evaluators]
    data=langsmith_client.list_examples(dataset_name=dataset_name, splits=splits)
    run_function = self.execute_from_dataset

    results = await aevaluate(
        run_function,
        data=data,
        client=langsmith_client,
        evaluators=evaluator_functions,
        max_concurrency=max_conc,
        experiment_prefix=evaluation_name,
    )
    return results
\end{lstlisting}

El Listado \ref{lst:cite_evaluator} muestra la implementación del evaluador de citas, donde se utilizan el estado de ejecución y el ejemplo anotado para calcular la precisión.

\begin{lstlisting}[caption={Evaluador de citas},label={lst:cite_evaluator}]
class CiteEvaluator(BaseEvaluator):
    async def evaluate(self, run: Run, example: Example) -> EvaluationResults:
        run_state = run.outputs.get("run_state")
        expected_cites = example.outputs.get("cite")
        expected_cites_list = get_list_from_string_comma_separated_values(expected_cites)
        actual_cites = get_cites_from_state_messages(run_state)

        citation_score = get_citation_score(expected_cites_list, actual_cites)
        return EvaluationResults(
            results=[
                EvaluationResult(
                    key="cite_precision",
                    score=StrictFloat(citation_score)
                )
            ]
        )
\end{lstlisting}


\subsection{Dataset anotado}
\label{sec:dataset}
La captura del conjunto de datos se ha realizado en una hoja de cálculo de Google Drive. Este documento se ha descargado posteriormente en formato de valores separados por coma (CSV) y se ha subido como conjunto de datos de LangSmith.

La captura comprende 46 ejemplos para el agente principal (el sistema completo) y aproximadamente 10 ejemplos para los agentes individuales. Para ello se han utilizado las preguntas anotadas en la captura de requisitos, filtrando y modificando en algunos casos las preguntas para ser lo suficientemente específicas como para tener una respuesta exacta, pero a su vez con cierto grado de complejidad que requiera un razonamiento sobre la consulta. Se han utilizado las siguientes consideraciones sobre cada agente:

\begin{itemize}
\item\textbf{Agente principal: }se han identificado tres tipos de preguntas: aquellas que se pueden responder consultando una única fuente de datos, las que requieren múltiples fuentes de datos, y las que requieren múltiples fuentes de datos en un orden secuencial, dependiendo la entrada de uno de la salida del otro.

\item\textbf{Planificador: }para este agente se ha anotado la respuesta como el plan generado para un ejemplo de ejecución, incluyendo la pregunta y el estado de ejecución actual. Se han identificado tres escenarios: preguntas donde se requiere un solo paso, preguntas donde se requieren varios pasos, y preguntas a medio completar donde se requiere que el agente genere el siguiente paso o decida finalizar el plan.

\item\textbf{Orquestador: }este agente se ha evaluado considerando los agentes seleccionados para ejecutar como herramientas. Para ello se han incluido preguntas de todas las temáticas, evaluando qué agentes llamar en cada caso:

\begin{itemize}
\item \textbf{Tareas de agente único: }se ha definido una consulta relacionada únicamente con cada agente especializado.

\item \textbf{Tareas multi-agente:} Las consultas de información general y documentación requieren principalmente el \opus{file_system_agent}, mientras que las de entorno y despliegue combinan este con el \opus{code_agent}. Las preguntas sobre arquitectura del sistema utilizan ambos agentes, y las de gestión del proyecto emplean el \opus{file_system_agent} junto con otros según el contexto específico. Para estándares y prácticas se añade el \opus{confluence_agent} cuando involucran frontend, y las tareas específicas de frontend combinan los agentes \opus{confluence_agent} y \opus{google_drive_agent}.

\end{itemize}
\item\textbf{Agentes sistema de ficheros, Confluence y Google Drive}: en los casos sencillos es necesario únicamente la herramienta de lectura de páginas. En los más desafiantes se requiere de las herramientas de búsqueda.
\item\textbf{Agente GitLab: }se han incluido ejemplos donde este debe llamar primero a la herramienta de obtener información sobre los usuarios para posteriormente utilizar esos nombres de usuario como parámetros en otras herramientas.
\item\textbf{Agente código: }en los casos sencillos los fragmentos relevantes deberían estar incluidos en el prompt de entrada, mientras que en los más complejos debería buscar información adicional.
\end{itemize}

\section{Resultados obtenidos}
En esta sección se presentan los resultados obtenidos mediante el sistema implementado. En primera instancia se exponen los resultados de evaluación correspondientes a la versión inicial de cada agente, contrastados con una segunda versión que incorpora un prompt optimizado. Posteriormente se evalúan los distintos sistemas de orquestación desarrollados. Finalmente se analizan las mejoras introducidas en el sistema.

Cabe destacar que los modelos de lenguaje no son deterministas: una misma pregunta puede generar respuestas diferentes. Además, al evaluar estas respuestas mediante otro LLM, se introduce una segunda capa de indeterminismo. Esto implica que el sistema presenta una tasa de error variable.

Los resultados mostrados corresponden al promedio de al menos dos ejecuciones. Si bien esto no garantiza certeza absoluta, sí permite afirmar que una evaluación más favorable tiene mayor probabilidad de ser una mejor solución. Esta metodología permite mejorar el sistema de manera objetiva, ya que las mejoras que generen resultados consistentemente superiores en las evaluaciones tienen mayor probabilidad de constituir avances reales.

\subsection{Mejora de versión inicial}
Inicialmente se mejoraron los agentes especializados que presentaron deficiencias en la evaluación inicial, incorporándose posteriormente el prompting few-shot en el orquestador y planificador.

\subsubsection{Agentes especializados}
La Figura \ref{fig:eval_esp} ilustra el rendimiento de los agentes especializados según las métricas de juez, citación y alucinación, así como su gasto promedio por cada ejemplo de ejecución en el dataset.

\begin{figure}[hbtp]
\centering

% Definir estilos comunes para todos los gráficos
\pgfplotsset{
    mybar/.style={
        ylabel=,  
        xlabel=,
        x tick label style={font=\tiny},
        yticklabel style={font=\tiny},
        ymin=0,
        enlarge x limits=0.12,
        ybar=4pt,
        bar width=5pt,
        symbolic x coords={Código, Confluence, S. ficheros, G. Drive, GitLab},
        xtick={Código, Confluence, S. ficheros, G. Drive, GitLab},
        x tick label style={rotate=45, anchor=east, font=\scriptsize},
        width=4.3cm,
        height=4.5cm,
        grid=major,
        grid style={dashed, gray!30},
        tick label style={font=\scriptsize},
        legend style={draw=none},
        scale only axis,
    }
}

% Primera fila con tres gráficos usando minipage
\begin{minipage}{0.32\textwidth}
\centering
\subfloat[LLM juez\label{fig:llm_juez}]{
\begin{tikzpicture}
\begin{axis}[
    mybar,
    ymax=1.1,
    ytick={0,0.2,0.4,0.6,0.8,1.0},
]
\addplot[
    fill=red!70,
    draw=red!80,
    line width=0.5pt
] coordinates {
    (Código, 0.67)
    (Confluence, 0.86)
    (S. ficheros, 0.88)
    (G. Drive, nan)
    (GitLab, nan)
};
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Código, 0.89)
    (Confluence, 0.86)
    (S. ficheros, 1.0)
    (G. Drive, 0.75)
    (GitLab, 0.9)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}
\begin{minipage}{0.32\textwidth}
\centering
\subfloat[Precisión citación\label{fig:precision_citas}]{
\begin{tikzpicture}
\begin{axis}[
    mybar,
    ymax=1.1,
    ytick={0,0.2,0.4,0.6,0.8,1.0},
]
\addplot[
    fill=red!70,
    draw=red!80,
    line width=0.5pt
] coordinates {
    (Código, 0.07)
    (Confluence, 0.86)
    (S. ficheros, 0.75)
    (G. Drive, nan)
    (GitLab, nan)
};
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Código, 0.21)
    (Confluence, 0.86)
    (S. ficheros, 0.71)
    (G. Drive, 0.84)
    (GitLab, 0.5)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}
\begin{minipage}{0.32\textwidth}
\centering
\subfloat[Precisión de herramientas\label{fig:precision_herramientas}]{
\begin{tikzpicture}
\begin{axis}[
    mybar,
    ymax=1.1,
    ytick={0,0.2,0.4,0.6,0.8,1.0},
]
\addplot[
    fill=red!70,
    draw=red!80,
    line width=0.5pt
] coordinates {
    (Código, 0.78)
    (Confluence, 0.87)
    (S. ficheros, 0.81)
    (G. Drive, nan)
    (GitLab, nan)
};
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Código, 0.76)
    (Confluence, 0.87)
    (S. ficheros, 0.84)
    (G. Drive, 0.82)
    (GitLab, 0.89)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}

\vspace{0.2cm}

% Segunda fila con dos gráficos centrados
\begin{minipage}{0.32\textwidth}
\centering
\subfloat[Alucinación (más es mejor)\label{fig:halucinacion}]{
\begin{tikzpicture}
\begin{axis}[
    mybar,
    ymax=1.1,
    ytick={0,0.2,0.4,0.6,0.8,1.0},
]
\addplot[
    fill=red!70,
    draw=red!80,
    line width=0.5pt
] coordinates {
    (Código, 0.67)
    (Confluence, 0.33)
    (S. ficheros, 0.16)
    (G. Drive, nan)
    (GitLab, nan)
};
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Código, 1.0)
    (Confluence, 0.33)
    (S. ficheros, 0.01)
    (G. Drive, 0.5)
    (GitLab, 0.67)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}
\hspace{0.05\textwidth}
\begin{minipage}{0.32\textwidth}
\centering
\subfloat[Tokens por ejecución (millares)\label{fig:tokens}]{
\begin{tikzpicture}
\begin{axis}[
    mybar,
    ymax=60,
    ytick={0,10,20,30,40,50,60},  % Para el gráfico de tokens
]
\addplot[
    fill=red!70,
    draw=red!80,
    line width=0.5pt
] coordinates {
    (Código, 55)
    (Confluence, 14)
    (S. ficheros, 11)
    (G. Drive, nan)
    (GitLab, nan)
};
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Código, 46)
    (Confluence, 12)
    (S. ficheros, 12)
    (G. Drive, 36)
    (GitLab, 15)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}

% Leyenda global centrada (simplificada)
\vspace{-0.2cm} % Justo antes del bloque de leyenda
\begin{center}
\begin{tikzpicture}
    \draw[fill=red!70,draw=red!80,line width=0.5pt] (0,0) rectangle (0.4,-0.25);
    \node[right] at (0.5,-0.125) {Inicial};
    \draw[fill=blue!70,draw=blue!80,line width=0.5pt] (2,0) rectangle (2.4,-0.25);
    \node[right] at (2.5,-0.125) {Mejorado};
\end{tikzpicture}
\end{center}

\caption{Comparación de métricas entre versión inicial y mejorada de agentes especialistas}
\label{fig:eval_esp}
\end{figure}
\vspace{-0.2cm} 

El agente de código presentó un gasto considerablemente superior debido a las múltiples consultas RAG realizadas para una misma pregunta. Se ajustó el prompt y la cantidad de fragmentos extraídos, reduciendo así el coste asociado a ejecuciones prolongadas.

La mejora del agente Confluence incorporó el enfoque de prompt caching. Aunque la precisión en todas las métricas resultó similar, este enfoque redujo ligeramente tanto el consumo de tokens como el gasto económico.

La mejora en el agente de sistema de ficheros consistió en la inclusión de la herramienta de RAG.

\subsubsection{Prompting few-shot}
La Figura \ref{fig:eval_fewshots} ilustra la comparación del agente planificador y orquestador tras incorporar el prompting few-shot. Los resultados muestran una mejora considerable en el rendimiento de ambos agentes.


\begin{figure}[hbtp]
\centering

% Primera fila con tres gráficos usando minipage
\begin{minipage}{0.32\textwidth}
\centering
\subfloat[LLM juez\label{fig:llm_judge_agentes}]{
\begin{tikzpicture}
\begin{axis}[
    ylabel=,  
    xlabel=,
    ymin=0,
    ymax=1.0,
    ytick={0,0.2,0.4,0.6,0.8,1.0},
    enlarge x limits=0.8,
    ybar=4pt,
    bar width=5pt,
    symbolic x coords={Planificador},
    xtick={Planificador},
    x tick label style={rotate=0, anchor=center, font=\scriptsize},
    yticklabel style={font=\tiny},
    width=4.3cm,
    height=4.5cm,
    grid=major,
    grid style={dashed, gray!30},
    tick label style={font=\scriptsize},
    scale only axis,
]
\addplot[
    fill=red!70,
    draw=red!80,
    line width=0.5pt
] coordinates {
    (Planificador, 0.69)
};
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Planificador, 0.84)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}
\begin{minipage}{0.32\textwidth}
\centering
\subfloat[Precisión herramientas\label{fig:tool_precision_agentes}]{
\begin{tikzpicture}
\begin{axis}[
    ylabel=,  
    xlabel=,
    ymin=0,
    ymax=1.0,
    ytick={0,0.2,0.4,0.6,0.8,1.0},
    enlarge x limits=0.8,
    ybar=4pt,
    bar width=5pt,
    symbolic x coords={Orquestador},
    xtick={Orquestador},
    x tick label style={rotate=0, anchor=center, font=\scriptsize},
    yticklabel style={font=\tiny},
    width=4.3cm,
    height=4.5cm,
    grid=major,
    grid style={dashed, gray!30},
    tick label style={font=\scriptsize},
    scale only axis,
]
\addplot[
    fill=red!70,
    draw=red!80,
    line width=0.5pt
] coordinates {
    (Orquestador, 0.70)
};
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Orquestador, 0.91)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}
\begin{minipage}{0.32\textwidth}
\centering
\subfloat[Alucinación (más es mejor)\label{fig:alucinacion_agentes}]{
\begin{tikzpicture}
\begin{axis}[
    ylabel=,  
    xlabel=,
    ymin=0,
    ymax=0.8,
    ytick={0,0.2,0.4,0.6,0.8},
    enlarge x limits=0.8,
    ybar=4pt,
    bar width=5pt,
    symbolic x coords={Planificador},
    xtick={Planificador},
    x tick label style={rotate=0, anchor=center, font=\scriptsize},
    yticklabel style={font=\tiny},
    width=4.3cm,
    height=4.5cm,
    grid=major,
    grid style={dashed, gray!30},
    tick label style={font=\scriptsize},
    scale only axis,
]
\addplot[
    fill=red!70,
    draw=red!80,
    line width=0.5pt
] coordinates {
    (Planificador, 0.01)
};
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Planificador, 0.66)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}

% Leyenda global centrada
\vspace{-0.2cm}
\begin{center}
\begin{tikzpicture}
    \draw[fill=red!70,draw=red!80,line width=0.5pt] (0,0) rectangle (0.4,-0.25);
    \node[right] at (0.5,-0.125) {Inicial};
    \draw[fill=blue!70,draw=blue!80,line width=0.5pt] (2,0) rectangle (2.4,-0.25);
    \node[right] at (2.5,-0.125) {Mejorado};
\end{tikzpicture}
\end{center}

\caption{Evaluación de agentes orquestador y planificador antes y después de prompting few-shot}
\label{fig:eval_fewshots}
\end{figure}
\vspace{-0.2cm}

\subsection{Variaciones de orquestación}
Esta evaluación (Figura \ref{fig:eval_orch}) compara los diferentes enfoques de orquestación: planificación dividida, planificación unificada y el sistema sin planificación. Adicionalmente se incluye la evaluación del sistema con planificación dividida previo a las mejoras implementadas en la sección anterior.

Los resultados muestran que las mejoras implementadas incrementaron significativamente el rendimiento del sistema respecto al inicial. El prompting few-shot en el orquestador y planificador redujo la cantidad de agentes utilizados, optimizando notablemente el coste total.

\begin{figure}[hbtp]
\centering

% Primera fila con dos gráficos usando minipage
\begin{minipage}{0.48\textwidth}
\centering
\subfloat[LLM-juez\label{fig:llm_judge_sistemas}]{
\begin{tikzpicture}
\begin{axis}[
    ylabel=,  
    xlabel=,
    ymin=0,
    ymax=1.0,
    ytick={0,0.2,0.4,0.6,0.8,1.0},
    enlarge x limits=0.15,
    ybar=4pt,
    bar width=8pt,
    symbolic x coords={Sistema inicial, Plan. dividida, Plan. fusionada, Sin planificación, Sistema adaptativo},
    xtick={Sistema inicial, Plan. dividida, Plan. fusionada, Sin planificación, Sistema adaptativo},
    x tick label style={rotate=45, anchor=north east, font=\scriptsize},
    yticklabel style={font=\tiny},
    width=5cm,
    height=4.5cm,
    grid=major,
    grid style={dashed, gray!30},
    tick label style={font=\scriptsize},
    scale only axis,
]
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Sistema inicial, 0.5183)
    (Plan. dividida, 0.7665)
    (Plan. fusionada, 0.7889)
    (Sin planificación, 0.7126)
    (Sistema adaptativo, 0.7307)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}
\begin{minipage}{0.48\textwidth}
\centering
\subfloat[Costo total de tokens (millones)\label{fig:token_cost_sistemas}]{
\begin{tikzpicture}
\begin{axis}[
    ylabel=,  
    xlabel=,
    ymin=0,
    ymax=8,
    ytick={0,2,4,6,8},
    enlarge x limits=0.15,
    ybar=4pt,
    bar width=8pt,
    symbolic x coords={Sistema inicial, Plan. dividida, Plan. fusionada, Sin planificación, Sistema adaptativo},
    xtick={Sistema inicial, Plan. dividida, Plan. fusionada, Sin planificación, Sistema adaptativo},
    x tick label style={rotate=45, anchor=north east, font=\scriptsize},
    yticklabel style={font=\tiny},
    width=5cm,
    height=4.5cm,
    grid=major,
    grid style={dashed, gray!30},
    tick label style={font=\scriptsize},
    scale only axis,
]
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Sistema inicial, 6.8)
    (Plan. dividida, 5.0)
    (Plan. fusionada, 4.5)
    (Sin planificación, 1.4)
    (Sistema adaptativo, 4.5)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}

\caption{Comparación de rendimiento y costo entre diferentes variaciones de orquestación}
\label{fig:eval_orch}
\end{figure}
\vspace{-0.2cm}

Los resultados de evaluación demuestran que el paso de planificación mejora el sistema. Específicamente, la planificación unificada supera a la dividida. Esto indica que, en este caso de uso, la división lógica del paso de planificación no resulta eficiente, siendo preferible que el agente planificador disponga directamente de la máxima información posible para fundamentar su plan.

Finalmente, los resultados del enfoque adaptativo no cumplieron las expectativas. Aunque el rendimiento se situó en el rango esperado —superior al sistema sin planificación pero inferior a la planificación unificada— y el coste resultó menor que la planificación tradicional (al no emplear modelos razonadores pese al uso similar de tokens), la mejora económica resultó mínima mientras que la pérdida de rendimiento fue considerable. Estos resultados indican que el enfoque adaptativo no es adecuado para este caso de uso, siendo la planificación unificada la opción más eficiente.

\subsection{Integración de memoria}
Para la evaluación de este componente se dividió el dataset del agente principal en entrenamiento (80\%) y evaluación (20\%). El objetivo consistió en analizar si las memorias acumuladas en la ejecución del dataset de entrenamiento mejoran el rendimiento en el de evaluación. Se ejecutó el sistema sobre ambos datasets secuencialmente, comparando el rendimiento con y sin memoria en el dataset de evaluación.

La Figura \ref{fig:mem_train} muestra que la inclusión de memorias del dataset de entrenamiento mejoró la precisión respecto al sistema sin memoria. Por otra parte, la Figura \ref{fig:mem_test} presenta la evaluación repetida del conjunto de test, añadiendo en cada ejecución las memorias correspondientes a los mismos ejemplos. Estos resultados fueron menos prometedores, indicando que en este caso la memoria funciona mejor como información adicional que como memoria caché.

\begin{figure}[hbtp]
\centering
% Primera fila con dos gráficos usando minipage
\begin{minipage}{0.48\textwidth}
\centering
\subfloat[Memorias de entrenamiento en evaluación\label{fig:memoria_comparacion}]{
\label{fig:mem_train}
\begin{tikzpicture}
\begin{axis}[
    ylabel=,  
    xlabel=,
    ymin=0,
    ymax=1.0,
    ytick={0,0.2,0.4,0.6,0.8,1.0},
    height=5.5cm,
    enlarge x limits=0.3,
    ybar=2pt,
    bar width=6pt,
    symbolic x coords={Sin memoria, Con memoria},
    xtick={Sin memoria, Con memoria},
    x tick label style={rotate=45, anchor=north east, font=\scriptsize},
    yticklabel style={font=\tiny},
    grid=major,
    grid style={dashed, gray!30},
    tick label style={font=\scriptsize},
    scale only axis,
    width=4.85cm,
]
\addplot[
    fill=blue!70,
    draw=blue!80,
    line width=0.5pt
] coordinates {
    (Sin memoria, 0.7377)
    (Con memoria, 0.8066)
};
\end{axis}
\end{tikzpicture}
}
\end{minipage}
\begin{minipage}{0.44\textwidth}
\centering
\subfloat[Memorias de evaluación en evaluación repetidamente\label{fig:evolucion_trials}]{
\label{fig:mem_test}
\begin{tikzpicture}
\begin{axis}[
    title={},
    xlabel={Ejecuciones consecutivas},
    ylabel={},
    xmin=1, xmax=3,
    ymin=65, ymax=130,
    xtick={1, 2, 3},
    ytick={0,20,40,60,80,100},
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
    width=8cm,
]
% gráfico original trial 1
\addplot[
    color=blue,
    mark=square,
    opacity=0.4,
    ]
    coordinates {
    (1, 70)(2, 73)(3, 80) 
    };
% trial 2 
\addplot[
    color=red,
    mark=square,
    opacity=0.4,
    ]
    coordinates {
      (1, 78)(2, 72)(3, 76)
    };
% trial 3 
\addplot[
    color=green!60!black,
    mark=square,
    opacity=0.4,
    ]
    coordinates {
      (1, 73)(2, 71)(3, 75)
    };
% línea promedio
\addplot[
    color=black,
    mark=*,
    line width=1.5pt,
    ]
    coordinates {
      (1, 73)(2, 72)(3, 77)
    };
\legend{trial 1, trial 2, trial 3, promedio}
\end{axis}
\end{tikzpicture}
}
\end{minipage}
\caption{Resultados de evaluación para el sistema con y sin memoria}
\label{fig:memoria_evolucion}
\end{figure}
